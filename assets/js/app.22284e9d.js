(window.webpackJsonp=window.webpackJsonp||[]).push([[0],[]]);!function(e){function n(n){for(var o,s,i=n[0],c=n[1],l=n[2],d=0,p=[];d<i.length;d++)s=i[d],Object.prototype.hasOwnProperty.call(r,s)&&r[s]&&p.push(r[s][0]),r[s]=0;for(o in c)Object.prototype.hasOwnProperty.call(c,o)&&(e[o]=c[o]);for(u&&u(n);p.length;)p.shift()();return a.push.apply(a,l||[]),t()}function t(){for(var e,n=0;n<a.length;n++){for(var t=a[n],o=!0,i=1;i<t.length;i++){var c=t[i];0!==r[c]&&(o=!1)}o&&(a.splice(n--,1),e=s(s.s=t[0]))}return e}var o={},r={1:0},a=[];function s(n){if(o[n])return o[n].exports;var t=o[n]={i:n,l:!1,exports:{}};return e[n].call(t.exports,t,t.exports,s),t.l=!0,t.exports}s.e=function(e){var n=[],t=r[e];if(0!==t)if(t)n.push(t[2]);else{var o=new Promise((function(n,o){t=r[e]=[n,o]}));n.push(t[2]=o);var a,i=document.createElement("script");i.charset="utf-8",i.timeout=120,s.nc&&i.setAttribute("nonce",s.nc),i.src=function(e){return s.p+"assets/js/"+({}[e]||e)+"."+{2:"73ab5a67",3:"9d834f89",4:"c4ebb801",5:"4d2ae7b6",6:"12afe53b",7:"008a403f",8:"378d2323",9:"9ddb870b",10:"aef316f2",11:"12d6f206",12:"cfc80d71",13:"cd12364d",14:"ee0987f2",15:"4c8e4ea6",16:"9e052c05",17:"02bfd5de",18:"8ebf1563",19:"c0b88fad",20:"90fa160e",21:"47b0131b",22:"e7647da8",23:"01e7394f",24:"a828f76f",25:"d48a658c",26:"f3403d78",27:"451d92cb",28:"4df42ea0",29:"2b13a07e",30:"10876572",31:"67883452",32:"b3ad1b85",33:"a001af47",34:"1b66aa13",35:"02920d25",36:"1624328d",37:"110afc3e",38:"1301a957",39:"cbfdd66a",40:"d7b53e6b",41:"60851a4a",42:"80495d79",43:"f7432b78",44:"48817c12",45:"f9ab51f4",46:"fd9540a7",47:"6fd69745",48:"287c5a92",49:"3c2c99b4",50:"810e0489"}[e]+".js"}(e);var c=new Error;a=function(n){i.onerror=i.onload=null,clearTimeout(l);var t=r[e];if(0!==t){if(t){var o=n&&("load"===n.type?"missing":n.type),a=n&&n.target&&n.target.src;c.message="Loading chunk "+e+" failed.\n("+o+": "+a+")",c.name="ChunkLoadError",c.type=o,c.request=a,t[1](c)}r[e]=void 0}};var l=setTimeout((function(){a({type:"timeout",target:i})}),12e4);i.onerror=i.onload=a,document.head.appendChild(i)}return Promise.all(n)},s.m=e,s.c=o,s.d=function(e,n,t){s.o(e,n)||Object.defineProperty(e,n,{enumerable:!0,get:t})},s.r=function(e){"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},s.t=function(e,n){if(1&n&&(e=s(e)),8&n)return e;if(4&n&&"object"==typeof e&&e&&e.__esModule)return e;var t=Object.create(null);if(s.r(t),Object.defineProperty(t,"default",{enumerable:!0,value:e}),2&n&&"string"!=typeof e)for(var o in e)s.d(t,o,function(n){return e[n]}.bind(null,o));return t},s.n=function(e){var n=e&&e.__esModule?function(){return e.default}:function(){return e};return s.d(n,"a",n),n},s.o=function(e,n){return Object.prototype.hasOwnProperty.call(e,n)},s.p="/",s.oe=function(e){throw console.error(e),e};var i=window.webpackJsonp=window.webpackJsonp||[],c=i.push.bind(i);i.push=n,i=i.slice();for(var l=0;l<i.length;l++)n(i[l]);var u=c;a.push([197,0]),t()}([function(e,n,t){var o=t(2),r=t(20).f,a=t(13),s=t(9),i=t(78),c=t(128),l=t(74);e.exports=function(e,n){var t,u,d,p,m,f=e.target,h=e.global,g=e.stat;if(t=h?o:g?o[f]||i(f,{}):(o[f]||{}).prototype)for(u in n){if(p=n[u],d=e.noTargetGet?(m=r(t,u))&&m.value:t[u],!l(h?u:f+(g?".":"#")+u,e.forced)&&void 0!==d){if(typeof p==typeof d)continue;c(p,d)}(e.sham||d&&d.sham)&&a(p,"sham",!0),s(t,u,p,e)}}},function(e,n){e.exports=function(e){try{return!!e()}catch(e){return!0}}},function(e,n){var t=function(e){return e&&e.Math==Math&&e};e.exports=t("object"==typeof globalThis&&globalThis)||t("object"==typeof window&&window)||t("object"==typeof self&&self)||t("object"==typeof global&&global)||function(){return this}()||Function("return this")()},function(e,n,t){var o=t(2),r=t(76),a=t(7),s=t(53),i=t(80),c=t(122),l=r("wks"),u=o.Symbol,d=c?u:u&&u.withoutSetter||s;e.exports=function(e){return a(l,e)||(i&&a(u,e)?l[e]=u[e]:l[e]=d("Symbol."+e)),l[e]}},function(e,n){e.exports=function(e){return"object"==typeof e?null!==e:"function"==typeof e}},function(e,n,t){var o=t(1);e.exports=!o((function(){return 7!=Object.defineProperty({},1,{get:function(){return 7}})[1]}))},function(e,n,t){var o=t(4);e.exports=function(e){if(!o(e))throw TypeError(String(e)+" is not an object");return e}},function(e,n){var t={}.hasOwnProperty;e.exports=function(e,n){return t.call(e,n)}},function(e,n,t){var o=t(5),r=t(121),a=t(6),s=t(39),i=Object.defineProperty;n.f=o?i:function(e,n,t){if(a(e),n=s(n,!0),a(t),r)try{return i(e,n,t)}catch(e){}if("get"in t||"set"in t)throw TypeError("Accessors not supported");return"value"in t&&(e[n]=t.value),e}},function(e,n,t){var o=t(2),r=t(13),a=t(7),s=t(78),i=t(83),c=t(31),l=c.get,u=c.enforce,d=String(String).split("String");(e.exports=function(e,n,t,i){var c,l=!!i&&!!i.unsafe,p=!!i&&!!i.enumerable,m=!!i&&!!i.noTargetGet;"function"==typeof t&&("string"!=typeof n||a(t,"name")||r(t,"name",n),(c=u(t)).source||(c.source=d.join("string"==typeof n?n:""))),e!==o?(l?!m&&e[n]&&(p=!0):delete e[n],p?e[n]=t:r(e,n,t)):p?e[n]=t:s(n,t)})(Function.prototype,"toString",(function(){return"function"==typeof this&&l(this).source||i(this)}))},function(e,n,t){var o=t(88),r=t(9),a=t(208);o||r(Object.prototype,"toString",a,{unsafe:!0})},function(e,n,t){var o=t(38),r=t(21);e.exports=function(e){return o(r(e))}},function(e,n,t){var o=t(21);e.exports=function(e){return Object(o(e))}},function(e,n,t){var o=t(5),r=t(8),a=t(35);e.exports=o?function(e,n,t){return r.f(e,n,a(1,t))}:function(e,n,t){return e[n]=t,e}},function(e,n,t){var o=t(46),r=Math.min;e.exports=function(e){return e>0?r(o(e),9007199254740991):0}},function(e,n){var t=Array.isArray;e.exports=t},function(e,n,t){var o=t(152),r="object"==typeof self&&self&&self.Object===Object&&self,a=o||r||Function("return this")();e.exports=a},function(e,n,t){"use strict";function o(e,n,t,o,r,a,s,i){var c,l="function"==typeof e?e.options:e;if(n&&(l.render=n,l.staticRenderFns=t,l._compiled=!0),o&&(l.functional=!0),a&&(l._scopeId="data-v-"+a),s?(c=function(e){(e=e||this.$vnode&&this.$vnode.ssrContext||this.parent&&this.parent.$vnode&&this.parent.$vnode.ssrContext)||"undefined"==typeof __VUE_SSR_CONTEXT__||(e=__VUE_SSR_CONTEXT__),r&&r.call(this,e),e&&e._registeredComponents&&e._registeredComponents.add(s)},l._ssrRegister=c):r&&(c=i?function(){r.call(this,(l.functional?this.parent:this).$root.$options.shadowRoot)}:r),c)if(l.functional){l._injectStyles=c;var u=l.render;l.render=function(e,n){return c.call(n),u(e,n)}}else{var d=l.beforeCreate;l.beforeCreate=d?[].concat(d,c):[c]}return{exports:e,options:l}}t.d(n,"a",(function(){return o}))},function(e,n,t){var o=t(5),r=t(1),a=t(7),s=Object.defineProperty,i={},c=function(e){throw e};e.exports=function(e,n){if(a(i,e))return i[e];n||(n={});var t=[][e],l=!!a(n,"ACCESSORS")&&n.ACCESSORS,u=a(n,0)?n[0]:c,d=a(n,1)?n[1]:void 0;return i[e]=!!t&&!r((function(){if(l&&!o)return!0;var e={length:-1};l?s(e,1,{enumerable:!0,get:c}):e[1]=1,t.call(e,u,d)}))}},function(e,n,t){var o=t(126),r=t(2),a=function(e){return"function"==typeof e?e:void 0};e.exports=function(e,n){return arguments.length<2?a(o[e])||a(r[e]):o[e]&&o[e][n]||r[e]&&r[e][n]}},function(e,n,t){var o=t(5),r=t(84),a=t(35),s=t(11),i=t(39),c=t(7),l=t(121),u=Object.getOwnPropertyDescriptor;n.f=o?u:function(e,n){if(e=s(e),n=i(n,!0),l)try{return u(e,n)}catch(e){}if(c(e,n))return a(!r.f.call(e,n),e[n])}},function(e,n){e.exports=function(e){if(null==e)throw TypeError("Can't call method on "+e);return e}},function(e,n,t){"use strict";var o=t(0),r=t(32).filter,a=t(59),s=t(18),i=a("filter"),c=s("filter");o({target:"Array",proto:!0,forced:!i||!c},{filter:function(e){return r(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,n){e.exports=!1},function(e,n){var t={}.toString;e.exports=function(e){return t.call(e).slice(8,-1)}},function(e,n){e.exports=function(e){if("function"!=typeof e)throw TypeError(String(e)+" is not a function");return e}},function(e,n,t){var o=t(235),r=t(238);e.exports=function(e,n){var t=r(e,n);return o(t)?t:void 0}},function(e,n,t){"use strict";var o=t(114).charAt,r=t(31),a=t(127),s=r.set,i=r.getterFor("String Iterator");a(String,"String",(function(e){s(this,{type:"String Iterator",string:String(e),index:0})}),(function(){var e,n=i(this),t=n.string,r=n.index;return r>=t.length?{value:void 0,done:!0}:(e=o(t,r),n.index+=e.length,{value:e,done:!1})}))},function(e,n,t){var o,r=t(6),a=t(110),s=t(82),i=t(40),c=t(125),l=t(79),u=t(55),d=u("IE_PROTO"),p=function(){},m=function(e){return"<script>"+e+"<\/script>"},f=function(){try{o=document.domain&&new ActiveXObject("htmlfile")}catch(e){}var e,n;f=o?function(e){e.write(m("")),e.close();var n=e.parentWindow.Object;return e=null,n}(o):((n=l("iframe")).style.display="none",c.appendChild(n),n.src=String("javascript:"),(e=n.contentWindow.document).open(),e.write(m("document.F=Object")),e.close(),e.F);for(var t=s.length;t--;)delete f.prototype[s[t]];return f()};i[d]=!0,e.exports=Object.create||function(e,n){var t;return null!==e?(p.prototype=r(e),t=new p,p.prototype=null,t[d]=e):t=f(),void 0===n?t:a(t,n)}},function(e,n){e.exports=function(e){return null!=e&&"object"==typeof e}},function(e,n,t){"use strict";var o=t(0),r=t(75);o({target:"RegExp",proto:!0,forced:/./.exec!==r},{exec:r})},function(e,n,t){var o,r,a,s=t(198),i=t(2),c=t(4),l=t(13),u=t(7),d=t(77),p=t(55),m=t(40),f=i.WeakMap;if(s){var h=d.state||(d.state=new f),g=h.get,v=h.has,b=h.set;o=function(e,n){return n.facade=e,b.call(h,e,n),n},r=function(e){return g.call(h,e)||{}},a=function(e){return v.call(h,e)}}else{var _=p("state");m[_]=!0,o=function(e,n){return n.facade=e,l(e,_,n),n},r=function(e){return u(e,_)?e[_]:{}},a=function(e){return u(e,_)}}e.exports={set:o,get:r,has:a,enforce:function(e){return a(e)?r(e):o(e,{})},getterFor:function(e){return function(n){var t;if(!c(n)||(t=r(n)).type!==e)throw TypeError("Incompatible receiver, "+e+" required");return t}}}},function(e,n,t){var o=t(48),r=t(38),a=t(12),s=t(14),i=t(113),c=[].push,l=function(e){var n=1==e,t=2==e,l=3==e,u=4==e,d=6==e,p=7==e,m=5==e||d;return function(f,h,g,v){for(var b,_,w=a(f),y=r(w),k=o(h,g,3),x=s(y.length),P=0,S=v||i,C=n?S(f,x):t||p?S(f,0):void 0;x>P;P++)if((m||P in y)&&(_=k(b=y[P],P,w),e))if(n)C[P]=_;else if(_)switch(e){case 3:return!0;case 5:return b;case 6:return P;case 2:c.call(C,b)}else switch(e){case 4:return!1;case 7:c.call(C,b)}return d?-1:l||u?u:C}};e.exports={forEach:l(0),map:l(1),filter:l(2),some:l(3),every:l(4),find:l(5),findIndex:l(6),filterOut:l(7)}},function(e,n,t){var o=t(2),r=t(140),a=t(109),s=t(13),i=t(3),c=i("iterator"),l=i("toStringTag"),u=a.values;for(var d in r){var p=o[d],m=p&&p.prototype;if(m){if(m[c]!==u)try{s(m,c,u)}catch(e){m[c]=u}if(m[l]||s(m,l,d),r[d])for(var f in a)if(m[f]!==a[f])try{s(m,f,a[f])}catch(e){m[f]=a[f]}}}},function(e,n,t){"use strict";var o=t(1);e.exports=function(e,n){var t=[][e];return!!t&&o((function(){t.call(null,n||function(){throw 1},1)}))}},function(e,n){e.exports=function(e,n){return{enumerable:!(1&e),configurable:!(2&e),writable:!(4&e),value:n}}},function(e,n,t){var o=t(24);e.exports=Array.isArray||function(e){return"Array"==o(e)}},function(e,n,t){var o=t(43),r=t(220),a=t(221),s=o?o.toStringTag:void 0;e.exports=function(e){return null==e?void 0===e?"[object Undefined]":"[object Null]":s&&s in Object(e)?r(e):a(e)}},function(e,n,t){var o=t(1),r=t(24),a="".split;e.exports=o((function(){return!Object("z").propertyIsEnumerable(0)}))?function(e){return"String"==r(e)?a.call(e,""):Object(e)}:Object},function(e,n,t){var o=t(4);e.exports=function(e,n){if(!o(e))return e;var t,r;if(n&&"function"==typeof(t=e.toString)&&!o(r=t.call(e)))return r;if("function"==typeof(t=e.valueOf)&&!o(r=t.call(e)))return r;if(!n&&"function"==typeof(t=e.toString)&&!o(r=t.call(e)))return r;throw TypeError("Can't convert object to primitive value")}},function(e,n){e.exports={}},function(e,n){e.exports={}},function(e,n,t){"use strict";var o=t(0),r=t(2),a=t(19),s=t(23),i=t(5),c=t(80),l=t(122),u=t(1),d=t(7),p=t(36),m=t(4),f=t(6),h=t(12),g=t(11),v=t(39),b=t(35),_=t(28),w=t(54),y=t(52),k=t(215),x=t(85),P=t(20),S=t(8),C=t(84),E=t(13),I=t(9),T=t(76),U=t(55),z=t(40),O=t(53),A=t(3),D=t(148),M=t(149),j=t(47),L=t(31),B=t(32).forEach,R=U("hidden"),N=A("toPrimitive"),F=L.set,G=L.getterFor("Symbol"),$=Object.prototype,q=r.Symbol,V=a("JSON","stringify"),W=P.f,H=S.f,K=k.f,J=C.f,Y=T("symbols"),X=T("op-symbols"),Q=T("string-to-symbol-registry"),Z=T("symbol-to-string-registry"),ee=T("wks"),ne=r.QObject,te=!ne||!ne.prototype||!ne.prototype.findChild,oe=i&&u((function(){return 7!=_(H({},"a",{get:function(){return H(this,"a",{value:7}).a}})).a}))?function(e,n,t){var o=W($,n);o&&delete $[n],H(e,n,t),o&&e!==$&&H($,n,o)}:H,re=function(e,n){var t=Y[e]=_(q.prototype);return F(t,{type:"Symbol",tag:e,description:n}),i||(t.description=n),t},ae=l?function(e){return"symbol"==typeof e}:function(e){return Object(e)instanceof q},se=function(e,n,t){e===$&&se(X,n,t),f(e);var o=v(n,!0);return f(t),d(Y,o)?(t.enumerable?(d(e,R)&&e[R][o]&&(e[R][o]=!1),t=_(t,{enumerable:b(0,!1)})):(d(e,R)||H(e,R,b(1,{})),e[R][o]=!0),oe(e,o,t)):H(e,o,t)},ie=function(e,n){f(e);var t=g(n),o=w(t).concat(de(t));return B(o,(function(n){i&&!ce.call(t,n)||se(e,n,t[n])})),e},ce=function(e){var n=v(e,!0),t=J.call(this,n);return!(this===$&&d(Y,n)&&!d(X,n))&&(!(t||!d(this,n)||!d(Y,n)||d(this,R)&&this[R][n])||t)},le=function(e,n){var t=g(e),o=v(n,!0);if(t!==$||!d(Y,o)||d(X,o)){var r=W(t,o);return!r||!d(Y,o)||d(t,R)&&t[R][o]||(r.enumerable=!0),r}},ue=function(e){var n=K(g(e)),t=[];return B(n,(function(e){d(Y,e)||d(z,e)||t.push(e)})),t},de=function(e){var n=e===$,t=K(n?X:g(e)),o=[];return B(t,(function(e){!d(Y,e)||n&&!d($,e)||o.push(Y[e])})),o};(c||(I((q=function(){if(this instanceof q)throw TypeError("Symbol is not a constructor");var e=arguments.length&&void 0!==arguments[0]?String(arguments[0]):void 0,n=O(e),t=function(e){this===$&&t.call(X,e),d(this,R)&&d(this[R],n)&&(this[R][n]=!1),oe(this,n,b(1,e))};return i&&te&&oe($,n,{configurable:!0,set:t}),re(n,e)}).prototype,"toString",(function(){return G(this).tag})),I(q,"withoutSetter",(function(e){return re(O(e),e)})),C.f=ce,S.f=se,P.f=le,y.f=k.f=ue,x.f=de,D.f=function(e){return re(A(e),e)},i&&(H(q.prototype,"description",{configurable:!0,get:function(){return G(this).description}}),s||I($,"propertyIsEnumerable",ce,{unsafe:!0}))),o({global:!0,wrap:!0,forced:!c,sham:!c},{Symbol:q}),B(w(ee),(function(e){M(e)})),o({target:"Symbol",stat:!0,forced:!c},{for:function(e){var n=String(e);if(d(Q,n))return Q[n];var t=q(n);return Q[n]=t,Z[t]=n,t},keyFor:function(e){if(!ae(e))throw TypeError(e+" is not a symbol");if(d(Z,e))return Z[e]},useSetter:function(){te=!0},useSimple:function(){te=!1}}),o({target:"Object",stat:!0,forced:!c,sham:!i},{create:function(e,n){return void 0===n?_(e):ie(_(e),n)},defineProperty:se,defineProperties:ie,getOwnPropertyDescriptor:le}),o({target:"Object",stat:!0,forced:!c},{getOwnPropertyNames:ue,getOwnPropertySymbols:de}),o({target:"Object",stat:!0,forced:u((function(){x.f(1)}))},{getOwnPropertySymbols:function(e){return x.f(h(e))}}),V)&&o({target:"JSON",stat:!0,forced:!c||u((function(){var e=q();return"[null]"!=V([e])||"{}"!=V({a:e})||"{}"!=V(Object(e))}))},{stringify:function(e,n,t){for(var o,r=[e],a=1;arguments.length>a;)r.push(arguments[a++]);if(o=n,(m(n)||void 0!==e)&&!ae(e))return p(n)||(n=function(e,n){if("function"==typeof o&&(n=o.call(this,e,n)),!ae(n))return n}),r[1]=n,V.apply(null,r)}});q.prototype[N]||E(q.prototype,N,q.prototype.valueOf),j(q,"Symbol"),z[R]=!0},function(e,n,t){var o=t(16).Symbol;e.exports=o},function(e,n,t){"use strict";t.d(n,"a",(function(){return a}));t(106);var o=t(45);t(42),t(62),t(89),t(150),t(10),t(27),t(33);var r=t(68);function a(e){return function(e){if(Array.isArray(e))return Object(o.a)(e)}(e)||function(e){if("undefined"!=typeof Symbol&&Symbol.iterator in Object(e))return Array.from(e)}(e)||Object(r.a)(e)||function(){throw new TypeError("Invalid attempt to spread non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(e,n,t){"use strict";function o(e,n){(null==n||n>e.length)&&(n=e.length);for(var t=0,o=new Array(n);t<n;t++)o[t]=e[t];return o}t.d(n,"a",(function(){return o}))},function(e,n){var t=Math.ceil,o=Math.floor;e.exports=function(e){return isNaN(e=+e)?0:(e>0?o:t)(e)}},function(e,n,t){var o=t(8).f,r=t(7),a=t(3)("toStringTag");e.exports=function(e,n,t){e&&!r(e=t?e:e.prototype,a)&&o(e,a,{configurable:!0,value:n})}},function(e,n,t){var o=t(25);e.exports=function(e,n,t){if(o(e),void 0===n)return e;switch(t){case 0:return function(){return e.call(n)};case 1:return function(t){return e.call(n,t)};case 2:return function(t,o){return e.call(n,t,o)};case 3:return function(t,o,r){return e.call(n,t,o,r)}}return function(){return e.apply(n,arguments)}}},function(e,n,t){"use strict";var o=t(0),r=t(4),a=t(36),s=t(124),i=t(14),c=t(11),l=t(60),u=t(3),d=t(59),p=t(18),m=d("slice"),f=p("slice",{ACCESSORS:!0,0:0,1:2}),h=u("species"),g=[].slice,v=Math.max;o({target:"Array",proto:!0,forced:!m||!f},{slice:function(e,n){var t,o,u,d=c(this),p=i(d.length),m=s(e,p),f=s(void 0===n?p:n,p);if(a(d)&&("function"!=typeof(t=d.constructor)||t!==Array&&!a(t.prototype)?r(t)&&null===(t=t[h])&&(t=void 0):t=void 0,t===Array||void 0===t))return g.call(d,m,f);for(o=new(void 0===t?Array:t)(v(f-m,0)),u=0;m<f;m++,u++)m in d&&l(o,u,d[m]);return o.length=u,o}})},function(e,n,t){"use strict";var o=t(0),r=t(141);o({target:"Array",proto:!0,forced:[].forEach!=r},{forEach:r})},function(e,n,t){var o=t(2),r=t(140),a=t(141),s=t(13);for(var i in r){var c=o[i],l=c&&c.prototype;if(l&&l.forEach!==a)try{s(l,"forEach",a)}catch(e){l.forEach=a}}},function(e,n,t){var o=t(123),r=t(82).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(e){return o(e,r)}},function(e,n){var t=0,o=Math.random();e.exports=function(e){return"Symbol("+String(void 0===e?"":e)+")_"+(++t+o).toString(36)}},function(e,n,t){var o=t(123),r=t(82);e.exports=Object.keys||function(e){return o(e,r)}},function(e,n,t){var o=t(76),r=t(53),a=o("keys");e.exports=function(e){return a[e]||(a[e]=r(e))}},function(e,n,t){var o=t(19);e.exports=o("navigator","userAgent")||""},function(e,n,t){var o=t(24),r=t(2);e.exports="process"==o(r.process)},function(e,n,t){var o,r,a=t(2),s=t(56),i=a.process,c=i&&i.versions,l=c&&c.v8;l?r=(o=l.split("."))[0]+o[1]:s&&(!(o=s.match(/Edge\/(\d+)/))||o[1]>=74)&&(o=s.match(/Chrome\/(\d+)/))&&(r=o[1]),e.exports=r&&+r},function(e,n,t){var o=t(1),r=t(3),a=t(58),s=r("species");e.exports=function(e){return a>=51||!o((function(){var n=[];return(n.constructor={})[s]=function(){return{foo:1}},1!==n[e](Boolean).foo}))}},function(e,n,t){"use strict";var o=t(39),r=t(8),a=t(35);e.exports=function(e,n,t){var s=o(n);s in e?r.f(e,s,a(0,t)):e[s]=t}},function(e,n,t){"use strict";t.d(n,"a",(function(){return r}));t(10);function o(e,n,t,o,r,a,s){try{var i=e[a](s),c=i.value}catch(e){return void t(e)}i.done?n(c):Promise.resolve(c).then(o,r)}function r(e){return function(){var n=this,t=arguments;return new Promise((function(r,a){var s=e.apply(n,t);function i(e){o(s,r,a,i,c,"next",e)}function c(e){o(s,r,a,i,c,"throw",e)}i(void 0)}))}}},function(e,n,t){"use strict";var o=t(0),r=t(5),a=t(2),s=t(7),i=t(4),c=t(8).f,l=t(128),u=a.Symbol;if(r&&"function"==typeof u&&(!("description"in u.prototype)||void 0!==u().description)){var d={},p=function(){var e=arguments.length<1||void 0===arguments[0]?void 0:String(arguments[0]),n=this instanceof p?new u(e):void 0===e?u():u(e);return""===e&&(d[n]=!0),n};l(p,u);var m=p.prototype=u.prototype;m.constructor=p;var f=m.toString,h="Symbol(test)"==String(u("test")),g=/^Symbol\((.*)\)[^)]+$/;c(m,"description",{configurable:!0,get:function(){var e=i(this)?this.valueOf():this,n=f.call(e);if(s(d,e))return"";var t=h?n.slice(7,-1):n.replace(g,"$1");return""===t?void 0:t}}),o({global:!0,forced:!0},{Symbol:p})}},function(e,n,t){var o=t(225),r=t(226),a=t(227),s=t(228),i=t(229);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var o=e[n];this.set(o[0],o[1])}}c.prototype.clear=o,c.prototype.delete=r,c.prototype.get=a,c.prototype.has=s,c.prototype.set=i,e.exports=c},function(e,n,t){var o=t(154);e.exports=function(e,n){for(var t=e.length;t--;)if(o(e[t][0],n))return t;return-1}},function(e,n,t){var o=t(26)(Object,"create");e.exports=o},function(e,n,t){var o=t(247);e.exports=function(e,n){var t=e.__data__;return o(n)?t["string"==typeof n?"string":"hash"]:t.map}},function(e,n,t){var o=t(98);e.exports=function(e){if("string"==typeof e||o(e))return e;var n=e+"";return"0"==n&&1/e==-1/0?"-0":n}},function(e,n,t){"use strict";t.d(n,"a",(function(){return r}));t(150),t(49),t(116),t(90),t(10),t(107),t(27);var o=t(45);function r(e,n){if(e){if("string"==typeof e)return Object(o.a)(e,n);var t=Object.prototype.toString.call(e).slice(8,-1);return"Object"===t&&e.constructor&&(t=e.constructor.name),"Map"===t||"Set"===t?Array.from(e):"Arguments"===t||/^(?:Ui|I)nt(?:8|16|32)(?:Clamped)?Array$/.test(t)?Object(o.a)(e,n):void 0}}},function(e,n){var t=/^\s+|\s+$/g,o=/^[-+]0x[0-9a-f]+$/i,r=/^0b[01]+$/i,a=/^0o[0-7]+$/i,s=parseInt,i="object"==typeof global&&global&&global.Object===Object&&global,c="object"==typeof self&&self&&self.Object===Object&&self,l=i||c||Function("return this")(),u=Object.prototype.toString,d=Math.max,p=Math.min,m=function(){return l.Date.now()};function f(e){var n=typeof e;return!!e&&("object"==n||"function"==n)}function h(e){if("number"==typeof e)return e;if(function(e){return"symbol"==typeof e||function(e){return!!e&&"object"==typeof e}(e)&&"[object Symbol]"==u.call(e)}(e))return NaN;if(f(e)){var n="function"==typeof e.valueOf?e.valueOf():e;e=f(n)?n+"":n}if("string"!=typeof e)return 0===e?e:+e;e=e.replace(t,"");var i=r.test(e);return i||a.test(e)?s(e.slice(2),i?2:8):o.test(e)?NaN:+e}e.exports=function(e,n,t){var o,r,a,s,i,c,l=0,u=!1,g=!1,v=!0;if("function"!=typeof e)throw new TypeError("Expected a function");function b(n){var t=o,a=r;return o=r=void 0,l=n,s=e.apply(a,t)}function _(e){return l=e,i=setTimeout(y,n),u?b(e):s}function w(e){var t=e-c;return void 0===c||t>=n||t<0||g&&e-l>=a}function y(){var e=m();if(w(e))return k(e);i=setTimeout(y,function(e){var t=n-(e-c);return g?p(t,a-(e-l)):t}(e))}function k(e){return i=void 0,v&&o?b(e):(o=r=void 0,s)}function x(){var e=m(),t=w(e);if(o=arguments,r=this,c=e,t){if(void 0===i)return _(c);if(g)return i=setTimeout(y,n),b(c)}return void 0===i&&(i=setTimeout(y,n)),s}return n=h(n)||0,f(t)&&(u=!!t.leading,a=(g="maxWait"in t)?d(h(t.maxWait)||0,n):a,v="trailing"in t?!!t.trailing:v),x.cancel=function(){void 0!==i&&clearTimeout(i),l=0,o=c=r=i=void 0},x.flush=function(){return void 0===i?s:k(m())},x}},function(e,n,t){var o,r;
/* NProgress, (c) 2013, 2014 Rico Sta. Cruz - http://ricostacruz.com/nprogress
 * @license MIT */void 0===(r="function"==typeof(o=function(){var e,n,t={version:"0.2.0"},o=t.settings={minimum:.08,easing:"ease",positionUsing:"",speed:200,trickle:!0,trickleRate:.02,trickleSpeed:800,showSpinner:!0,barSelector:'[role="bar"]',spinnerSelector:'[role="spinner"]',parent:"body",template:'<div class="bar" role="bar"><div class="peg"></div></div><div class="spinner" role="spinner"><div class="spinner-icon"></div></div>'};function r(e,n,t){return e<n?n:e>t?t:e}function a(e){return 100*(-1+e)}t.configure=function(e){var n,t;for(n in e)void 0!==(t=e[n])&&e.hasOwnProperty(n)&&(o[n]=t);return this},t.status=null,t.set=function(e){var n=t.isStarted();e=r(e,o.minimum,1),t.status=1===e?null:e;var c=t.render(!n),l=c.querySelector(o.barSelector),u=o.speed,d=o.easing;return c.offsetWidth,s((function(n){""===o.positionUsing&&(o.positionUsing=t.getPositioningCSS()),i(l,function(e,n,t){var r;return(r="translate3d"===o.positionUsing?{transform:"translate3d("+a(e)+"%,0,0)"}:"translate"===o.positionUsing?{transform:"translate("+a(e)+"%,0)"}:{"margin-left":a(e)+"%"}).transition="all "+n+"ms "+t,r}(e,u,d)),1===e?(i(c,{transition:"none",opacity:1}),c.offsetWidth,setTimeout((function(){i(c,{transition:"all "+u+"ms linear",opacity:0}),setTimeout((function(){t.remove(),n()}),u)}),u)):setTimeout(n,u)})),this},t.isStarted=function(){return"number"==typeof t.status},t.start=function(){t.status||t.set(0);var e=function(){setTimeout((function(){t.status&&(t.trickle(),e())}),o.trickleSpeed)};return o.trickle&&e(),this},t.done=function(e){return e||t.status?t.inc(.3+.5*Math.random()).set(1):this},t.inc=function(e){var n=t.status;return n?("number"!=typeof e&&(e=(1-n)*r(Math.random()*n,.1,.95)),n=r(n+e,0,.994),t.set(n)):t.start()},t.trickle=function(){return t.inc(Math.random()*o.trickleRate)},e=0,n=0,t.promise=function(o){return o&&"resolved"!==o.state()?(0===n&&t.start(),e++,n++,o.always((function(){0==--n?(e=0,t.done()):t.set((e-n)/e)})),this):this},t.render=function(e){if(t.isRendered())return document.getElementById("nprogress");l(document.documentElement,"nprogress-busy");var n=document.createElement("div");n.id="nprogress",n.innerHTML=o.template;var r,s=n.querySelector(o.barSelector),c=e?"-100":a(t.status||0),u=document.querySelector(o.parent);return i(s,{transition:"all 0 linear",transform:"translate3d("+c+"%,0,0)"}),o.showSpinner||(r=n.querySelector(o.spinnerSelector))&&p(r),u!=document.body&&l(u,"nprogress-custom-parent"),u.appendChild(n),n},t.remove=function(){u(document.documentElement,"nprogress-busy"),u(document.querySelector(o.parent),"nprogress-custom-parent");var e=document.getElementById("nprogress");e&&p(e)},t.isRendered=function(){return!!document.getElementById("nprogress")},t.getPositioningCSS=function(){var e=document.body.style,n="WebkitTransform"in e?"Webkit":"MozTransform"in e?"Moz":"msTransform"in e?"ms":"OTransform"in e?"O":"";return n+"Perspective"in e?"translate3d":n+"Transform"in e?"translate":"margin"};var s=function(){var e=[];function n(){var t=e.shift();t&&t(n)}return function(t){e.push(t),1==e.length&&n()}}(),i=function(){var e=["Webkit","O","Moz","ms"],n={};function t(t){return t=t.replace(/^-ms-/,"ms-").replace(/-([\da-z])/gi,(function(e,n){return n.toUpperCase()})),n[t]||(n[t]=function(n){var t=document.body.style;if(n in t)return n;for(var o,r=e.length,a=n.charAt(0).toUpperCase()+n.slice(1);r--;)if((o=e[r]+a)in t)return o;return n}(t))}function o(e,n,o){n=t(n),e.style[n]=o}return function(e,n){var t,r,a=arguments;if(2==a.length)for(t in n)void 0!==(r=n[t])&&n.hasOwnProperty(t)&&o(e,t,r);else o(e,a[1],a[2])}}();function c(e,n){return("string"==typeof e?e:d(e)).indexOf(" "+n+" ")>=0}function l(e,n){var t=d(e),o=t+n;c(t,n)||(e.className=o.substring(1))}function u(e,n){var t,o=d(e);c(e,n)&&(t=o.replace(" "+n+" "," "),e.className=t.substring(1,t.length-1))}function d(e){return(" "+(e.className||"")+" ").replace(/\s+/gi," ")}function p(e){e&&e.parentNode&&e.parentNode.removeChild(e)}return t})?o.call(n,t,n,e):o)||(e.exports=r)},function(e,n,t){"use strict";var o=t(0),r=t(32).map,a=t(59),s=t(18),i=a("map"),c=s("map");o({target:"Array",proto:!0,forced:!i||!c},{map:function(e){return r(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){"use strict";var o=t(179),r=t(6),a=t(14),s=t(46),i=t(21),c=t(184),l=t(214),u=t(180),d=Math.max,p=Math.min;o("replace",2,(function(e,n,t,o){var m=o.REGEXP_REPLACE_SUBSTITUTES_UNDEFINED_CAPTURE,f=o.REPLACE_KEEPS_$0,h=m?"$":"$0";return[function(t,o){var r=i(this),a=null==t?void 0:t[e];return void 0!==a?a.call(t,r,o):n.call(String(r),t,o)},function(e,o){if(!m&&f||"string"==typeof o&&-1===o.indexOf(h)){var i=t(n,e,this,o);if(i.done)return i.value}var g=r(e),v=String(this),b="function"==typeof o;b||(o=String(o));var _=g.global;if(_){var w=g.unicode;g.lastIndex=0}for(var y=[];;){var k=u(g,v);if(null===k)break;if(y.push(k),!_)break;""===String(k[0])&&(g.lastIndex=c(v,a(g.lastIndex),w))}for(var x,P="",S=0,C=0;C<y.length;C++){k=y[C];for(var E=String(k[0]),I=d(p(s(k.index),v.length),0),T=[],U=1;U<k.length;U++)T.push(void 0===(x=k[U])?x:String(x));var z=k.groups;if(b){var O=[E].concat(T,I,v);void 0!==z&&O.push(z);var A=String(o.apply(void 0,O))}else A=l(E,v,I,T,z,o);I>=S&&(P+=v.slice(S,I)+A,S=I+E.length)}return P+v.slice(S)}]}))},function(e,n,t){var o=t(0),r=t(12),a=t(54);o({target:"Object",stat:!0,forced:t(1)((function(){a(1)}))},{keys:function(e){return a(r(e))}})},function(e,n,t){var o=t(1),r=/#|\.prototype\./,a=function(e,n){var t=i[s(e)];return t==l||t!=c&&("function"==typeof n?o(n):!!n)},s=a.normalize=function(e){return String(e).replace(r,".").toLowerCase()},i=a.data={},c=a.NATIVE="N",l=a.POLYFILL="P";e.exports=a},function(e,n,t){"use strict";var o,r,a=t(115),s=t(191),i=RegExp.prototype.exec,c=String.prototype.replace,l=i,u=(o=/a/,r=/b*/g,i.call(o,"a"),i.call(r,"a"),0!==o.lastIndex||0!==r.lastIndex),d=s.UNSUPPORTED_Y||s.BROKEN_CARET,p=void 0!==/()??/.exec("")[1];(u||p||d)&&(l=function(e){var n,t,o,r,s=this,l=d&&s.sticky,m=a.call(s),f=s.source,h=0,g=e;return l&&(-1===(m=m.replace("y","")).indexOf("g")&&(m+="g"),g=String(e).slice(s.lastIndex),s.lastIndex>0&&(!s.multiline||s.multiline&&"\n"!==e[s.lastIndex-1])&&(f="(?: "+f+")",g=" "+g,h++),t=new RegExp("^(?:"+f+")",m)),p&&(t=new RegExp("^"+f+"$(?!\\s)",m)),u&&(n=s.lastIndex),o=i.call(l?t:s,g),l?o?(o.input=o.input.slice(h),o[0]=o[0].slice(h),o.index=s.lastIndex,s.lastIndex+=o[0].length):s.lastIndex=0:u&&o&&(s.lastIndex=s.global?o.index+o[0].length:n),p&&o&&o.length>1&&c.call(o[0],t,(function(){for(r=1;r<arguments.length-2;r++)void 0===arguments[r]&&(o[r]=void 0)})),o}),e.exports=l},function(e,n,t){var o=t(23),r=t(77);(e.exports=function(e,n){return r[e]||(r[e]=void 0!==n?n:{})})("versions",[]).push({version:"3.8.2",mode:o?"pure":"global",copyright:"© 2021 Denis Pushkarev (zloirock.ru)"})},function(e,n,t){var o=t(2),r=t(78),a=o["__core-js_shared__"]||r("__core-js_shared__",{});e.exports=a},function(e,n,t){var o=t(2),r=t(13);e.exports=function(e,n){try{r(o,e,n)}catch(t){o[e]=n}return n}},function(e,n,t){var o=t(2),r=t(4),a=o.document,s=r(a)&&r(a.createElement);e.exports=function(e){return s?a.createElement(e):{}}},function(e,n,t){var o=t(1);e.exports=!!Object.getOwnPropertySymbols&&!o((function(){return!String(Symbol())}))},function(e,n,t){var o=t(11),r=t(14),a=t(124),s=function(e){return function(n,t,s){var i,c=o(n),l=r(c.length),u=a(s,l);if(e&&t!=t){for(;l>u;)if((i=c[u++])!=i)return!0}else for(;l>u;u++)if((e||u in c)&&c[u]===t)return e||u||0;return!e&&-1}};e.exports={includes:s(!0),indexOf:s(!1)}},function(e,n){e.exports=["constructor","hasOwnProperty","isPrototypeOf","propertyIsEnumerable","toLocaleString","toString","valueOf"]},function(e,n,t){var o=t(77),r=Function.toString;"function"!=typeof o.inspectSource&&(o.inspectSource=function(e){return r.call(e)}),e.exports=o.inspectSource},function(e,n,t){"use strict";var o={}.propertyIsEnumerable,r=Object.getOwnPropertyDescriptor,a=r&&!o.call({1:2},1);n.f=a?function(e){var n=r(this,e);return!!n&&n.enumerable}:o},function(e,n){n.f=Object.getOwnPropertySymbols},function(e,n,t){var o=t(7),r=t(12),a=t(55),s=t(131),i=a("IE_PROTO"),c=Object.prototype;e.exports=s?Object.getPrototypeOf:function(e){return e=r(e),o(e,i)?e[i]:"function"==typeof e.constructor&&e instanceof e.constructor?e.constructor.prototype:e instanceof Object?c:null}},function(e,n,t){var o=t(6),r=t(199);e.exports=Object.setPrototypeOf||("__proto__"in{}?function(){var e,n=!1,t={};try{(e=Object.getOwnPropertyDescriptor(Object.prototype,"__proto__").set).call(t,[]),n=t instanceof Array}catch(e){}return function(t,a){return o(t),r(a),n?e.call(t,a):t.__proto__=a,t}}():void 0)},function(e,n,t){var o={};o[t(3)("toStringTag")]="z",e.exports="[object z]"===String(o)},function(e,n,t){t(149)("iterator")},function(e,n,t){var o=t(5),r=t(8).f,a=Function.prototype,s=a.toString,i=/^\s*function ([^ (]*)/;o&&!("name"in a)&&r(a,"name",{configurable:!0,get:function(){try{return s.call(this).match(i)[1]}catch(e){return""}}})},function(e,n,t){var o=t(219),r=t(29),a=Object.prototype,s=a.hasOwnProperty,i=a.propertyIsEnumerable,c=o(function(){return arguments}())?o:function(e){return r(e)&&s.call(e,"callee")&&!i.call(e,"callee")};e.exports=c},function(e,n,t){var o=t(26)(t(16),"Map");e.exports=o},function(e,n){e.exports=function(e){var n=typeof e;return null!=e&&("object"==n||"function"==n)}},function(e,n,t){var o=t(239),r=t(246),a=t(248),s=t(249),i=t(250);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var o=e[n];this.set(o[0],o[1])}}c.prototype.clear=o,c.prototype.delete=r,c.prototype.get=a,c.prototype.has=s,c.prototype.set=i,e.exports=c},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e){t[++n]=e})),t}},function(e,n){e.exports=function(e){return"number"==typeof e&&e>-1&&e%1==0&&e<=9007199254740991}},function(e,n,t){var o=t(15),r=t(98),a=/\.|\[(?:[^[\]]*|(["'])(?:(?!\1)[^\\]|\\.)*?\1)\]/,s=/^\w*$/;e.exports=function(e,n){if(o(e))return!1;var t=typeof e;return!("number"!=t&&"symbol"!=t&&"boolean"!=t&&null!=e&&!r(e))||(s.test(e)||!a.test(e)||null!=n&&e in Object(n))}},function(e,n,t){var o=t(37),r=t(29);e.exports=function(e){return"symbol"==typeof e||r(e)&&"[object Symbol]"==o(e)}},function(e,n){e.exports=function(e){return e}},function(e,n,t){var o=t(0),r=t(2),a=t(56),s=[].slice,i=function(e){return function(n,t){var o=arguments.length>2,r=o?s.call(arguments,2):void 0;return e(o?function(){("function"==typeof n?n:Function(n)).apply(this,r)}:n,t)}};o({global:!0,bind:!0,forced:/MSIE .\./.test(a)},{setTimeout:i(r.setTimeout),setInterval:i(r.setInterval)})},function(e,n,t){var o=t(0),r=t(5);o({target:"Object",stat:!0,forced:!r,sham:!r},{defineProperty:t(8).f})},function(e,n,t){"use strict";t.d(n,"a",(function(){return r}));t(106);t(42),t(62),t(89),t(10),t(27),t(33);var o=t(68);function r(e,n){return function(e){if(Array.isArray(e))return e}(e)||function(e,n){if("undefined"!=typeof Symbol&&Symbol.iterator in Object(e)){var t=[],o=!0,r=!1,a=void 0;try{for(var s,i=e[Symbol.iterator]();!(o=(s=i.next()).done)&&(t.push(s.value),!n||t.length!==n);o=!0);}catch(e){r=!0,a=e}finally{try{o||null==i.return||i.return()}finally{if(r)throw a}}return t}}(e,n)||Object(o.a)(e,n)||function(){throw new TypeError("Invalid attempt to destructure non-iterable instance.\nIn order to be iterable, non-array objects must have a [Symbol.iterator]() method.")}()}},function(e,n,t){var o=t(3),r=t(28),a=t(8),s=o("unscopables"),i=Array.prototype;null==i[s]&&a.f(i,s,{configurable:!0,value:r(null)}),e.exports=function(e){i[s][e]=!0}},function(e,n,t){var o=t(111),r=t(41),a=t(3)("iterator");e.exports=function(e){if(null!=e)return e[a]||e["@@iterator"]||r[o(e)]}},function(e,n,t){var o=function(e){"use strict";var n=Object.prototype,t=n.hasOwnProperty,o="function"==typeof Symbol?Symbol:{},r=o.iterator||"@@iterator",a=o.asyncIterator||"@@asyncIterator",s=o.toStringTag||"@@toStringTag";function i(e,n,t){return Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}),e[n]}try{i({},"")}catch(e){i=function(e,n,t){return e[n]=t}}function c(e,n,t,o){var r=n&&n.prototype instanceof d?n:d,a=Object.create(r.prototype),s=new x(o||[]);return a._invoke=function(e,n,t){var o="suspendedStart";return function(r,a){if("executing"===o)throw new Error("Generator is already running");if("completed"===o){if("throw"===r)throw a;return S()}for(t.method=r,t.arg=a;;){var s=t.delegate;if(s){var i=w(s,t);if(i){if(i===u)continue;return i}}if("next"===t.method)t.sent=t._sent=t.arg;else if("throw"===t.method){if("suspendedStart"===o)throw o="completed",t.arg;t.dispatchException(t.arg)}else"return"===t.method&&t.abrupt("return",t.arg);o="executing";var c=l(e,n,t);if("normal"===c.type){if(o=t.done?"completed":"suspendedYield",c.arg===u)continue;return{value:c.arg,done:t.done}}"throw"===c.type&&(o="completed",t.method="throw",t.arg=c.arg)}}}(e,t,s),a}function l(e,n,t){try{return{type:"normal",arg:e.call(n,t)}}catch(e){return{type:"throw",arg:e}}}e.wrap=c;var u={};function d(){}function p(){}function m(){}var f={};f[r]=function(){return this};var h=Object.getPrototypeOf,g=h&&h(h(P([])));g&&g!==n&&t.call(g,r)&&(f=g);var v=m.prototype=d.prototype=Object.create(f);function b(e){["next","throw","return"].forEach((function(n){i(e,n,(function(e){return this._invoke(n,e)}))}))}function _(e,n){var o;this._invoke=function(r,a){function s(){return new n((function(o,s){!function o(r,a,s,i){var c=l(e[r],e,a);if("throw"!==c.type){var u=c.arg,d=u.value;return d&&"object"==typeof d&&t.call(d,"__await")?n.resolve(d.__await).then((function(e){o("next",e,s,i)}),(function(e){o("throw",e,s,i)})):n.resolve(d).then((function(e){u.value=e,s(u)}),(function(e){return o("throw",e,s,i)}))}i(c.arg)}(r,a,o,s)}))}return o=o?o.then(s,s):s()}}function w(e,n){var t=e.iterator[n.method];if(void 0===t){if(n.delegate=null,"throw"===n.method){if(e.iterator.return&&(n.method="return",n.arg=void 0,w(e,n),"throw"===n.method))return u;n.method="throw",n.arg=new TypeError("The iterator does not provide a 'throw' method")}return u}var o=l(t,e.iterator,n.arg);if("throw"===o.type)return n.method="throw",n.arg=o.arg,n.delegate=null,u;var r=o.arg;return r?r.done?(n[e.resultName]=r.value,n.next=e.nextLoc,"return"!==n.method&&(n.method="next",n.arg=void 0),n.delegate=null,u):r:(n.method="throw",n.arg=new TypeError("iterator result is not an object"),n.delegate=null,u)}function y(e){var n={tryLoc:e[0]};1 in e&&(n.catchLoc=e[1]),2 in e&&(n.finallyLoc=e[2],n.afterLoc=e[3]),this.tryEntries.push(n)}function k(e){var n=e.completion||{};n.type="normal",delete n.arg,e.completion=n}function x(e){this.tryEntries=[{tryLoc:"root"}],e.forEach(y,this),this.reset(!0)}function P(e){if(e){var n=e[r];if(n)return n.call(e);if("function"==typeof e.next)return e;if(!isNaN(e.length)){var o=-1,a=function n(){for(;++o<e.length;)if(t.call(e,o))return n.value=e[o],n.done=!1,n;return n.value=void 0,n.done=!0,n};return a.next=a}}return{next:S}}function S(){return{value:void 0,done:!0}}return p.prototype=v.constructor=m,m.constructor=p,p.displayName=i(m,s,"GeneratorFunction"),e.isGeneratorFunction=function(e){var n="function"==typeof e&&e.constructor;return!!n&&(n===p||"GeneratorFunction"===(n.displayName||n.name))},e.mark=function(e){return Object.setPrototypeOf?Object.setPrototypeOf(e,m):(e.__proto__=m,i(e,s,"GeneratorFunction")),e.prototype=Object.create(v),e},e.awrap=function(e){return{__await:e}},b(_.prototype),_.prototype[a]=function(){return this},e.AsyncIterator=_,e.async=function(n,t,o,r,a){void 0===a&&(a=Promise);var s=new _(c(n,t,o,r),a);return e.isGeneratorFunction(t)?s:s.next().then((function(e){return e.done?e.value:s.next()}))},b(v),i(v,s,"Generator"),v[r]=function(){return this},v.toString=function(){return"[object Generator]"},e.keys=function(e){var n=[];for(var t in e)n.push(t);return n.reverse(),function t(){for(;n.length;){var o=n.pop();if(o in e)return t.value=o,t.done=!1,t}return t.done=!0,t}},e.values=P,x.prototype={constructor:x,reset:function(e){if(this.prev=0,this.next=0,this.sent=this._sent=void 0,this.done=!1,this.delegate=null,this.method="next",this.arg=void 0,this.tryEntries.forEach(k),!e)for(var n in this)"t"===n.charAt(0)&&t.call(this,n)&&!isNaN(+n.slice(1))&&(this[n]=void 0)},stop:function(){this.done=!0;var e=this.tryEntries[0].completion;if("throw"===e.type)throw e.arg;return this.rval},dispatchException:function(e){if(this.done)throw e;var n=this;function o(t,o){return s.type="throw",s.arg=e,n.next=t,o&&(n.method="next",n.arg=void 0),!!o}for(var r=this.tryEntries.length-1;r>=0;--r){var a=this.tryEntries[r],s=a.completion;if("root"===a.tryLoc)return o("end");if(a.tryLoc<=this.prev){var i=t.call(a,"catchLoc"),c=t.call(a,"finallyLoc");if(i&&c){if(this.prev<a.catchLoc)return o(a.catchLoc,!0);if(this.prev<a.finallyLoc)return o(a.finallyLoc)}else if(i){if(this.prev<a.catchLoc)return o(a.catchLoc,!0)}else{if(!c)throw new Error("try statement without catch or finally");if(this.prev<a.finallyLoc)return o(a.finallyLoc)}}}},abrupt:function(e,n){for(var o=this.tryEntries.length-1;o>=0;--o){var r=this.tryEntries[o];if(r.tryLoc<=this.prev&&t.call(r,"finallyLoc")&&this.prev<r.finallyLoc){var a=r;break}}a&&("break"===e||"continue"===e)&&a.tryLoc<=n&&n<=a.finallyLoc&&(a=null);var s=a?a.completion:{};return s.type=e,s.arg=n,a?(this.method="next",this.next=a.finallyLoc,u):this.complete(s)},complete:function(e,n){if("throw"===e.type)throw e.arg;return"break"===e.type||"continue"===e.type?this.next=e.arg:"return"===e.type?(this.rval=this.arg=e.arg,this.method="return",this.next="end"):"normal"===e.type&&n&&(this.next=n),u},finish:function(e){for(var n=this.tryEntries.length-1;n>=0;--n){var t=this.tryEntries[n];if(t.finallyLoc===e)return this.complete(t.completion,t.afterLoc),k(t),u}},catch:function(e){for(var n=this.tryEntries.length-1;n>=0;--n){var t=this.tryEntries[n];if(t.tryLoc===e){var o=t.completion;if("throw"===o.type){var r=o.arg;k(t)}return r}}throw new Error("illegal catch attempt")},delegateYield:function(e,n,t){return this.delegate={iterator:P(e),resultName:n,nextLoc:t},"next"===this.method&&(this.arg=void 0),u}},e}(e.exports);try{regeneratorRuntime=o}catch(e){Function("r","regeneratorRuntime = r")(o)}},function(e,n,t){t(0)({target:"Array",stat:!0},{isArray:t(36)})},function(e,n,t){"use strict";var o=t(9),r=t(6),a=t(1),s=t(115),i=RegExp.prototype,c=i.toString,l=a((function(){return"/a/b"!=c.call({source:"a",flags:"b"})})),u="toString"!=c.name;(l||u)&&o(RegExp.prototype,"toString",(function(){var e=r(this),n=String(e.source),t=e.flags;return"/"+n+"/"+String(void 0===t&&e instanceof RegExp&&!("flags"in i)?s.call(e):t)}),{unsafe:!0})},function(e,n,t){"use strict";var o=t(0),r=t(1),a=t(36),s=t(4),i=t(12),c=t(14),l=t(60),u=t(113),d=t(59),p=t(3),m=t(58),f=p("isConcatSpreadable"),h=m>=51||!r((function(){var e=[];return e[f]=!1,e.concat()[0]!==e})),g=d("concat"),v=function(e){if(!s(e))return!1;var n=e[f];return void 0!==n?!!n:a(e)};o({target:"Array",proto:!0,forced:!h||!g},{concat:function(e){var n,t,o,r,a,s=i(this),d=u(s,0),p=0;for(n=-1,o=arguments.length;n<o;n++)if(v(a=-1===n?s:arguments[n])){if(p+(r=c(a.length))>9007199254740991)throw TypeError("Maximum allowed index exceeded");for(t=0;t<r;t++,p++)t in a&&l(d,p,a[t])}else{if(p>=9007199254740991)throw TypeError("Maximum allowed index exceeded");l(d,p++,a)}return d.length=p,d}})},function(e,n,t){"use strict";var o=t(11),r=t(103),a=t(41),s=t(31),i=t(127),c=s.set,l=s.getterFor("Array Iterator");e.exports=i(Array,"Array",(function(e,n){c(this,{type:"Array Iterator",target:o(e),index:0,kind:n})}),(function(){var e=l(this),n=e.target,t=e.kind,o=e.index++;return!n||o>=n.length?(e.target=void 0,{value:void 0,done:!0}):"keys"==t?{value:o,done:!1}:"values"==t?{value:n[o],done:!1}:{value:[o,n[o]],done:!1}}),"values"),a.Arguments=a.Array,r("keys"),r("values"),r("entries")},function(e,n,t){var o=t(5),r=t(8),a=t(6),s=t(54);e.exports=o?Object.defineProperties:function(e,n){a(e);for(var t,o=s(n),i=o.length,c=0;i>c;)r.f(e,t=o[c++],n[t]);return e}},function(e,n,t){var o=t(88),r=t(24),a=t(3)("toStringTag"),s="Arguments"==r(function(){return arguments}());e.exports=o?r:function(e){var n,t,o;return void 0===e?"Undefined":null===e?"Null":"string"==typeof(t=function(e,n){try{return e[n]}catch(e){}}(n=Object(e),a))?t:s?r(n):"Object"==(o=r(n))&&"function"==typeof n.callee?"Arguments":o}},function(e,n,t){var o=t(6),r=t(25),a=t(3)("species");e.exports=function(e,n){var t,s=o(e).constructor;return void 0===s||null==(t=o(s)[a])?n:r(t)}},function(e,n,t){var o=t(4),r=t(36),a=t(3)("species");e.exports=function(e,n){var t;return r(e)&&("function"!=typeof(t=e.constructor)||t!==Array&&!r(t.prototype)?o(t)&&null===(t=t[a])&&(t=void 0):t=void 0),new(void 0===t?Array:t)(0===n?0:n)}},function(e,n,t){var o=t(46),r=t(21),a=function(e){return function(n,t){var a,s,i=String(r(n)),c=o(t),l=i.length;return c<0||c>=l?e?"":void 0:(a=i.charCodeAt(c))<55296||a>56319||c+1===l||(s=i.charCodeAt(c+1))<56320||s>57343?e?i.charAt(c):a:e?i.slice(c,c+2):s-56320+(a-55296<<10)+65536}};e.exports={codeAt:a(!1),charAt:a(!0)}},function(e,n,t){"use strict";var o=t(6);e.exports=function(){var e=o(this),n="";return e.global&&(n+="g"),e.ignoreCase&&(n+="i"),e.multiline&&(n+="m"),e.dotAll&&(n+="s"),e.unicode&&(n+="u"),e.sticky&&(n+="y"),n}},function(e,n,t){var o=t(9),r=Date.prototype,a=r.toString,s=r.getTime;new Date(NaN)+""!="Invalid Date"&&o(r,"toString",(function(){var e=s.call(this);return e==e?a.call(this):"Invalid Date"}))},function(e,n){e.exports=function(e){return e.webpackPolyfill||(e.deprecate=function(){},e.paths=[],e.children||(e.children=[]),Object.defineProperty(e,"loaded",{enumerable:!0,get:function(){return e.l}}),Object.defineProperty(e,"id",{enumerable:!0,get:function(){return e.i}}),e.webpackPolyfill=1),e}},function(e,n,t){var o=t(21),r="["+t(119)+"]",a=RegExp("^"+r+r+"*"),s=RegExp(r+r+"*$"),i=function(e){return function(n){var t=String(o(n));return 1&e&&(t=t.replace(a,"")),2&e&&(t=t.replace(s,"")),t}};e.exports={start:i(1),end:i(2),trim:i(3)}},function(e,n){e.exports="\t\n\v\f\r                　\u2028\u2029\ufeff"},function(e,n,t){"use strict";t.d(n,"a",(function(){return a}));t(42),t(22),t(50),t(312),t(101),t(313),t(143),t(73),t(51);function o(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);n&&(o=o.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,o)}return t}function a(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){o(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}},function(e,n,t){var o=t(5),r=t(1),a=t(79);e.exports=!o&&!r((function(){return 7!=Object.defineProperty(a("div"),"a",{get:function(){return 7}}).a}))},function(e,n,t){var o=t(80);e.exports=o&&!Symbol.sham&&"symbol"==typeof Symbol.iterator},function(e,n,t){var o=t(7),r=t(11),a=t(81).indexOf,s=t(40);e.exports=function(e,n){var t,i=r(e),c=0,l=[];for(t in i)!o(s,t)&&o(i,t)&&l.push(t);for(;n.length>c;)o(i,t=n[c++])&&(~a(l,t)||l.push(t));return l}},function(e,n,t){var o=t(46),r=Math.max,a=Math.min;e.exports=function(e,n){var t=o(e);return t<0?r(t+n,0):a(t,n)}},function(e,n,t){var o=t(19);e.exports=o("document","documentElement")},function(e,n,t){var o=t(2);e.exports=o},function(e,n,t){"use strict";var o=t(0),r=t(186),a=t(86),s=t(87),i=t(47),c=t(13),l=t(9),u=t(3),d=t(23),p=t(41),m=t(130),f=m.IteratorPrototype,h=m.BUGGY_SAFARI_ITERATORS,g=u("iterator"),v=function(){return this};e.exports=function(e,n,t,u,m,b,_){r(t,n,u);var w,y,k,x=function(e){if(e===m&&I)return I;if(!h&&e in C)return C[e];switch(e){case"keys":case"values":case"entries":return function(){return new t(this,e)}}return function(){return new t(this)}},P=n+" Iterator",S=!1,C=e.prototype,E=C[g]||C["@@iterator"]||m&&C[m],I=!h&&E||x(m),T="Array"==n&&C.entries||E;if(T&&(w=a(T.call(new e)),f!==Object.prototype&&w.next&&(d||a(w)===f||(s?s(w,f):"function"!=typeof w[g]&&c(w,g,v)),i(w,P,!0,!0),d&&(p[P]=v))),"values"==m&&E&&"values"!==E.name&&(S=!0,I=function(){return E.call(this)}),d&&!_||C[g]===I||c(C,g,I),p[n]=I,m)if(y={values:x("values"),keys:b?I:x("keys"),entries:x("entries")},_)for(k in y)(h||S||!(k in C))&&l(C,k,y[k]);else o({target:n,proto:!0,forced:h||S},y);return y}},function(e,n,t){var o=t(7),r=t(129),a=t(20),s=t(8);e.exports=function(e,n){for(var t=r(n),i=s.f,c=a.f,l=0;l<t.length;l++){var u=t[l];o(e,u)||i(e,u,c(n,u))}}},function(e,n,t){var o=t(19),r=t(52),a=t(85),s=t(6);e.exports=o("Reflect","ownKeys")||function(e){var n=r.f(s(e)),t=a.f;return t?n.concat(t(e)):n}},function(e,n,t){"use strict";var o,r,a,s=t(86),i=t(13),c=t(7),l=t(3),u=t(23),d=l("iterator"),p=!1;[].keys&&("next"in(a=[].keys())?(r=s(s(a)))!==Object.prototype&&(o=r):p=!0),null==o&&(o={}),u||c(o,d)||i(o,d,(function(){return this})),e.exports={IteratorPrototype:o,BUGGY_SAFARI_ITERATORS:p}},function(e,n,t){var o=t(1);e.exports=!o((function(){function e(){}return e.prototype.constructor=null,Object.getPrototypeOf(new e)!==e.prototype}))},function(e,n,t){var o=t(2);e.exports=o.Promise},function(e,n,t){var o=t(3),r=t(41),a=o("iterator"),s=Array.prototype;e.exports=function(e){return void 0!==e&&(r.Array===e||s[a]===e)}},function(e,n,t){var o=t(6);e.exports=function(e){var n=e.return;if(void 0!==n)return o(n.call(e)).value}},function(e,n,t){var o=t(3)("iterator"),r=!1;try{var a=0,s={next:function(){return{done:!!a++}},return:function(){r=!0}};s[o]=function(){return this},Array.from(s,(function(){throw 2}))}catch(e){}e.exports=function(e,n){if(!n&&!r)return!1;var t=!1;try{var a={};a[o]=function(){return{next:function(){return{done:t=!0}}}},e(a)}catch(e){}return t}},function(e,n,t){var o,r,a,s=t(2),i=t(1),c=t(48),l=t(125),u=t(79),d=t(137),p=t(57),m=s.location,f=s.setImmediate,h=s.clearImmediate,g=s.process,v=s.MessageChannel,b=s.Dispatch,_=0,w={},y=function(e){if(w.hasOwnProperty(e)){var n=w[e];delete w[e],n()}},k=function(e){return function(){y(e)}},x=function(e){y(e.data)},P=function(e){s.postMessage(e+"",m.protocol+"//"+m.host)};f&&h||(f=function(e){for(var n=[],t=1;arguments.length>t;)n.push(arguments[t++]);return w[++_]=function(){("function"==typeof e?e:Function(e)).apply(void 0,n)},o(_),_},h=function(e){delete w[e]},p?o=function(e){g.nextTick(k(e))}:b&&b.now?o=function(e){b.now(k(e))}:v&&!d?(a=(r=new v).port2,r.port1.onmessage=x,o=c(a.postMessage,a,1)):s.addEventListener&&"function"==typeof postMessage&&!s.importScripts&&m&&"file:"!==m.protocol&&!i(P)?(o=P,s.addEventListener("message",x,!1)):o="onreadystatechange"in u("script")?function(e){l.appendChild(u("script")).onreadystatechange=function(){l.removeChild(this),y(e)}}:function(e){setTimeout(k(e),0)}),e.exports={set:f,clear:h}},function(e,n,t){var o=t(56);e.exports=/(iphone|ipod|ipad).*applewebkit/i.test(o)},function(e,n,t){var o=t(6),r=t(4),a=t(139);e.exports=function(e,n){if(o(e),r(n)&&n.constructor===e)return n;var t=a.f(e);return(0,t.resolve)(n),t.promise}},function(e,n,t){"use strict";var o=t(25),r=function(e){var n,t;this.promise=new e((function(e,o){if(void 0!==n||void 0!==t)throw TypeError("Bad Promise constructor");n=e,t=o})),this.resolve=o(n),this.reject=o(t)};e.exports.f=function(e){return new r(e)}},function(e,n){e.exports={CSSRuleList:0,CSSStyleDeclaration:0,CSSValueList:0,ClientRectList:0,DOMRectList:0,DOMStringList:0,DOMTokenList:1,DataTransferItemList:0,FileList:0,HTMLAllCollection:0,HTMLCollection:0,HTMLFormElement:0,HTMLSelectElement:0,MediaList:0,MimeTypeArray:0,NamedNodeMap:0,NodeList:1,PaintRequestList:0,Plugin:0,PluginArray:0,SVGLengthList:0,SVGNumberList:0,SVGPathSegList:0,SVGPointList:0,SVGStringList:0,SVGTransformList:0,SourceBufferList:0,StyleSheetList:0,TextTrackCueList:0,TextTrackList:0,TouchList:0}},function(e,n,t){"use strict";var o=t(32).forEach,r=t(34),a=t(18),s=r("forEach"),i=a("forEach");e.exports=s&&i?[].forEach:function(e){return o(this,e,arguments.length>1?arguments[1]:void 0)}},function(e,n,t){var o=t(1);e.exports=!o((function(){return Object.isExtensible(Object.preventExtensions({}))}))},function(e,n,t){var o=t(0),r=t(5),a=t(129),s=t(11),i=t(20),c=t(60);o({target:"Object",stat:!0,sham:!r},{getOwnPropertyDescriptors:function(e){for(var n,t,o=s(e),r=i.f,l=a(o),u={},d=0;l.length>d;)void 0!==(t=r(o,n=l[d++]))&&c(u,n,t);return u}})},function(e,n,t){var o=t(0),r=t(1),a=t(12),s=t(86),i=t(131);o({target:"Object",stat:!0,forced:r((function(){s(1)})),sham:!i},{getPrototypeOf:function(e){return s(a(e))}})},function(e,n,t){var o=t(183);e.exports=function(e){if(o(e))throw TypeError("The method doesn't accept regular expressions");return e}},function(e,n,t){var o=t(3)("match");e.exports=function(e){var n=/./;try{"/./"[e](n)}catch(t){try{return n[o]=!1,"/./"[e](n)}catch(e){}}return!1}},function(e,n,t){t(0)({target:"Object",stat:!0,sham:!t(5)},{create:t(28)})},function(e,n,t){var o=t(3);n.f=o},function(e,n,t){var o=t(126),r=t(7),a=t(148),s=t(8).f;e.exports=function(e){var n=o.Symbol||(o.Symbol={});r(n,e)||s(n,e,{value:a.f(e)})}},function(e,n,t){var o=t(0),r=t(193);o({target:"Array",stat:!0,forced:!t(135)((function(e){Array.from(e)}))},{from:r})},function(e,n){e.exports=function(e,n){for(var t=-1,o=n.length,r=e.length;++t<o;)e[r+t]=n[t];return e}},function(e,n){var t="object"==typeof global&&global&&global.Object===Object&&global;e.exports=t},function(e,n,t){var o=t(63),r=t(230),a=t(231),s=t(232),i=t(233),c=t(234);function l(e){var n=this.__data__=new o(e);this.size=n.size}l.prototype.clear=r,l.prototype.delete=a,l.prototype.get=s,l.prototype.has=i,l.prototype.set=c,e.exports=l},function(e,n){e.exports=function(e,n){return e===n||e!=e&&n!=n}},function(e,n,t){var o=t(37),r=t(93);e.exports=function(e){if(!r(e))return!1;var n=o(e);return"[object Function]"==n||"[object GeneratorFunction]"==n||"[object AsyncFunction]"==n||"[object Proxy]"==n}},function(e,n){var t=Function.prototype.toString;e.exports=function(e){if(null!=e){try{return t.call(e)}catch(e){}try{return e+""}catch(e){}}return""}},function(e,n,t){var o=t(251),r=t(29);e.exports=function e(n,t,a,s,i){return n===t||(null==n||null==t||!r(n)&&!r(t)?n!=n&&t!=t:o(n,t,a,s,e,i))}},function(e,n,t){var o=t(159),r=t(254),a=t(160);e.exports=function(e,n,t,s,i,c){var l=1&t,u=e.length,d=n.length;if(u!=d&&!(l&&d>u))return!1;var p=c.get(e),m=c.get(n);if(p&&m)return p==n&&m==e;var f=-1,h=!0,g=2&t?new o:void 0;for(c.set(e,n),c.set(n,e);++f<u;){var v=e[f],b=n[f];if(s)var _=l?s(b,v,f,n,e,c):s(v,b,f,e,n,c);if(void 0!==_){if(_)continue;h=!1;break}if(g){if(!r(n,(function(e,n){if(!a(g,n)&&(v===e||i(v,e,t,s,c)))return g.push(n)}))){h=!1;break}}else if(v!==b&&!i(v,b,t,s,c)){h=!1;break}}return c.delete(e),c.delete(n),h}},function(e,n,t){var o=t(94),r=t(252),a=t(253);function s(e){var n=-1,t=null==e?0:e.length;for(this.__data__=new o;++n<t;)this.add(e[n])}s.prototype.add=s.prototype.push=r,s.prototype.has=a,e.exports=s},function(e,n){e.exports=function(e,n){return e.has(n)}},function(e,n,t){var o=t(264),r=t(270),a=t(165);e.exports=function(e){return a(e)?o(e):r(e)}},function(e,n,t){(function(e){var o=t(16),r=t(266),a=n&&!n.nodeType&&n,s=a&&"object"==typeof e&&e&&!e.nodeType&&e,i=s&&s.exports===a?o.Buffer:void 0,c=(i?i.isBuffer:void 0)||r;e.exports=c}).call(this,t(117)(e))},function(e,n){var t=/^(?:0|[1-9]\d*)$/;e.exports=function(e,n){var o=typeof e;return!!(n=null==n?9007199254740991:n)&&("number"==o||"symbol"!=o&&t.test(e))&&e>-1&&e%1==0&&e<n}},function(e,n,t){var o=t(267),r=t(268),a=t(269),s=a&&a.isTypedArray,i=s?r(s):o;e.exports=i},function(e,n,t){var o=t(155),r=t(96);e.exports=function(e){return null!=e&&r(e.length)&&!o(e)}},function(e,n,t){var o=t(26)(t(16),"Set");e.exports=o},function(e,n,t){var o=t(93);e.exports=function(e){return e==e&&!o(e)}},function(e,n){e.exports=function(e,n){return function(t){return null!=t&&(t[e]===n&&(void 0!==n||e in Object(t)))}}},function(e,n,t){var o=t(170),r=t(67);e.exports=function(e,n){for(var t=0,a=(n=o(n,e)).length;null!=e&&t<a;)e=e[r(n[t++])];return t&&t==a?e:void 0}},function(e,n,t){var o=t(15),r=t(97),a=t(281),s=t(284);e.exports=function(e,n){return o(e)?e:r(e,n)?[e]:a(s(e))}},function(e,n,t){},function(e,n,t){},function(e,n,t){t(0)({target:"Object",stat:!0},{setPrototypeOf:t(87)})},function(e,n,t){var o=t(0),r=t(19),a=t(25),s=t(6),i=t(4),c=t(28),l=t(321),u=t(1),d=r("Reflect","construct"),p=u((function(){function e(){}return!(d((function(){}),[],e)instanceof e)})),m=!u((function(){d((function(){}))})),f=p||m;o({target:"Reflect",stat:!0,forced:f,sham:f},{construct:function(e,n){a(e),s(n);var t=arguments.length<3?e:a(arguments[2]);if(m&&!p)return d(e,n,t);if(e==t){switch(n.length){case 0:return new e;case 1:return new e(n[0]);case 2:return new e(n[0],n[1]);case 3:return new e(n[0],n[1],n[2]);case 4:return new e(n[0],n[1],n[2],n[3])}var o=[null];return o.push.apply(o,n),new(l.apply(e,o))}var r=t.prototype,u=c(i(r)?r:Object.prototype),f=Function.apply.call(e,u,n);return i(f)?f:u}})},function(e,n,t){},function(e,n,t){},function(e,n,t){var o=t(217),r=t(222),a=t(293),s=t(301),i=t(310),c=t(194),l=a((function(e){var n=c(e);return i(n)&&(n=void 0),s(o(e,1,i,!0),r(n,2))}));e.exports=l},function(e,n,t){"use strict";var o=t(0),r=t(32).some,a=t(34),s=t(18),i=a("some"),c=s("some");o({target:"Array",proto:!0,forced:!i||!c},{some:function(e){return r(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){"use strict";t(30);var o=t(9),r=t(1),a=t(3),s=t(75),i=t(13),c=a("species"),l=!r((function(){var e=/./;return e.exec=function(){var e=[];return e.groups={a:"7"},e},"7"!=="".replace(e,"$<a>")})),u="$0"==="a".replace(/./,"$0"),d=a("replace"),p=!!/./[d]&&""===/./[d]("a","$0"),m=!r((function(){var e=/(?:)/,n=e.exec;e.exec=function(){return n.apply(this,arguments)};var t="ab".split(e);return 2!==t.length||"a"!==t[0]||"b"!==t[1]}));e.exports=function(e,n,t,d){var f=a(e),h=!r((function(){var n={};return n[f]=function(){return 7},7!=""[e](n)})),g=h&&!r((function(){var n=!1,t=/a/;return"split"===e&&((t={}).constructor={},t.constructor[c]=function(){return t},t.flags="",t[f]=/./[f]),t.exec=function(){return n=!0,null},t[f](""),!n}));if(!h||!g||"replace"===e&&(!l||!u||p)||"split"===e&&!m){var v=/./[f],b=t(f,""[e],(function(e,n,t,o,r){return n.exec===s?h&&!r?{done:!0,value:v.call(n,t,o)}:{done:!0,value:e.call(t,n,o)}:{done:!1}}),{REPLACE_KEEPS_$0:u,REGEXP_REPLACE_SUBSTITUTES_UNDEFINED_CAPTURE:p}),_=b[0],w=b[1];o(String.prototype,e,_),o(RegExp.prototype,f,2==n?function(e,n){return w.call(e,this,n)}:function(e){return w.call(e,this)})}d&&i(RegExp.prototype[f],"sham",!0)}},function(e,n,t){var o=t(24),r=t(75);e.exports=function(e,n){var t=e.exec;if("function"==typeof t){var a=t.call(e,n);if("object"!=typeof a)throw TypeError("RegExp exec method returned something other than an Object or null");return a}if("RegExp"!==o(e))throw TypeError("RegExp#exec called on incompatible receiver");return r.call(e,n)}},function(e,n,t){"use strict";var o=t(0),r=t(81).indexOf,a=t(34),s=t(18),i=[].indexOf,c=!!i&&1/[1].indexOf(1,-0)<0,l=a("indexOf"),u=s("indexOf",{ACCESSORS:!0,1:0});o({target:"Array",proto:!0,forced:c||!l||!u},{indexOf:function(e){return c?i.apply(this,arguments)||0:r(this,e,arguments.length>1?arguments[1]:void 0)}})},function(e,n){e.exports=function(e,n,t){if(!(e instanceof n))throw TypeError("Incorrect "+(t?t+" ":"")+"invocation");return e}},function(e,n,t){var o=t(4),r=t(24),a=t(3)("match");e.exports=function(e){var n;return o(e)&&(void 0!==(n=e[a])?!!n:"RegExp"==r(e))}},function(e,n,t){"use strict";var o=t(114).charAt;e.exports=function(e,n,t){return n+(t?o(e,n).length:1)}},function(e,n,t){"use strict";var o=t(0),r=t(38),a=t(11),s=t(34),i=[].join,c=r!=Object,l=s("join",",");o({target:"Array",proto:!0,forced:c||!l},{join:function(e){return i.call(a(this),void 0===e?",":e)}})},function(e,n,t){"use strict";var o=t(130).IteratorPrototype,r=t(28),a=t(35),s=t(47),i=t(41),c=function(){return this};e.exports=function(e,n,t){var l=n+" Iterator";return e.prototype=r(o,{next:a(1,t)}),s(e,l,!1,!0),i[l]=c,e}},function(e,n,t){var o=t(9);e.exports=function(e,n,t){for(var r in n)o(e,r,n[r],t);return e}},function(e,n,t){"use strict";var o=t(19),r=t(8),a=t(3),s=t(5),i=a("species");e.exports=function(e){var n=o(e),t=r.f;s&&n&&!n[i]&&t(n,i,{configurable:!0,get:function(){return this}})}},function(e,n,t){"use strict";var o=t(5),r=t(1),a=t(54),s=t(85),i=t(84),c=t(12),l=t(38),u=Object.assign,d=Object.defineProperty;e.exports=!u||r((function(){if(o&&1!==u({b:1},u(d({},"a",{enumerable:!0,get:function(){d(this,"b",{value:3,enumerable:!1})}}),{b:2})).b)return!0;var e={},n={},t=Symbol();return e[t]=7,"abcdefghijklmnopqrst".split("").forEach((function(e){n[e]=e})),7!=u({},e)[t]||"abcdefghijklmnopqrst"!=a(u({},n)).join("")}))?function(e,n){for(var t=c(e),r=arguments.length,u=1,d=s.f,p=i.f;r>u;)for(var m,f=l(arguments[u++]),h=d?a(f).concat(d(f)):a(f),g=h.length,v=0;g>v;)m=h[v++],o&&!p.call(f,m)||(t[m]=f[m]);return t}:u},function(e,n,t){"use strict";var o=t(0),r=t(81).includes,a=t(103);o({target:"Array",proto:!0,forced:!t(18)("indexOf",{ACCESSORS:!0,1:0})},{includes:function(e){return r(this,e,arguments.length>1?arguments[1]:void 0)}}),a("includes")},function(e,n,t){"use strict";var o=t(1);function r(e,n){return RegExp(e,n)}n.UNSUPPORTED_Y=o((function(){var e=r("a","y");return e.lastIndex=2,null!=e.exec("abcd")})),n.BROKEN_CARET=o((function(){var e=r("^r","gy");return e.lastIndex=2,null!=e.exec("str")}))},function(e,n,t){"use strict";var o=t(0),r=t(145),a=t(21);o({target:"String",proto:!0,forced:!t(146)("includes")},{includes:function(e){return!!~String(a(this)).indexOf(r(e),arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){"use strict";var o=t(48),r=t(12),a=t(216),s=t(133),i=t(14),c=t(60),l=t(104);e.exports=function(e){var n,t,u,d,p,m,f=r(e),h="function"==typeof this?this:Array,g=arguments.length,v=g>1?arguments[1]:void 0,b=void 0!==v,_=l(f),w=0;if(b&&(v=o(v,g>2?arguments[2]:void 0,2)),null==_||h==Array&&s(_))for(t=new h(n=i(f.length));n>w;w++)m=b?v(f[w],w):f[w],c(t,w,m);else for(p=(d=_.call(f)).next,t=new h;!(u=p.call(d)).done;w++)m=b?a(d,v,[u.value,w],!0):u.value,c(t,w,m);return t.length=w,t}},function(e,n){e.exports=function(e){var n=null==e?0:e.length;return n?e[n-1]:void 0}},function(e,n,t){var o=t(0),r=t(314);o({global:!0,forced:parseInt!=r},{parseInt:r})},function(e,n,t){var o=t(4),r=t(87);e.exports=function(e,n,t){var a,s;return r&&"function"==typeof(a=n.constructor)&&a!==t&&o(s=a.prototype)&&s!==t.prototype&&r(e,s),e}},function(e,n,t){e.exports=t(324)},function(e,n,t){var o=t(2),r=t(83),a=o.WeakMap;e.exports="function"==typeof a&&/native code/.test(r(a))},function(e,n,t){var o=t(4);e.exports=function(e){if(!o(e)&&null!==e)throw TypeError("Can't set "+String(e)+" as a prototype");return e}},function(e,n,t){"use strict";var o,r,a,s,i=t(0),c=t(23),l=t(2),u=t(19),d=t(132),p=t(9),m=t(187),f=t(47),h=t(188),g=t(4),v=t(25),b=t(182),_=t(83),w=t(201),y=t(135),k=t(112),x=t(136).set,P=t(202),S=t(138),C=t(204),E=t(139),I=t(205),T=t(31),U=t(74),z=t(3),O=t(57),A=t(58),D=z("species"),M="Promise",j=T.get,L=T.set,B=T.getterFor(M),R=d,N=l.TypeError,F=l.document,G=l.process,$=u("fetch"),q=E.f,V=q,W=!!(F&&F.createEvent&&l.dispatchEvent),H="function"==typeof PromiseRejectionEvent,K=U(M,(function(){if(!(_(R)!==String(R))){if(66===A)return!0;if(!O&&!H)return!0}if(c&&!R.prototype.finally)return!0;if(A>=51&&/native code/.test(R))return!1;var e=R.resolve(1),n=function(e){e((function(){}),(function(){}))};return(e.constructor={})[D]=n,!(e.then((function(){}))instanceof n)})),J=K||!y((function(e){R.all(e).catch((function(){}))})),Y=function(e){var n;return!(!g(e)||"function"!=typeof(n=e.then))&&n},X=function(e,n){if(!e.notified){e.notified=!0;var t=e.reactions;P((function(){for(var o=e.value,r=1==e.state,a=0;t.length>a;){var s,i,c,l=t[a++],u=r?l.ok:l.fail,d=l.resolve,p=l.reject,m=l.domain;try{u?(r||(2===e.rejection&&ne(e),e.rejection=1),!0===u?s=o:(m&&m.enter(),s=u(o),m&&(m.exit(),c=!0)),s===l.promise?p(N("Promise-chain cycle")):(i=Y(s))?i.call(s,d,p):d(s)):p(o)}catch(e){m&&!c&&m.exit(),p(e)}}e.reactions=[],e.notified=!1,n&&!e.rejection&&Z(e)}))}},Q=function(e,n,t){var o,r;W?((o=F.createEvent("Event")).promise=n,o.reason=t,o.initEvent(e,!1,!0),l.dispatchEvent(o)):o={promise:n,reason:t},!H&&(r=l["on"+e])?r(o):"unhandledrejection"===e&&C("Unhandled promise rejection",t)},Z=function(e){x.call(l,(function(){var n,t=e.facade,o=e.value;if(ee(e)&&(n=I((function(){O?G.emit("unhandledRejection",o,t):Q("unhandledrejection",t,o)})),e.rejection=O||ee(e)?2:1,n.error))throw n.value}))},ee=function(e){return 1!==e.rejection&&!e.parent},ne=function(e){x.call(l,(function(){var n=e.facade;O?G.emit("rejectionHandled",n):Q("rejectionhandled",n,e.value)}))},te=function(e,n,t){return function(o){e(n,o,t)}},oe=function(e,n,t){e.done||(e.done=!0,t&&(e=t),e.value=n,e.state=2,X(e,!0))},re=function(e,n,t){if(!e.done){e.done=!0,t&&(e=t);try{if(e.facade===n)throw N("Promise can't be resolved itself");var o=Y(n);o?P((function(){var t={done:!1};try{o.call(n,te(re,t,e),te(oe,t,e))}catch(n){oe(t,n,e)}})):(e.value=n,e.state=1,X(e,!1))}catch(n){oe({done:!1},n,e)}}};K&&(R=function(e){b(this,R,M),v(e),o.call(this);var n=j(this);try{e(te(re,n),te(oe,n))}catch(e){oe(n,e)}},(o=function(e){L(this,{type:M,done:!1,notified:!1,parent:!1,reactions:[],rejection:!1,state:0,value:void 0})}).prototype=m(R.prototype,{then:function(e,n){var t=B(this),o=q(k(this,R));return o.ok="function"!=typeof e||e,o.fail="function"==typeof n&&n,o.domain=O?G.domain:void 0,t.parent=!0,t.reactions.push(o),0!=t.state&&X(t,!1),o.promise},catch:function(e){return this.then(void 0,e)}}),r=function(){var e=new o,n=j(e);this.promise=e,this.resolve=te(re,n),this.reject=te(oe,n)},E.f=q=function(e){return e===R||e===a?new r(e):V(e)},c||"function"!=typeof d||(s=d.prototype.then,p(d.prototype,"then",(function(e,n){var t=this;return new R((function(e,n){s.call(t,e,n)})).then(e,n)}),{unsafe:!0}),"function"==typeof $&&i({global:!0,enumerable:!0,forced:!0},{fetch:function(e){return S(R,$.apply(l,arguments))}}))),i({global:!0,wrap:!0,forced:K},{Promise:R}),f(R,M,!1,!0),h(M),a=u(M),i({target:M,stat:!0,forced:K},{reject:function(e){var n=q(this);return n.reject.call(void 0,e),n.promise}}),i({target:M,stat:!0,forced:c||K},{resolve:function(e){return S(c&&this===a?R:this,e)}}),i({target:M,stat:!0,forced:J},{all:function(e){var n=this,t=q(n),o=t.resolve,r=t.reject,a=I((function(){var t=v(n.resolve),a=[],s=0,i=1;w(e,(function(e){var c=s++,l=!1;a.push(void 0),i++,t.call(n,e).then((function(e){l||(l=!0,a[c]=e,--i||o(a))}),r)})),--i||o(a)}));return a.error&&r(a.value),t.promise},race:function(e){var n=this,t=q(n),o=t.reject,r=I((function(){var r=v(n.resolve);w(e,(function(e){r.call(n,e).then(t.resolve,o)}))}));return r.error&&o(r.value),t.promise}})},function(e,n,t){var o=t(6),r=t(133),a=t(14),s=t(48),i=t(104),c=t(134),l=function(e,n){this.stopped=e,this.result=n};e.exports=function(e,n,t){var u,d,p,m,f,h,g,v=t&&t.that,b=!(!t||!t.AS_ENTRIES),_=!(!t||!t.IS_ITERATOR),w=!(!t||!t.INTERRUPTED),y=s(n,v,1+b+w),k=function(e){return u&&c(u),new l(!0,e)},x=function(e){return b?(o(e),w?y(e[0],e[1],k):y(e[0],e[1])):w?y(e,k):y(e)};if(_)u=e;else{if("function"!=typeof(d=i(e)))throw TypeError("Target is not iterable");if(r(d)){for(p=0,m=a(e.length);m>p;p++)if((f=x(e[p]))&&f instanceof l)return f;return new l(!1)}u=d.call(e)}for(h=u.next;!(g=h.call(u)).done;){try{f=x(g.value)}catch(e){throw c(u),e}if("object"==typeof f&&f&&f instanceof l)return f}return new l(!1)}},function(e,n,t){var o,r,a,s,i,c,l,u,d=t(2),p=t(20).f,m=t(136).set,f=t(137),h=t(203),g=t(57),v=d.MutationObserver||d.WebKitMutationObserver,b=d.document,_=d.process,w=d.Promise,y=p(d,"queueMicrotask"),k=y&&y.value;k||(o=function(){var e,n;for(g&&(e=_.domain)&&e.exit();r;){n=r.fn,r=r.next;try{n()}catch(e){throw r?s():a=void 0,e}}a=void 0,e&&e.enter()},f||g||h||!v||!b?w&&w.resolve?(l=w.resolve(void 0),u=l.then,s=function(){u.call(l,o)}):s=g?function(){_.nextTick(o)}:function(){m.call(d,o)}:(i=!0,c=b.createTextNode(""),new v(o).observe(c,{characterData:!0}),s=function(){c.data=i=!i})),e.exports=k||function(e){var n={fn:e,next:void 0};a&&(a.next=n),r||(r=n,s()),a=n}},function(e,n,t){var o=t(56);e.exports=/web0s(?!.*chrome)/i.test(o)},function(e,n,t){var o=t(2);e.exports=function(e,n){var t=o.console;t&&t.error&&(1===arguments.length?t.error(e):t.error(e,n))}},function(e,n){e.exports=function(e){try{return{error:!1,value:e()}}catch(e){return{error:!0,value:e}}}},function(e,n,t){var o=t(0),r=t(189);o({target:"Object",stat:!0,forced:Object.assign!==r},{assign:r})},function(e,n,t){"use strict";var o=t(0),r=t(23),a=t(132),s=t(1),i=t(19),c=t(112),l=t(138),u=t(9);o({target:"Promise",proto:!0,real:!0,forced:!!a&&s((function(){a.prototype.finally.call({then:function(){}},(function(){}))}))},{finally:function(e){var n=c(this,i("Promise")),t="function"==typeof e;return this.then(t?function(t){return l(n,e()).then((function(){return t}))}:e,t?function(t){return l(n,e()).then((function(){throw t}))}:e)}}),r||"function"!=typeof a||a.prototype.finally||u(a.prototype,"finally",i("Promise").prototype.finally)},function(e,n,t){"use strict";var o=t(88),r=t(111);e.exports=o?{}.toString:function(){return"[object "+r(this)+"]"}},function(e,n,t){"use strict";var o=t(0),r=t(210).left,a=t(34),s=t(18),i=t(58),c=t(57),l=a("reduce"),u=s("reduce",{1:0});o({target:"Array",proto:!0,forced:!l||!u||!c&&i>79&&i<83},{reduce:function(e){return r(this,e,arguments.length,arguments.length>1?arguments[1]:void 0)}})},function(e,n,t){var o=t(25),r=t(12),a=t(38),s=t(14),i=function(e){return function(n,t,i,c){o(t);var l=r(n),u=a(l),d=s(l.length),p=e?d-1:0,m=e?-1:1;if(i<2)for(;;){if(p in u){c=u[p],p+=m;break}if(p+=m,e?p<0:d<=p)throw TypeError("Reduce of empty array with no initial value")}for(;e?p>=0:d>p;p+=m)p in u&&(c=t(c,u[p],p,l));return c}};e.exports={left:i(!1),right:i(!0)}},function(e,n,t){var o=t(0),r=t(142),a=t(1),s=t(4),i=t(212).onFreeze,c=Object.freeze;o({target:"Object",stat:!0,forced:a((function(){c(1)})),sham:!r},{freeze:function(e){return c&&s(e)?c(i(e)):e}})},function(e,n,t){var o=t(40),r=t(4),a=t(7),s=t(8).f,i=t(53),c=t(142),l=i("meta"),u=0,d=Object.isExtensible||function(){return!0},p=function(e){s(e,l,{value:{objectID:"O"+ ++u,weakData:{}}})},m=e.exports={REQUIRED:!1,fastKey:function(e,n){if(!r(e))return"symbol"==typeof e?e:("string"==typeof e?"S":"P")+e;if(!a(e,l)){if(!d(e))return"F";if(!n)return"E";p(e)}return e[l].objectID},getWeakData:function(e,n){if(!a(e,l)){if(!d(e))return!0;if(!n)return!1;p(e)}return e[l].weakData},onFreeze:function(e){return c&&m.REQUIRED&&d(e)&&!a(e,l)&&p(e),e}};o[l]=!0},function(e,n,t){"use strict";var o,r=t(0),a=t(20).f,s=t(14),i=t(145),c=t(21),l=t(146),u=t(23),d="".startsWith,p=Math.min,m=l("startsWith");r({target:"String",proto:!0,forced:!!(u||m||(o=a(String.prototype,"startsWith"),!o||o.writable))&&!m},{startsWith:function(e){var n=String(c(this));i(e);var t=s(p(arguments.length>1?arguments[1]:void 0,n.length)),o=String(e);return d?d.call(n,o,t):n.slice(t,t+o.length)===o}})},function(e,n,t){var o=t(12),r=Math.floor,a="".replace,s=/\$([$&'`]|\d\d?|<[^>]*>)/g,i=/\$([$&'`]|\d\d?)/g;e.exports=function(e,n,t,c,l,u){var d=t+e.length,p=c.length,m=i;return void 0!==l&&(l=o(l),m=s),a.call(u,m,(function(o,a){var s;switch(a.charAt(0)){case"$":return"$";case"&":return e;case"`":return n.slice(0,t);case"'":return n.slice(d);case"<":s=l[a.slice(1,-1)];break;default:var i=+a;if(0===i)return o;if(i>p){var u=r(i/10);return 0===u?o:u<=p?void 0===c[u-1]?a.charAt(1):c[u-1]+a.charAt(1):o}s=c[i-1]}return void 0===s?"":s}))}},function(e,n,t){var o=t(11),r=t(52).f,a={}.toString,s="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[];e.exports.f=function(e){return s&&"[object Window]"==a.call(e)?function(e){try{return r(e)}catch(e){return s.slice()}}(e):r(o(e))}},function(e,n,t){var o=t(6),r=t(134);e.exports=function(e,n,t,a){try{return a?n(o(t)[0],t[1]):n(t)}catch(n){throw r(e),n}}},function(e,n,t){var o=t(151),r=t(218);e.exports=function e(n,t,a,s,i){var c=-1,l=n.length;for(a||(a=r),i||(i=[]);++c<l;){var u=n[c];t>0&&a(u)?t>1?e(u,t-1,a,s,i):o(i,u):s||(i[i.length]=u)}return i}},function(e,n,t){var o=t(43),r=t(91),a=t(15),s=o?o.isConcatSpreadable:void 0;e.exports=function(e){return a(e)||r(e)||!!(s&&e&&e[s])}},function(e,n,t){var o=t(37),r=t(29);e.exports=function(e){return r(e)&&"[object Arguments]"==o(e)}},function(e,n,t){var o=t(43),r=Object.prototype,a=r.hasOwnProperty,s=r.toString,i=o?o.toStringTag:void 0;e.exports=function(e){var n=a.call(e,i),t=e[i];try{e[i]=void 0;var o=!0}catch(e){}var r=s.call(e);return o&&(n?e[i]=t:delete e[i]),r}},function(e,n){var t=Object.prototype.toString;e.exports=function(e){return t.call(e)}},function(e,n,t){var o=t(223),r=t(279),a=t(99),s=t(15),i=t(290);e.exports=function(e){return"function"==typeof e?e:null==e?a:"object"==typeof e?s(e)?r(e[0],e[1]):o(e):i(e)}},function(e,n,t){var o=t(224),r=t(278),a=t(168);e.exports=function(e){var n=r(e);return 1==n.length&&n[0][2]?a(n[0][0],n[0][1]):function(t){return t===e||o(t,e,n)}}},function(e,n,t){var o=t(153),r=t(157);e.exports=function(e,n,t,a){var s=t.length,i=s,c=!a;if(null==e)return!i;for(e=Object(e);s--;){var l=t[s];if(c&&l[2]?l[1]!==e[l[0]]:!(l[0]in e))return!1}for(;++s<i;){var u=(l=t[s])[0],d=e[u],p=l[1];if(c&&l[2]){if(void 0===d&&!(u in e))return!1}else{var m=new o;if(a)var f=a(d,p,u,e,n,m);if(!(void 0===f?r(p,d,3,a,m):f))return!1}}return!0}},function(e,n){e.exports=function(){this.__data__=[],this.size=0}},function(e,n,t){var o=t(64),r=Array.prototype.splice;e.exports=function(e){var n=this.__data__,t=o(n,e);return!(t<0)&&(t==n.length-1?n.pop():r.call(n,t,1),--this.size,!0)}},function(e,n,t){var o=t(64);e.exports=function(e){var n=this.__data__,t=o(n,e);return t<0?void 0:n[t][1]}},function(e,n,t){var o=t(64);e.exports=function(e){return o(this.__data__,e)>-1}},function(e,n,t){var o=t(64);e.exports=function(e,n){var t=this.__data__,r=o(t,e);return r<0?(++this.size,t.push([e,n])):t[r][1]=n,this}},function(e,n,t){var o=t(63);e.exports=function(){this.__data__=new o,this.size=0}},function(e,n){e.exports=function(e){var n=this.__data__,t=n.delete(e);return this.size=n.size,t}},function(e,n){e.exports=function(e){return this.__data__.get(e)}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n,t){var o=t(63),r=t(92),a=t(94);e.exports=function(e,n){var t=this.__data__;if(t instanceof o){var s=t.__data__;if(!r||s.length<199)return s.push([e,n]),this.size=++t.size,this;t=this.__data__=new a(s)}return t.set(e,n),this.size=t.size,this}},function(e,n,t){var o=t(155),r=t(236),a=t(93),s=t(156),i=/^\[object .+?Constructor\]$/,c=Function.prototype,l=Object.prototype,u=c.toString,d=l.hasOwnProperty,p=RegExp("^"+u.call(d).replace(/[\\^$.*+?()[\]{}|]/g,"\\$&").replace(/hasOwnProperty|(function).*?(?=\\\()| for .+?(?=\\\])/g,"$1.*?")+"$");e.exports=function(e){return!(!a(e)||r(e))&&(o(e)?p:i).test(s(e))}},function(e,n,t){var o,r=t(237),a=(o=/[^.]+$/.exec(r&&r.keys&&r.keys.IE_PROTO||""))?"Symbol(src)_1."+o:"";e.exports=function(e){return!!a&&a in e}},function(e,n,t){var o=t(16)["__core-js_shared__"];e.exports=o},function(e,n){e.exports=function(e,n){return null==e?void 0:e[n]}},function(e,n,t){var o=t(240),r=t(63),a=t(92);e.exports=function(){this.size=0,this.__data__={hash:new o,map:new(a||r),string:new o}}},function(e,n,t){var o=t(241),r=t(242),a=t(243),s=t(244),i=t(245);function c(e){var n=-1,t=null==e?0:e.length;for(this.clear();++n<t;){var o=e[n];this.set(o[0],o[1])}}c.prototype.clear=o,c.prototype.delete=r,c.prototype.get=a,c.prototype.has=s,c.prototype.set=i,e.exports=c},function(e,n,t){var o=t(65);e.exports=function(){this.__data__=o?o(null):{},this.size=0}},function(e,n){e.exports=function(e){var n=this.has(e)&&delete this.__data__[e];return this.size-=n?1:0,n}},function(e,n,t){var o=t(65),r=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;if(o){var t=n[e];return"__lodash_hash_undefined__"===t?void 0:t}return r.call(n,e)?n[e]:void 0}},function(e,n,t){var o=t(65),r=Object.prototype.hasOwnProperty;e.exports=function(e){var n=this.__data__;return o?void 0!==n[e]:r.call(n,e)}},function(e,n,t){var o=t(65);e.exports=function(e,n){var t=this.__data__;return this.size+=this.has(e)?0:1,t[e]=o&&void 0===n?"__lodash_hash_undefined__":n,this}},function(e,n,t){var o=t(66);e.exports=function(e){var n=o(this,e).delete(e);return this.size-=n?1:0,n}},function(e,n){e.exports=function(e){var n=typeof e;return"string"==n||"number"==n||"symbol"==n||"boolean"==n?"__proto__"!==e:null===e}},function(e,n,t){var o=t(66);e.exports=function(e){return o(this,e).get(e)}},function(e,n,t){var o=t(66);e.exports=function(e){return o(this,e).has(e)}},function(e,n,t){var o=t(66);e.exports=function(e,n){var t=o(this,e),r=t.size;return t.set(e,n),this.size+=t.size==r?0:1,this}},function(e,n,t){var o=t(153),r=t(158),a=t(255),s=t(258),i=t(274),c=t(15),l=t(162),u=t(164),d="[object Object]",p=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,m,f,h){var g=c(e),v=c(n),b=g?"[object Array]":i(e),_=v?"[object Array]":i(n),w=(b="[object Arguments]"==b?d:b)==d,y=(_="[object Arguments]"==_?d:_)==d,k=b==_;if(k&&l(e)){if(!l(n))return!1;g=!0,w=!1}if(k&&!w)return h||(h=new o),g||u(e)?r(e,n,t,m,f,h):a(e,n,b,t,m,f,h);if(!(1&t)){var x=w&&p.call(e,"__wrapped__"),P=y&&p.call(n,"__wrapped__");if(x||P){var S=x?e.value():e,C=P?n.value():n;return h||(h=new o),f(S,C,t,m,h)}}return!!k&&(h||(h=new o),s(e,n,t,m,f,h))}},function(e,n){e.exports=function(e){return this.__data__.set(e,"__lodash_hash_undefined__"),this}},function(e,n){e.exports=function(e){return this.__data__.has(e)}},function(e,n){e.exports=function(e,n){for(var t=-1,o=null==e?0:e.length;++t<o;)if(n(e[t],t,e))return!0;return!1}},function(e,n,t){var o=t(43),r=t(256),a=t(154),s=t(158),i=t(257),c=t(95),l=o?o.prototype:void 0,u=l?l.valueOf:void 0;e.exports=function(e,n,t,o,l,d,p){switch(t){case"[object DataView]":if(e.byteLength!=n.byteLength||e.byteOffset!=n.byteOffset)return!1;e=e.buffer,n=n.buffer;case"[object ArrayBuffer]":return!(e.byteLength!=n.byteLength||!d(new r(e),new r(n)));case"[object Boolean]":case"[object Date]":case"[object Number]":return a(+e,+n);case"[object Error]":return e.name==n.name&&e.message==n.message;case"[object RegExp]":case"[object String]":return e==n+"";case"[object Map]":var m=i;case"[object Set]":var f=1&o;if(m||(m=c),e.size!=n.size&&!f)return!1;var h=p.get(e);if(h)return h==n;o|=2,p.set(e,n);var g=s(m(e),m(n),o,l,d,p);return p.delete(e),g;case"[object Symbol]":if(u)return u.call(e)==u.call(n)}return!1}},function(e,n,t){var o=t(16).Uint8Array;e.exports=o},function(e,n){e.exports=function(e){var n=-1,t=Array(e.size);return e.forEach((function(e,o){t[++n]=[o,e]})),t}},function(e,n,t){var o=t(259),r=Object.prototype.hasOwnProperty;e.exports=function(e,n,t,a,s,i){var c=1&t,l=o(e),u=l.length;if(u!=o(n).length&&!c)return!1;for(var d=u;d--;){var p=l[d];if(!(c?p in n:r.call(n,p)))return!1}var m=i.get(e),f=i.get(n);if(m&&f)return m==n&&f==e;var h=!0;i.set(e,n),i.set(n,e);for(var g=c;++d<u;){var v=e[p=l[d]],b=n[p];if(a)var _=c?a(b,v,p,n,e,i):a(v,b,p,e,n,i);if(!(void 0===_?v===b||s(v,b,t,a,i):_)){h=!1;break}g||(g="constructor"==p)}if(h&&!g){var w=e.constructor,y=n.constructor;w==y||!("constructor"in e)||!("constructor"in n)||"function"==typeof w&&w instanceof w&&"function"==typeof y&&y instanceof y||(h=!1)}return i.delete(e),i.delete(n),h}},function(e,n,t){var o=t(260),r=t(261),a=t(161);e.exports=function(e){return o(e,a,r)}},function(e,n,t){var o=t(151),r=t(15);e.exports=function(e,n,t){var a=n(e);return r(e)?a:o(a,t(e))}},function(e,n,t){var o=t(262),r=t(263),a=Object.prototype.propertyIsEnumerable,s=Object.getOwnPropertySymbols,i=s?function(e){return null==e?[]:(e=Object(e),o(s(e),(function(n){return a.call(e,n)})))}:r;e.exports=i},function(e,n){e.exports=function(e,n){for(var t=-1,o=null==e?0:e.length,r=0,a=[];++t<o;){var s=e[t];n(s,t,e)&&(a[r++]=s)}return a}},function(e,n){e.exports=function(){return[]}},function(e,n,t){var o=t(265),r=t(91),a=t(15),s=t(162),i=t(163),c=t(164),l=Object.prototype.hasOwnProperty;e.exports=function(e,n){var t=a(e),u=!t&&r(e),d=!t&&!u&&s(e),p=!t&&!u&&!d&&c(e),m=t||u||d||p,f=m?o(e.length,String):[],h=f.length;for(var g in e)!n&&!l.call(e,g)||m&&("length"==g||d&&("offset"==g||"parent"==g)||p&&("buffer"==g||"byteLength"==g||"byteOffset"==g)||i(g,h))||f.push(g);return f}},function(e,n){e.exports=function(e,n){for(var t=-1,o=Array(e);++t<e;)o[t]=n(t);return o}},function(e,n){e.exports=function(){return!1}},function(e,n,t){var o=t(37),r=t(96),a=t(29),s={};s["[object Float32Array]"]=s["[object Float64Array]"]=s["[object Int8Array]"]=s["[object Int16Array]"]=s["[object Int32Array]"]=s["[object Uint8Array]"]=s["[object Uint8ClampedArray]"]=s["[object Uint16Array]"]=s["[object Uint32Array]"]=!0,s["[object Arguments]"]=s["[object Array]"]=s["[object ArrayBuffer]"]=s["[object Boolean]"]=s["[object DataView]"]=s["[object Date]"]=s["[object Error]"]=s["[object Function]"]=s["[object Map]"]=s["[object Number]"]=s["[object Object]"]=s["[object RegExp]"]=s["[object Set]"]=s["[object String]"]=s["[object WeakMap]"]=!1,e.exports=function(e){return a(e)&&r(e.length)&&!!s[o(e)]}},function(e,n){e.exports=function(e){return function(n){return e(n)}}},function(e,n,t){(function(e){var o=t(152),r=n&&!n.nodeType&&n,a=r&&"object"==typeof e&&e&&!e.nodeType&&e,s=a&&a.exports===r&&o.process,i=function(){try{var e=a&&a.require&&a.require("util").types;return e||s&&s.binding&&s.binding("util")}catch(e){}}();e.exports=i}).call(this,t(117)(e))},function(e,n,t){var o=t(271),r=t(272),a=Object.prototype.hasOwnProperty;e.exports=function(e){if(!o(e))return r(e);var n=[];for(var t in Object(e))a.call(e,t)&&"constructor"!=t&&n.push(t);return n}},function(e,n){var t=Object.prototype;e.exports=function(e){var n=e&&e.constructor;return e===("function"==typeof n&&n.prototype||t)}},function(e,n,t){var o=t(273)(Object.keys,Object);e.exports=o},function(e,n){e.exports=function(e,n){return function(t){return e(n(t))}}},function(e,n,t){var o=t(275),r=t(92),a=t(276),s=t(166),i=t(277),c=t(37),l=t(156),u=l(o),d=l(r),p=l(a),m=l(s),f=l(i),h=c;(o&&"[object DataView]"!=h(new o(new ArrayBuffer(1)))||r&&"[object Map]"!=h(new r)||a&&"[object Promise]"!=h(a.resolve())||s&&"[object Set]"!=h(new s)||i&&"[object WeakMap]"!=h(new i))&&(h=function(e){var n=c(e),t="[object Object]"==n?e.constructor:void 0,o=t?l(t):"";if(o)switch(o){case u:return"[object DataView]";case d:return"[object Map]";case p:return"[object Promise]";case m:return"[object Set]";case f:return"[object WeakMap]"}return n}),e.exports=h},function(e,n,t){var o=t(26)(t(16),"DataView");e.exports=o},function(e,n,t){var o=t(26)(t(16),"Promise");e.exports=o},function(e,n,t){var o=t(26)(t(16),"WeakMap");e.exports=o},function(e,n,t){var o=t(167),r=t(161);e.exports=function(e){for(var n=r(e),t=n.length;t--;){var a=n[t],s=e[a];n[t]=[a,s,o(s)]}return n}},function(e,n,t){var o=t(157),r=t(280),a=t(287),s=t(97),i=t(167),c=t(168),l=t(67);e.exports=function(e,n){return s(e)&&i(n)?c(l(e),n):function(t){var s=r(t,e);return void 0===s&&s===n?a(t,e):o(n,s,3)}}},function(e,n,t){var o=t(169);e.exports=function(e,n,t){var r=null==e?void 0:o(e,n);return void 0===r?t:r}},function(e,n,t){var o=t(282),r=/[^.[\]]+|\[(?:(-?\d+(?:\.\d+)?)|(["'])((?:(?!\2)[^\\]|\\.)*?)\2)\]|(?=(?:\.|\[\])(?:\.|\[\]|$))/g,a=/\\(\\)?/g,s=o((function(e){var n=[];return 46===e.charCodeAt(0)&&n.push(""),e.replace(r,(function(e,t,o,r){n.push(o?r.replace(a,"$1"):t||e)})),n}));e.exports=s},function(e,n,t){var o=t(283);e.exports=function(e){var n=o(e,(function(e){return 500===t.size&&t.clear(),e})),t=n.cache;return n}},function(e,n,t){var o=t(94);function r(e,n){if("function"!=typeof e||null!=n&&"function"!=typeof n)throw new TypeError("Expected a function");var t=function(){var o=arguments,r=n?n.apply(this,o):o[0],a=t.cache;if(a.has(r))return a.get(r);var s=e.apply(this,o);return t.cache=a.set(r,s)||a,s};return t.cache=new(r.Cache||o),t}r.Cache=o,e.exports=r},function(e,n,t){var o=t(285);e.exports=function(e){return null==e?"":o(e)}},function(e,n,t){var o=t(43),r=t(286),a=t(15),s=t(98),i=o?o.prototype:void 0,c=i?i.toString:void 0;e.exports=function e(n){if("string"==typeof n)return n;if(a(n))return r(n,e)+"";if(s(n))return c?c.call(n):"";var t=n+"";return"0"==t&&1/n==-1/0?"-0":t}},function(e,n){e.exports=function(e,n){for(var t=-1,o=null==e?0:e.length,r=Array(o);++t<o;)r[t]=n(e[t],t,e);return r}},function(e,n,t){var o=t(288),r=t(289);e.exports=function(e,n){return null!=e&&r(e,n,o)}},function(e,n){e.exports=function(e,n){return null!=e&&n in Object(e)}},function(e,n,t){var o=t(170),r=t(91),a=t(15),s=t(163),i=t(96),c=t(67);e.exports=function(e,n,t){for(var l=-1,u=(n=o(n,e)).length,d=!1;++l<u;){var p=c(n[l]);if(!(d=null!=e&&t(e,p)))break;e=e[p]}return d||++l!=u?d:!!(u=null==e?0:e.length)&&i(u)&&s(p,u)&&(a(e)||r(e))}},function(e,n,t){var o=t(291),r=t(292),a=t(97),s=t(67);e.exports=function(e){return a(e)?o(s(e)):r(e)}},function(e,n){e.exports=function(e){return function(n){return null==n?void 0:n[e]}}},function(e,n,t){var o=t(169);e.exports=function(e){return function(n){return o(n,e)}}},function(e,n,t){var o=t(99),r=t(294),a=t(296);e.exports=function(e,n){return a(r(e,n,o),e+"")}},function(e,n,t){var o=t(295),r=Math.max;e.exports=function(e,n,t){return n=r(void 0===n?e.length-1:n,0),function(){for(var a=arguments,s=-1,i=r(a.length-n,0),c=Array(i);++s<i;)c[s]=a[n+s];s=-1;for(var l=Array(n+1);++s<n;)l[s]=a[s];return l[n]=t(c),o(e,this,l)}}},function(e,n){e.exports=function(e,n,t){switch(t.length){case 0:return e.call(n);case 1:return e.call(n,t[0]);case 2:return e.call(n,t[0],t[1]);case 3:return e.call(n,t[0],t[1],t[2])}return e.apply(n,t)}},function(e,n,t){var o=t(297),r=t(300)(o);e.exports=r},function(e,n,t){var o=t(298),r=t(299),a=t(99),s=r?function(e,n){return r(e,"toString",{configurable:!0,enumerable:!1,value:o(n),writable:!0})}:a;e.exports=s},function(e,n){e.exports=function(e){return function(){return e}}},function(e,n,t){var o=t(26),r=function(){try{var e=o(Object,"defineProperty");return e({},"",{}),e}catch(e){}}();e.exports=r},function(e,n){var t=Date.now;e.exports=function(e){var n=0,o=0;return function(){var r=t(),a=16-(r-o);if(o=r,a>0){if(++n>=800)return arguments[0]}else n=0;return e.apply(void 0,arguments)}}},function(e,n,t){var o=t(159),r=t(302),a=t(307),s=t(160),i=t(308),c=t(95);e.exports=function(e,n,t){var l=-1,u=r,d=e.length,p=!0,m=[],f=m;if(t)p=!1,u=a;else if(d>=200){var h=n?null:i(e);if(h)return c(h);p=!1,u=s,f=new o}else f=n?[]:m;e:for(;++l<d;){var g=e[l],v=n?n(g):g;if(g=t||0!==g?g:0,p&&v==v){for(var b=f.length;b--;)if(f[b]===v)continue e;n&&f.push(v),m.push(g)}else u(f,v,t)||(f!==m&&f.push(v),m.push(g))}return m}},function(e,n,t){var o=t(303);e.exports=function(e,n){return!!(null==e?0:e.length)&&o(e,n,0)>-1}},function(e,n,t){var o=t(304),r=t(305),a=t(306);e.exports=function(e,n,t){return n==n?a(e,n,t):o(e,r,t)}},function(e,n){e.exports=function(e,n,t,o){for(var r=e.length,a=t+(o?1:-1);o?a--:++a<r;)if(n(e[a],a,e))return a;return-1}},function(e,n){e.exports=function(e){return e!=e}},function(e,n){e.exports=function(e,n,t){for(var o=t-1,r=e.length;++o<r;)if(e[o]===n)return o;return-1}},function(e,n){e.exports=function(e,n,t){for(var o=-1,r=null==e?0:e.length;++o<r;)if(t(n,e[o]))return!0;return!1}},function(e,n,t){var o=t(166),r=t(309),a=t(95),s=o&&1/a(new o([,-0]))[1]==1/0?function(e){return new o(e)}:r;e.exports=s},function(e,n){e.exports=function(){}},function(e,n,t){var o=t(165),r=t(29);e.exports=function(e){return r(e)&&o(e)}},function(e,n,t){},function(e,n,t){var o=t(0),r=t(5);o({target:"Object",stat:!0,forced:!r,sham:!r},{defineProperties:t(110)})},function(e,n,t){var o=t(0),r=t(1),a=t(11),s=t(20).f,i=t(5),c=r((function(){s(1)}));o({target:"Object",stat:!0,forced:!i||c,sham:!i},{getOwnPropertyDescriptor:function(e,n){return s(a(e),n)}})},function(e,n,t){var o=t(2),r=t(118).trim,a=t(119),s=o.parseInt,i=/^[+-]?0[Xx]/,c=8!==s(a+"08")||22!==s(a+"0x16");e.exports=c?function(e,n){var t=r(String(e));return s(t,n>>>0||(i.test(t)?16:10))}:s},function(e,n,t){"use strict";t(171)},function(e,n,t){},function(e,n,t){},function(e,n,t){"use strict";var o=t(5),r=t(2),a=t(74),s=t(9),i=t(7),c=t(24),l=t(196),u=t(39),d=t(1),p=t(28),m=t(52).f,f=t(20).f,h=t(8).f,g=t(118).trim,v=r.Number,b=v.prototype,_="Number"==c(p(b)),w=function(e){var n,t,o,r,a,s,i,c,l=u(e,!1);if("string"==typeof l&&l.length>2)if(43===(n=(l=g(l)).charCodeAt(0))||45===n){if(88===(t=l.charCodeAt(2))||120===t)return NaN}else if(48===n){switch(l.charCodeAt(1)){case 66:case 98:o=2,r=49;break;case 79:case 111:o=8,r=55;break;default:return+l}for(s=(a=l.slice(2)).length,i=0;i<s;i++)if((c=a.charCodeAt(i))<48||c>r)return NaN;return parseInt(a,o)}return+l};if(a("Number",!v(" 0o1")||!v("0b1")||v("+0x1"))){for(var y,k=function(e){var n=arguments.length<1?0:e,t=this;return t instanceof k&&(_?d((function(){b.valueOf.call(t)})):"Number"!=c(t))?l(new v(w(n)),t,k):w(n)},x=o?m(v):"MAX_VALUE,MIN_VALUE,NaN,NEGATIVE_INFINITY,POSITIVE_INFINITY,EPSILON,isFinite,isInteger,isNaN,isSafeInteger,MAX_SAFE_INTEGER,MIN_SAFE_INTEGER,parseFloat,parseInt,isInteger,fromString,range".split(","),P=0;x.length>P;P++)i(v,y=x[P])&&!i(k,y)&&h(k,y,f(v,y));k.prototype=b,b.constructor=k,s(r,"Number",k)}},function(e,n,t){"use strict";t(172)},function(e,n,t){},function(e,n,t){"use strict";var o=t(25),r=t(4),a=[].slice,s={},i=function(e,n,t){if(!(n in s)){for(var o=[],r=0;r<n;r++)o[r]="a["+r+"]";s[n]=Function("C,a","return new C("+o.join(",")+")")}return s[n](e,t)};e.exports=Function.bind||function(e){var n=o(this),t=a.call(arguments,1),s=function(){var o=t.concat(a.call(arguments));return this instanceof s?i(n,o.length,o):n.apply(e,o)};return r(n.prototype)&&(s.prototype=n.prototype),s}},function(e,n,t){"use strict";t(175)},function(e,n,t){"use strict";t(176)},function(e,n,t){"use strict";t.r(n);t(109),t(200),t(206),t(207),t(22),t(71),t(49),t(10),t(27),t(33),t(105);var o=t(61),r=Object.freeze({});function a(e){return null==e}function s(e){return null!=e}function i(e){return!0===e}function c(e){return"string"==typeof e||"number"==typeof e||"symbol"==typeof e||"boolean"==typeof e}function l(e){return null!==e&&"object"==typeof e}var u=Object.prototype.toString;function d(e){return"[object Object]"===u.call(e)}function p(e){return"[object RegExp]"===u.call(e)}function m(e){var n=parseFloat(String(e));return n>=0&&Math.floor(n)===n&&isFinite(e)}function f(e){return s(e)&&"function"==typeof e.then&&"function"==typeof e.catch}function h(e){return null==e?"":Array.isArray(e)||d(e)&&e.toString===u?JSON.stringify(e,null,2):String(e)}function g(e){var n=parseFloat(e);return isNaN(n)?e:n}function v(e,n){for(var t=Object.create(null),o=e.split(","),r=0;r<o.length;r++)t[o[r]]=!0;return n?function(e){return t[e.toLowerCase()]}:function(e){return t[e]}}v("slot,component",!0);var b=v("key,ref,slot,slot-scope,is");function _(e,n){if(e.length){var t=e.indexOf(n);if(t>-1)return e.splice(t,1)}}var w=Object.prototype.hasOwnProperty;function y(e,n){return w.call(e,n)}function k(e){var n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}var x=/-(\w)/g,P=k((function(e){return e.replace(x,(function(e,n){return n?n.toUpperCase():""}))})),S=k((function(e){return e.charAt(0).toUpperCase()+e.slice(1)})),C=/\B([A-Z])/g,E=k((function(e){return e.replace(C,"-$1").toLowerCase()}));var I=Function.prototype.bind?function(e,n){return e.bind(n)}:function(e,n){function t(t){var o=arguments.length;return o?o>1?e.apply(n,arguments):e.call(n,t):e.call(n)}return t._length=e.length,t};function T(e,n){n=n||0;for(var t=e.length-n,o=new Array(t);t--;)o[t]=e[t+n];return o}function U(e,n){for(var t in n)e[t]=n[t];return e}function z(e){for(var n={},t=0;t<e.length;t++)e[t]&&U(n,e[t]);return n}function O(e,n,t){}var A=function(e,n,t){return!1},D=function(e){return e};function M(e,n){if(e===n)return!0;var t=l(e),o=l(n);if(!t||!o)return!t&&!o&&String(e)===String(n);try{var r=Array.isArray(e),a=Array.isArray(n);if(r&&a)return e.length===n.length&&e.every((function(e,t){return M(e,n[t])}));if(e instanceof Date&&n instanceof Date)return e.getTime()===n.getTime();if(r||a)return!1;var s=Object.keys(e),i=Object.keys(n);return s.length===i.length&&s.every((function(t){return M(e[t],n[t])}))}catch(e){return!1}}function j(e,n){for(var t=0;t<e.length;t++)if(M(e[t],n))return t;return-1}function L(e){var n=!1;return function(){n||(n=!0,e.apply(this,arguments))}}var B=["component","directive","filter"],R=["beforeCreate","created","beforeMount","mounted","beforeUpdate","updated","beforeDestroy","destroyed","activated","deactivated","errorCaptured","serverPrefetch"],N={optionMergeStrategies:Object.create(null),silent:!1,productionTip:!1,devtools:!1,performance:!1,errorHandler:null,warnHandler:null,ignoredElements:[],keyCodes:Object.create(null),isReservedTag:A,isReservedAttr:A,isUnknownElement:A,getTagNamespace:O,parsePlatformTagName:D,mustUseProp:A,async:!0,_lifecycleHooks:R},F=/a-zA-Z\u00B7\u00C0-\u00D6\u00D8-\u00F6\u00F8-\u037D\u037F-\u1FFF\u200C-\u200D\u203F-\u2040\u2070-\u218F\u2C00-\u2FEF\u3001-\uD7FF\uF900-\uFDCF\uFDF0-\uFFFD/;function G(e,n,t,o){Object.defineProperty(e,n,{value:t,enumerable:!!o,writable:!0,configurable:!0})}var $=new RegExp("[^"+F.source+".$_\\d]");var q,V="__proto__"in{},W="undefined"!=typeof window,H="undefined"!=typeof WXEnvironment&&!!WXEnvironment.platform,K=H&&WXEnvironment.platform.toLowerCase(),J=W&&window.navigator.userAgent.toLowerCase(),Y=J&&/msie|trident/.test(J),X=J&&J.indexOf("msie 9.0")>0,Q=J&&J.indexOf("edge/")>0,Z=(J&&J.indexOf("android"),J&&/iphone|ipad|ipod|ios/.test(J)||"ios"===K),ee=(J&&/chrome\/\d+/.test(J),J&&/phantomjs/.test(J),J&&J.match(/firefox\/(\d+)/)),ne={}.watch,te=!1;if(W)try{var oe={};Object.defineProperty(oe,"passive",{get:function(){te=!0}}),window.addEventListener("test-passive",null,oe)}catch(e){}var re=function(){return void 0===q&&(q=!W&&!H&&"undefined"!=typeof global&&(global.process&&"server"===global.process.env.VUE_ENV)),q},ae=W&&window.__VUE_DEVTOOLS_GLOBAL_HOOK__;function se(e){return"function"==typeof e&&/native code/.test(e.toString())}var ie,ce="undefined"!=typeof Symbol&&se(Symbol)&&"undefined"!=typeof Reflect&&se(Reflect.ownKeys);ie="undefined"!=typeof Set&&se(Set)?Set:function(){function e(){this.set=Object.create(null)}return e.prototype.has=function(e){return!0===this.set[e]},e.prototype.add=function(e){this.set[e]=!0},e.prototype.clear=function(){this.set=Object.create(null)},e}();var le=O,ue=0,de=function(){this.id=ue++,this.subs=[]};de.prototype.addSub=function(e){this.subs.push(e)},de.prototype.removeSub=function(e){_(this.subs,e)},de.prototype.depend=function(){de.target&&de.target.addDep(this)},de.prototype.notify=function(){var e=this.subs.slice();for(var n=0,t=e.length;n<t;n++)e[n].update()},de.target=null;var pe=[];function me(e){pe.push(e),de.target=e}function fe(){pe.pop(),de.target=pe[pe.length-1]}var he=function(e,n,t,o,r,a,s,i){this.tag=e,this.data=n,this.children=t,this.text=o,this.elm=r,this.ns=void 0,this.context=a,this.fnContext=void 0,this.fnOptions=void 0,this.fnScopeId=void 0,this.key=n&&n.key,this.componentOptions=s,this.componentInstance=void 0,this.parent=void 0,this.raw=!1,this.isStatic=!1,this.isRootInsert=!0,this.isComment=!1,this.isCloned=!1,this.isOnce=!1,this.asyncFactory=i,this.asyncMeta=void 0,this.isAsyncPlaceholder=!1},ge={child:{configurable:!0}};ge.child.get=function(){return this.componentInstance},Object.defineProperties(he.prototype,ge);var ve=function(e){void 0===e&&(e="");var n=new he;return n.text=e,n.isComment=!0,n};function be(e){return new he(void 0,void 0,void 0,String(e))}function _e(e){var n=new he(e.tag,e.data,e.children&&e.children.slice(),e.text,e.elm,e.context,e.componentOptions,e.asyncFactory);return n.ns=e.ns,n.isStatic=e.isStatic,n.key=e.key,n.isComment=e.isComment,n.fnContext=e.fnContext,n.fnOptions=e.fnOptions,n.fnScopeId=e.fnScopeId,n.asyncMeta=e.asyncMeta,n.isCloned=!0,n}var we=Array.prototype,ye=Object.create(we);["push","pop","shift","unshift","splice","sort","reverse"].forEach((function(e){var n=we[e];G(ye,e,(function(){for(var t=[],o=arguments.length;o--;)t[o]=arguments[o];var r,a=n.apply(this,t),s=this.__ob__;switch(e){case"push":case"unshift":r=t;break;case"splice":r=t.slice(2)}return r&&s.observeArray(r),s.dep.notify(),a}))}));var ke=Object.getOwnPropertyNames(ye),xe=!0;function Pe(e){xe=e}var Se=function(e){this.value=e,this.dep=new de,this.vmCount=0,G(e,"__ob__",this),Array.isArray(e)?(V?function(e,n){e.__proto__=n}(e,ye):function(e,n,t){for(var o=0,r=t.length;o<r;o++){var a=t[o];G(e,a,n[a])}}(e,ye,ke),this.observeArray(e)):this.walk(e)};function Ce(e,n){var t;if(l(e)&&!(e instanceof he))return y(e,"__ob__")&&e.__ob__ instanceof Se?t=e.__ob__:xe&&!re()&&(Array.isArray(e)||d(e))&&Object.isExtensible(e)&&!e._isVue&&(t=new Se(e)),n&&t&&t.vmCount++,t}function Ee(e,n,t,o,r){var a=new de,s=Object.getOwnPropertyDescriptor(e,n);if(!s||!1!==s.configurable){var i=s&&s.get,c=s&&s.set;i&&!c||2!==arguments.length||(t=e[n]);var l=!r&&Ce(t);Object.defineProperty(e,n,{enumerable:!0,configurable:!0,get:function(){var n=i?i.call(e):t;return de.target&&(a.depend(),l&&(l.dep.depend(),Array.isArray(n)&&Ue(n))),n},set:function(n){var o=i?i.call(e):t;n===o||n!=n&&o!=o||i&&!c||(c?c.call(e,n):t=n,l=!r&&Ce(n),a.notify())}})}}function Ie(e,n,t){if(Array.isArray(e)&&m(n))return e.length=Math.max(e.length,n),e.splice(n,1,t),t;if(n in e&&!(n in Object.prototype))return e[n]=t,t;var o=e.__ob__;return e._isVue||o&&o.vmCount?t:o?(Ee(o.value,n,t),o.dep.notify(),t):(e[n]=t,t)}function Te(e,n){if(Array.isArray(e)&&m(n))e.splice(n,1);else{var t=e.__ob__;e._isVue||t&&t.vmCount||y(e,n)&&(delete e[n],t&&t.dep.notify())}}function Ue(e){for(var n=void 0,t=0,o=e.length;t<o;t++)(n=e[t])&&n.__ob__&&n.__ob__.dep.depend(),Array.isArray(n)&&Ue(n)}Se.prototype.walk=function(e){for(var n=Object.keys(e),t=0;t<n.length;t++)Ee(e,n[t])},Se.prototype.observeArray=function(e){for(var n=0,t=e.length;n<t;n++)Ce(e[n])};var ze=N.optionMergeStrategies;function Oe(e,n){if(!n)return e;for(var t,o,r,a=ce?Reflect.ownKeys(n):Object.keys(n),s=0;s<a.length;s++)"__ob__"!==(t=a[s])&&(o=e[t],r=n[t],y(e,t)?o!==r&&d(o)&&d(r)&&Oe(o,r):Ie(e,t,r));return e}function Ae(e,n,t){return t?function(){var o="function"==typeof n?n.call(t,t):n,r="function"==typeof e?e.call(t,t):e;return o?Oe(o,r):r}:n?e?function(){return Oe("function"==typeof n?n.call(this,this):n,"function"==typeof e?e.call(this,this):e)}:n:e}function De(e,n){var t=n?e?e.concat(n):Array.isArray(n)?n:[n]:e;return t?function(e){for(var n=[],t=0;t<e.length;t++)-1===n.indexOf(e[t])&&n.push(e[t]);return n}(t):t}function Me(e,n,t,o){var r=Object.create(e||null);return n?U(r,n):r}ze.data=function(e,n,t){return t?Ae(e,n,t):n&&"function"!=typeof n?e:Ae(e,n)},R.forEach((function(e){ze[e]=De})),B.forEach((function(e){ze[e+"s"]=Me})),ze.watch=function(e,n,t,o){if(e===ne&&(e=void 0),n===ne&&(n=void 0),!n)return Object.create(e||null);if(!e)return n;var r={};for(var a in U(r,e),n){var s=r[a],i=n[a];s&&!Array.isArray(s)&&(s=[s]),r[a]=s?s.concat(i):Array.isArray(i)?i:[i]}return r},ze.props=ze.methods=ze.inject=ze.computed=function(e,n,t,o){if(!e)return n;var r=Object.create(null);return U(r,e),n&&U(r,n),r},ze.provide=Ae;var je=function(e,n){return void 0===n?e:n};function Le(e,n,t){if("function"==typeof n&&(n=n.options),function(e,n){var t=e.props;if(t){var o,r,a={};if(Array.isArray(t))for(o=t.length;o--;)"string"==typeof(r=t[o])&&(a[P(r)]={type:null});else if(d(t))for(var s in t)r=t[s],a[P(s)]=d(r)?r:{type:r};else 0;e.props=a}}(n),function(e,n){var t=e.inject;if(t){var o=e.inject={};if(Array.isArray(t))for(var r=0;r<t.length;r++)o[t[r]]={from:t[r]};else if(d(t))for(var a in t){var s=t[a];o[a]=d(s)?U({from:a},s):{from:s}}else 0}}(n),function(e){var n=e.directives;if(n)for(var t in n){var o=n[t];"function"==typeof o&&(n[t]={bind:o,update:o})}}(n),!n._base&&(n.extends&&(e=Le(e,n.extends,t)),n.mixins))for(var o=0,r=n.mixins.length;o<r;o++)e=Le(e,n.mixins[o],t);var a,s={};for(a in e)i(a);for(a in n)y(e,a)||i(a);function i(o){var r=ze[o]||je;s[o]=r(e[o],n[o],t,o)}return s}function Be(e,n,t,o){if("string"==typeof t){var r=e[n];if(y(r,t))return r[t];var a=P(t);if(y(r,a))return r[a];var s=S(a);return y(r,s)?r[s]:r[t]||r[a]||r[s]}}function Re(e,n,t,o){var r=n[e],a=!y(t,e),s=t[e],i=Ge(Boolean,r.type);if(i>-1)if(a&&!y(r,"default"))s=!1;else if(""===s||s===E(e)){var c=Ge(String,r.type);(c<0||i<c)&&(s=!0)}if(void 0===s){s=function(e,n,t){if(!y(n,"default"))return;var o=n.default;0;if(e&&e.$options.propsData&&void 0===e.$options.propsData[t]&&void 0!==e._props[t])return e._props[t];return"function"==typeof o&&"Function"!==Ne(n.type)?o.call(e):o}(o,r,e);var l=xe;Pe(!0),Ce(s),Pe(l)}return s}function Ne(e){var n=e&&e.toString().match(/^\s*function (\w+)/);return n?n[1]:""}function Fe(e,n){return Ne(e)===Ne(n)}function Ge(e,n){if(!Array.isArray(n))return Fe(n,e)?0:-1;for(var t=0,o=n.length;t<o;t++)if(Fe(n[t],e))return t;return-1}function $e(e,n,t){me();try{if(n)for(var o=n;o=o.$parent;){var r=o.$options.errorCaptured;if(r)for(var a=0;a<r.length;a++)try{if(!1===r[a].call(o,e,n,t))return}catch(e){Ve(e,o,"errorCaptured hook")}}Ve(e,n,t)}finally{fe()}}function qe(e,n,t,o,r){var a;try{(a=t?e.apply(n,t):e.call(n))&&!a._isVue&&f(a)&&!a._handled&&(a.catch((function(e){return $e(e,o,r+" (Promise/async)")})),a._handled=!0)}catch(e){$e(e,o,r)}return a}function Ve(e,n,t){if(N.errorHandler)try{return N.errorHandler.call(null,e,n,t)}catch(n){n!==e&&We(n,null,"config.errorHandler")}We(e,n,t)}function We(e,n,t){if(!W&&!H||"undefined"==typeof console)throw e;console.error(e)}var He,Ke=!1,Je=[],Ye=!1;function Xe(){Ye=!1;var e=Je.slice(0);Je.length=0;for(var n=0;n<e.length;n++)e[n]()}if("undefined"!=typeof Promise&&se(Promise)){var Qe=Promise.resolve();He=function(){Qe.then(Xe),Z&&setTimeout(O)},Ke=!0}else if(Y||"undefined"==typeof MutationObserver||!se(MutationObserver)&&"[object MutationObserverConstructor]"!==MutationObserver.toString())He="undefined"!=typeof setImmediate&&se(setImmediate)?function(){setImmediate(Xe)}:function(){setTimeout(Xe,0)};else{var Ze=1,en=new MutationObserver(Xe),nn=document.createTextNode(String(Ze));en.observe(nn,{characterData:!0}),He=function(){Ze=(Ze+1)%2,nn.data=String(Ze)},Ke=!0}function tn(e,n){var t;if(Je.push((function(){if(e)try{e.call(n)}catch(e){$e(e,n,"nextTick")}else t&&t(n)})),Ye||(Ye=!0,He()),!e&&"undefined"!=typeof Promise)return new Promise((function(e){t=e}))}var on=new ie;function rn(e){!function e(n,t){var o,r,a=Array.isArray(n);if(!a&&!l(n)||Object.isFrozen(n)||n instanceof he)return;if(n.__ob__){var s=n.__ob__.dep.id;if(t.has(s))return;t.add(s)}if(a)for(o=n.length;o--;)e(n[o],t);else for(r=Object.keys(n),o=r.length;o--;)e(n[r[o]],t)}(e,on),on.clear()}var an=k((function(e){var n="&"===e.charAt(0),t="~"===(e=n?e.slice(1):e).charAt(0),o="!"===(e=t?e.slice(1):e).charAt(0);return{name:e=o?e.slice(1):e,once:t,capture:o,passive:n}}));function sn(e,n){function t(){var e=arguments,o=t.fns;if(!Array.isArray(o))return qe(o,null,arguments,n,"v-on handler");for(var r=o.slice(),a=0;a<r.length;a++)qe(r[a],null,e,n,"v-on handler")}return t.fns=e,t}function cn(e,n,t,o,r,s){var c,l,u,d;for(c in e)l=e[c],u=n[c],d=an(c),a(l)||(a(u)?(a(l.fns)&&(l=e[c]=sn(l,s)),i(d.once)&&(l=e[c]=r(d.name,l,d.capture)),t(d.name,l,d.capture,d.passive,d.params)):l!==u&&(u.fns=l,e[c]=u));for(c in n)a(e[c])&&o((d=an(c)).name,n[c],d.capture)}function ln(e,n,t){var o;e instanceof he&&(e=e.data.hook||(e.data.hook={}));var r=e[n];function c(){t.apply(this,arguments),_(o.fns,c)}a(r)?o=sn([c]):s(r.fns)&&i(r.merged)?(o=r).fns.push(c):o=sn([r,c]),o.merged=!0,e[n]=o}function un(e,n,t,o,r){if(s(n)){if(y(n,t))return e[t]=n[t],r||delete n[t],!0;if(y(n,o))return e[t]=n[o],r||delete n[o],!0}return!1}function dn(e){return c(e)?[be(e)]:Array.isArray(e)?function e(n,t){var o,r,l,u,d=[];for(o=0;o<n.length;o++)a(r=n[o])||"boolean"==typeof r||(l=d.length-1,u=d[l],Array.isArray(r)?r.length>0&&(pn((r=e(r,(t||"")+"_"+o))[0])&&pn(u)&&(d[l]=be(u.text+r[0].text),r.shift()),d.push.apply(d,r)):c(r)?pn(u)?d[l]=be(u.text+r):""!==r&&d.push(be(r)):pn(r)&&pn(u)?d[l]=be(u.text+r.text):(i(n._isVList)&&s(r.tag)&&a(r.key)&&s(t)&&(r.key="__vlist"+t+"_"+o+"__"),d.push(r)));return d}(e):void 0}function pn(e){return s(e)&&s(e.text)&&!1===e.isComment}function mn(e,n){if(e){for(var t=Object.create(null),o=ce?Reflect.ownKeys(e):Object.keys(e),r=0;r<o.length;r++){var a=o[r];if("__ob__"!==a){for(var s=e[a].from,i=n;i;){if(i._provided&&y(i._provided,s)){t[a]=i._provided[s];break}i=i.$parent}if(!i)if("default"in e[a]){var c=e[a].default;t[a]="function"==typeof c?c.call(n):c}else 0}}return t}}function fn(e,n){if(!e||!e.length)return{};for(var t={},o=0,r=e.length;o<r;o++){var a=e[o],s=a.data;if(s&&s.attrs&&s.attrs.slot&&delete s.attrs.slot,a.context!==n&&a.fnContext!==n||!s||null==s.slot)(t.default||(t.default=[])).push(a);else{var i=s.slot,c=t[i]||(t[i]=[]);"template"===a.tag?c.push.apply(c,a.children||[]):c.push(a)}}for(var l in t)t[l].every(hn)&&delete t[l];return t}function hn(e){return e.isComment&&!e.asyncFactory||" "===e.text}function gn(e,n,t){var o,a=Object.keys(n).length>0,s=e?!!e.$stable:!a,i=e&&e.$key;if(e){if(e._normalized)return e._normalized;if(s&&t&&t!==r&&i===t.$key&&!a&&!t.$hasNormal)return t;for(var c in o={},e)e[c]&&"$"!==c[0]&&(o[c]=vn(n,c,e[c]))}else o={};for(var l in n)l in o||(o[l]=bn(n,l));return e&&Object.isExtensible(e)&&(e._normalized=o),G(o,"$stable",s),G(o,"$key",i),G(o,"$hasNormal",a),o}function vn(e,n,t){var o=function(){var e=arguments.length?t.apply(null,arguments):t({});return(e=e&&"object"==typeof e&&!Array.isArray(e)?[e]:dn(e))&&(0===e.length||1===e.length&&e[0].isComment)?void 0:e};return t.proxy&&Object.defineProperty(e,n,{get:o,enumerable:!0,configurable:!0}),o}function bn(e,n){return function(){return e[n]}}function _n(e,n){var t,o,r,a,i;if(Array.isArray(e)||"string"==typeof e)for(t=new Array(e.length),o=0,r=e.length;o<r;o++)t[o]=n(e[o],o);else if("number"==typeof e)for(t=new Array(e),o=0;o<e;o++)t[o]=n(o+1,o);else if(l(e))if(ce&&e[Symbol.iterator]){t=[];for(var c=e[Symbol.iterator](),u=c.next();!u.done;)t.push(n(u.value,t.length)),u=c.next()}else for(a=Object.keys(e),t=new Array(a.length),o=0,r=a.length;o<r;o++)i=a[o],t[o]=n(e[i],i,o);return s(t)||(t=[]),t._isVList=!0,t}function wn(e,n,t,o){var r,a=this.$scopedSlots[e];a?(t=t||{},o&&(t=U(U({},o),t)),r=a(t)||n):r=this.$slots[e]||n;var s=t&&t.slot;return s?this.$createElement("template",{slot:s},r):r}function yn(e){return Be(this.$options,"filters",e)||D}function kn(e,n){return Array.isArray(e)?-1===e.indexOf(n):e!==n}function xn(e,n,t,o,r){var a=N.keyCodes[n]||t;return r&&o&&!N.keyCodes[n]?kn(r,o):a?kn(a,e):o?E(o)!==n:void 0}function Pn(e,n,t,o,r){if(t)if(l(t)){var a;Array.isArray(t)&&(t=z(t));var s=function(s){if("class"===s||"style"===s||b(s))a=e;else{var i=e.attrs&&e.attrs.type;a=o||N.mustUseProp(n,i,s)?e.domProps||(e.domProps={}):e.attrs||(e.attrs={})}var c=P(s),l=E(s);c in a||l in a||(a[s]=t[s],r&&((e.on||(e.on={}))["update:"+s]=function(e){t[s]=e}))};for(var i in t)s(i)}else;return e}function Sn(e,n){var t=this._staticTrees||(this._staticTrees=[]),o=t[e];return o&&!n||En(o=t[e]=this.$options.staticRenderFns[e].call(this._renderProxy,null,this),"__static__"+e,!1),o}function Cn(e,n,t){return En(e,"__once__"+n+(t?"_"+t:""),!0),e}function En(e,n,t){if(Array.isArray(e))for(var o=0;o<e.length;o++)e[o]&&"string"!=typeof e[o]&&In(e[o],n+"_"+o,t);else In(e,n,t)}function In(e,n,t){e.isStatic=!0,e.key=n,e.isOnce=t}function Tn(e,n){if(n)if(d(n)){var t=e.on=e.on?U({},e.on):{};for(var o in n){var r=t[o],a=n[o];t[o]=r?[].concat(r,a):a}}else;return e}function Un(e,n,t,o){n=n||{$stable:!t};for(var r=0;r<e.length;r++){var a=e[r];Array.isArray(a)?Un(a,n,t):a&&(a.proxy&&(a.fn.proxy=!0),n[a.key]=a.fn)}return o&&(n.$key=o),n}function zn(e,n){for(var t=0;t<n.length;t+=2){var o=n[t];"string"==typeof o&&o&&(e[n[t]]=n[t+1])}return e}function On(e,n){return"string"==typeof e?n+e:e}function An(e){e._o=Cn,e._n=g,e._s=h,e._l=_n,e._t=wn,e._q=M,e._i=j,e._m=Sn,e._f=yn,e._k=xn,e._b=Pn,e._v=be,e._e=ve,e._u=Un,e._g=Tn,e._d=zn,e._p=On}function Dn(e,n,t,o,a){var s,c=this,l=a.options;y(o,"_uid")?(s=Object.create(o))._original=o:(s=o,o=o._original);var u=i(l._compiled),d=!u;this.data=e,this.props=n,this.children=t,this.parent=o,this.listeners=e.on||r,this.injections=mn(l.inject,o),this.slots=function(){return c.$slots||gn(e.scopedSlots,c.$slots=fn(t,o)),c.$slots},Object.defineProperty(this,"scopedSlots",{enumerable:!0,get:function(){return gn(e.scopedSlots,this.slots())}}),u&&(this.$options=l,this.$slots=this.slots(),this.$scopedSlots=gn(e.scopedSlots,this.$slots)),l._scopeId?this._c=function(e,n,t,r){var a=Fn(s,e,n,t,r,d);return a&&!Array.isArray(a)&&(a.fnScopeId=l._scopeId,a.fnContext=o),a}:this._c=function(e,n,t,o){return Fn(s,e,n,t,o,d)}}function Mn(e,n,t,o,r){var a=_e(e);return a.fnContext=t,a.fnOptions=o,n.slot&&((a.data||(a.data={})).slot=n.slot),a}function jn(e,n){for(var t in n)e[P(t)]=n[t]}An(Dn.prototype);var Ln={init:function(e,n){if(e.componentInstance&&!e.componentInstance._isDestroyed&&e.data.keepAlive){var t=e;Ln.prepatch(t,t)}else{(e.componentInstance=function(e,n){var t={_isComponent:!0,_parentVnode:e,parent:n},o=e.data.inlineTemplate;s(o)&&(t.render=o.render,t.staticRenderFns=o.staticRenderFns);return new e.componentOptions.Ctor(t)}(e,Xn)).$mount(n?e.elm:void 0,n)}},prepatch:function(e,n){var t=n.componentOptions;!function(e,n,t,o,a){0;var s=o.data.scopedSlots,i=e.$scopedSlots,c=!!(s&&!s.$stable||i!==r&&!i.$stable||s&&e.$scopedSlots.$key!==s.$key),l=!!(a||e.$options._renderChildren||c);e.$options._parentVnode=o,e.$vnode=o,e._vnode&&(e._vnode.parent=o);if(e.$options._renderChildren=a,e.$attrs=o.data.attrs||r,e.$listeners=t||r,n&&e.$options.props){Pe(!1);for(var u=e._props,d=e.$options._propKeys||[],p=0;p<d.length;p++){var m=d[p],f=e.$options.props;u[m]=Re(m,f,n,e)}Pe(!0),e.$options.propsData=n}t=t||r;var h=e.$options._parentListeners;e.$options._parentListeners=t,Yn(e,t,h),l&&(e.$slots=fn(a,o.context),e.$forceUpdate());0}(n.componentInstance=e.componentInstance,t.propsData,t.listeners,n,t.children)},insert:function(e){var n,t=e.context,o=e.componentInstance;o._isMounted||(o._isMounted=!0,nt(o,"mounted")),e.data.keepAlive&&(t._isMounted?((n=o)._inactive=!1,ot.push(n)):et(o,!0))},destroy:function(e){var n=e.componentInstance;n._isDestroyed||(e.data.keepAlive?function e(n,t){if(t&&(n._directInactive=!0,Zn(n)))return;if(!n._inactive){n._inactive=!0;for(var o=0;o<n.$children.length;o++)e(n.$children[o]);nt(n,"deactivated")}}(n,!0):n.$destroy())}},Bn=Object.keys(Ln);function Rn(e,n,t,o,c){if(!a(e)){var u=t.$options._base;if(l(e)&&(e=u.extend(e)),"function"==typeof e){var d;if(a(e.cid)&&void 0===(e=function(e,n){if(i(e.error)&&s(e.errorComp))return e.errorComp;if(s(e.resolved))return e.resolved;var t=$n;t&&s(e.owners)&&-1===e.owners.indexOf(t)&&e.owners.push(t);if(i(e.loading)&&s(e.loadingComp))return e.loadingComp;if(t&&!s(e.owners)){var o=e.owners=[t],r=!0,c=null,u=null;t.$on("hook:destroyed",(function(){return _(o,t)}));var d=function(e){for(var n=0,t=o.length;n<t;n++)o[n].$forceUpdate();e&&(o.length=0,null!==c&&(clearTimeout(c),c=null),null!==u&&(clearTimeout(u),u=null))},p=L((function(t){e.resolved=qn(t,n),r?o.length=0:d(!0)})),m=L((function(n){s(e.errorComp)&&(e.error=!0,d(!0))})),h=e(p,m);return l(h)&&(f(h)?a(e.resolved)&&h.then(p,m):f(h.component)&&(h.component.then(p,m),s(h.error)&&(e.errorComp=qn(h.error,n)),s(h.loading)&&(e.loadingComp=qn(h.loading,n),0===h.delay?e.loading=!0:c=setTimeout((function(){c=null,a(e.resolved)&&a(e.error)&&(e.loading=!0,d(!1))}),h.delay||200)),s(h.timeout)&&(u=setTimeout((function(){u=null,a(e.resolved)&&m(null)}),h.timeout)))),r=!1,e.loading?e.loadingComp:e.resolved}}(d=e,u)))return function(e,n,t,o,r){var a=ve();return a.asyncFactory=e,a.asyncMeta={data:n,context:t,children:o,tag:r},a}(d,n,t,o,c);n=n||{},xt(e),s(n.model)&&function(e,n){var t=e.model&&e.model.prop||"value",o=e.model&&e.model.event||"input";(n.attrs||(n.attrs={}))[t]=n.model.value;var r=n.on||(n.on={}),a=r[o],i=n.model.callback;s(a)?(Array.isArray(a)?-1===a.indexOf(i):a!==i)&&(r[o]=[i].concat(a)):r[o]=i}(e.options,n);var p=function(e,n,t){var o=n.options.props;if(!a(o)){var r={},i=e.attrs,c=e.props;if(s(i)||s(c))for(var l in o){var u=E(l);un(r,c,l,u,!0)||un(r,i,l,u,!1)}return r}}(n,e);if(i(e.options.functional))return function(e,n,t,o,a){var i=e.options,c={},l=i.props;if(s(l))for(var u in l)c[u]=Re(u,l,n||r);else s(t.attrs)&&jn(c,t.attrs),s(t.props)&&jn(c,t.props);var d=new Dn(t,c,a,o,e),p=i.render.call(null,d._c,d);if(p instanceof he)return Mn(p,t,d.parent,i,d);if(Array.isArray(p)){for(var m=dn(p)||[],f=new Array(m.length),h=0;h<m.length;h++)f[h]=Mn(m[h],t,d.parent,i,d);return f}}(e,p,n,t,o);var m=n.on;if(n.on=n.nativeOn,i(e.options.abstract)){var h=n.slot;n={},h&&(n.slot=h)}!function(e){for(var n=e.hook||(e.hook={}),t=0;t<Bn.length;t++){var o=Bn[t],r=n[o],a=Ln[o];r===a||r&&r._merged||(n[o]=r?Nn(a,r):a)}}(n);var g=e.options.name||c;return new he("vue-component-"+e.cid+(g?"-"+g:""),n,void 0,void 0,void 0,t,{Ctor:e,propsData:p,listeners:m,tag:c,children:o},d)}}}function Nn(e,n){var t=function(t,o){e(t,o),n(t,o)};return t._merged=!0,t}function Fn(e,n,t,o,r,u){return(Array.isArray(t)||c(t))&&(r=o,o=t,t=void 0),i(u)&&(r=2),function(e,n,t,o,r){if(s(t)&&s(t.__ob__))return ve();s(t)&&s(t.is)&&(n=t.is);if(!n)return ve();0;Array.isArray(o)&&"function"==typeof o[0]&&((t=t||{}).scopedSlots={default:o[0]},o.length=0);2===r?o=dn(o):1===r&&(o=function(e){for(var n=0;n<e.length;n++)if(Array.isArray(e[n]))return Array.prototype.concat.apply([],e);return e}(o));var c,u;if("string"==typeof n){var d;u=e.$vnode&&e.$vnode.ns||N.getTagNamespace(n),c=N.isReservedTag(n)?new he(N.parsePlatformTagName(n),t,o,void 0,void 0,e):t&&t.pre||!s(d=Be(e.$options,"components",n))?new he(n,t,o,void 0,void 0,e):Rn(d,t,e,o,n)}else c=Rn(n,t,e,o);return Array.isArray(c)?c:s(c)?(s(u)&&function e(n,t,o){n.ns=t,"foreignObject"===n.tag&&(t=void 0,o=!0);if(s(n.children))for(var r=0,c=n.children.length;r<c;r++){var l=n.children[r];s(l.tag)&&(a(l.ns)||i(o)&&"svg"!==l.tag)&&e(l,t,o)}}(c,u),s(t)&&function(e){l(e.style)&&rn(e.style);l(e.class)&&rn(e.class)}(t),c):ve()}(e,n,t,o,r)}var Gn,$n=null;function qn(e,n){return(e.__esModule||ce&&"Module"===e[Symbol.toStringTag])&&(e=e.default),l(e)?n.extend(e):e}function Vn(e){return e.isComment&&e.asyncFactory}function Wn(e){if(Array.isArray(e))for(var n=0;n<e.length;n++){var t=e[n];if(s(t)&&(s(t.componentOptions)||Vn(t)))return t}}function Hn(e,n){Gn.$on(e,n)}function Kn(e,n){Gn.$off(e,n)}function Jn(e,n){var t=Gn;return function o(){var r=n.apply(null,arguments);null!==r&&t.$off(e,o)}}function Yn(e,n,t){Gn=e,cn(n,t||{},Hn,Kn,Jn,e),Gn=void 0}var Xn=null;function Qn(e){var n=Xn;return Xn=e,function(){Xn=n}}function Zn(e){for(;e&&(e=e.$parent);)if(e._inactive)return!0;return!1}function et(e,n){if(n){if(e._directInactive=!1,Zn(e))return}else if(e._directInactive)return;if(e._inactive||null===e._inactive){e._inactive=!1;for(var t=0;t<e.$children.length;t++)et(e.$children[t]);nt(e,"activated")}}function nt(e,n){me();var t=e.$options[n],o=n+" hook";if(t)for(var r=0,a=t.length;r<a;r++)qe(t[r],e,null,e,o);e._hasHookEvent&&e.$emit("hook:"+n),fe()}var tt=[],ot=[],rt={},at=!1,st=!1,it=0;var ct=0,lt=Date.now;if(W&&!Y){var ut=window.performance;ut&&"function"==typeof ut.now&&lt()>document.createEvent("Event").timeStamp&&(lt=function(){return ut.now()})}function dt(){var e,n;for(ct=lt(),st=!0,tt.sort((function(e,n){return e.id-n.id})),it=0;it<tt.length;it++)(e=tt[it]).before&&e.before(),n=e.id,rt[n]=null,e.run();var t=ot.slice(),o=tt.slice();it=tt.length=ot.length=0,rt={},at=st=!1,function(e){for(var n=0;n<e.length;n++)e[n]._inactive=!0,et(e[n],!0)}(t),function(e){var n=e.length;for(;n--;){var t=e[n],o=t.vm;o._watcher===t&&o._isMounted&&!o._isDestroyed&&nt(o,"updated")}}(o),ae&&N.devtools&&ae.emit("flush")}var pt=0,mt=function(e,n,t,o,r){this.vm=e,r&&(e._watcher=this),e._watchers.push(this),o?(this.deep=!!o.deep,this.user=!!o.user,this.lazy=!!o.lazy,this.sync=!!o.sync,this.before=o.before):this.deep=this.user=this.lazy=this.sync=!1,this.cb=t,this.id=++pt,this.active=!0,this.dirty=this.lazy,this.deps=[],this.newDeps=[],this.depIds=new ie,this.newDepIds=new ie,this.expression="","function"==typeof n?this.getter=n:(this.getter=function(e){if(!$.test(e)){var n=e.split(".");return function(e){for(var t=0;t<n.length;t++){if(!e)return;e=e[n[t]]}return e}}}(n),this.getter||(this.getter=O)),this.value=this.lazy?void 0:this.get()};mt.prototype.get=function(){var e;me(this);var n=this.vm;try{e=this.getter.call(n,n)}catch(e){if(!this.user)throw e;$e(e,n,'getter for watcher "'+this.expression+'"')}finally{this.deep&&rn(e),fe(),this.cleanupDeps()}return e},mt.prototype.addDep=function(e){var n=e.id;this.newDepIds.has(n)||(this.newDepIds.add(n),this.newDeps.push(e),this.depIds.has(n)||e.addSub(this))},mt.prototype.cleanupDeps=function(){for(var e=this.deps.length;e--;){var n=this.deps[e];this.newDepIds.has(n.id)||n.removeSub(this)}var t=this.depIds;this.depIds=this.newDepIds,this.newDepIds=t,this.newDepIds.clear(),t=this.deps,this.deps=this.newDeps,this.newDeps=t,this.newDeps.length=0},mt.prototype.update=function(){this.lazy?this.dirty=!0:this.sync?this.run():function(e){var n=e.id;if(null==rt[n]){if(rt[n]=!0,st){for(var t=tt.length-1;t>it&&tt[t].id>e.id;)t--;tt.splice(t+1,0,e)}else tt.push(e);at||(at=!0,tn(dt))}}(this)},mt.prototype.run=function(){if(this.active){var e=this.get();if(e!==this.value||l(e)||this.deep){var n=this.value;if(this.value=e,this.user)try{this.cb.call(this.vm,e,n)}catch(e){$e(e,this.vm,'callback for watcher "'+this.expression+'"')}else this.cb.call(this.vm,e,n)}}},mt.prototype.evaluate=function(){this.value=this.get(),this.dirty=!1},mt.prototype.depend=function(){for(var e=this.deps.length;e--;)this.deps[e].depend()},mt.prototype.teardown=function(){if(this.active){this.vm._isBeingDestroyed||_(this.vm._watchers,this);for(var e=this.deps.length;e--;)this.deps[e].removeSub(this);this.active=!1}};var ft={enumerable:!0,configurable:!0,get:O,set:O};function ht(e,n,t){ft.get=function(){return this[n][t]},ft.set=function(e){this[n][t]=e},Object.defineProperty(e,t,ft)}function gt(e){e._watchers=[];var n=e.$options;n.props&&function(e,n){var t=e.$options.propsData||{},o=e._props={},r=e.$options._propKeys=[];e.$parent&&Pe(!1);var a=function(a){r.push(a);var s=Re(a,n,t,e);Ee(o,a,s),a in e||ht(e,"_props",a)};for(var s in n)a(s);Pe(!0)}(e,n.props),n.methods&&function(e,n){e.$options.props;for(var t in n)e[t]="function"!=typeof n[t]?O:I(n[t],e)}(e,n.methods),n.data?function(e){var n=e.$options.data;d(n=e._data="function"==typeof n?function(e,n){me();try{return e.call(n,n)}catch(e){return $e(e,n,"data()"),{}}finally{fe()}}(n,e):n||{})||(n={});var t=Object.keys(n),o=e.$options.props,r=(e.$options.methods,t.length);for(;r--;){var a=t[r];0,o&&y(o,a)||(s=void 0,36!==(s=(a+"").charCodeAt(0))&&95!==s&&ht(e,"_data",a))}var s;Ce(n,!0)}(e):Ce(e._data={},!0),n.computed&&function(e,n){var t=e._computedWatchers=Object.create(null),o=re();for(var r in n){var a=n[r],s="function"==typeof a?a:a.get;0,o||(t[r]=new mt(e,s||O,O,vt)),r in e||bt(e,r,a)}}(e,n.computed),n.watch&&n.watch!==ne&&function(e,n){for(var t in n){var o=n[t];if(Array.isArray(o))for(var r=0;r<o.length;r++)yt(e,t,o[r]);else yt(e,t,o)}}(e,n.watch)}var vt={lazy:!0};function bt(e,n,t){var o=!re();"function"==typeof t?(ft.get=o?_t(n):wt(t),ft.set=O):(ft.get=t.get?o&&!1!==t.cache?_t(n):wt(t.get):O,ft.set=t.set||O),Object.defineProperty(e,n,ft)}function _t(e){return function(){var n=this._computedWatchers&&this._computedWatchers[e];if(n)return n.dirty&&n.evaluate(),de.target&&n.depend(),n.value}}function wt(e){return function(){return e.call(this,this)}}function yt(e,n,t,o){return d(t)&&(o=t,t=t.handler),"string"==typeof t&&(t=e[t]),e.$watch(n,t,o)}var kt=0;function xt(e){var n=e.options;if(e.super){var t=xt(e.super);if(t!==e.superOptions){e.superOptions=t;var o=function(e){var n,t=e.options,o=e.sealedOptions;for(var r in t)t[r]!==o[r]&&(n||(n={}),n[r]=t[r]);return n}(e);o&&U(e.extendOptions,o),(n=e.options=Le(t,e.extendOptions)).name&&(n.components[n.name]=e)}}return n}function Pt(e){this._init(e)}function St(e){e.cid=0;var n=1;e.extend=function(e){e=e||{};var t=this,o=t.cid,r=e._Ctor||(e._Ctor={});if(r[o])return r[o];var a=e.name||t.options.name;var s=function(e){this._init(e)};return(s.prototype=Object.create(t.prototype)).constructor=s,s.cid=n++,s.options=Le(t.options,e),s.super=t,s.options.props&&function(e){var n=e.options.props;for(var t in n)ht(e.prototype,"_props",t)}(s),s.options.computed&&function(e){var n=e.options.computed;for(var t in n)bt(e.prototype,t,n[t])}(s),s.extend=t.extend,s.mixin=t.mixin,s.use=t.use,B.forEach((function(e){s[e]=t[e]})),a&&(s.options.components[a]=s),s.superOptions=t.options,s.extendOptions=e,s.sealedOptions=U({},s.options),r[o]=s,s}}function Ct(e){return e&&(e.Ctor.options.name||e.tag)}function Et(e,n){return Array.isArray(e)?e.indexOf(n)>-1:"string"==typeof e?e.split(",").indexOf(n)>-1:!!p(e)&&e.test(n)}function It(e,n){var t=e.cache,o=e.keys,r=e._vnode;for(var a in t){var s=t[a];if(s){var i=Ct(s.componentOptions);i&&!n(i)&&Tt(t,a,o,r)}}}function Tt(e,n,t,o){var r=e[n];!r||o&&r.tag===o.tag||r.componentInstance.$destroy(),e[n]=null,_(t,n)}Pt.prototype._init=function(e){var n=this;n._uid=kt++,n._isVue=!0,e&&e._isComponent?function(e,n){var t=e.$options=Object.create(e.constructor.options),o=n._parentVnode;t.parent=n.parent,t._parentVnode=o;var r=o.componentOptions;t.propsData=r.propsData,t._parentListeners=r.listeners,t._renderChildren=r.children,t._componentTag=r.tag,n.render&&(t.render=n.render,t.staticRenderFns=n.staticRenderFns)}(n,e):n.$options=Le(xt(n.constructor),e||{},n),n._renderProxy=n,n._self=n,function(e){var n=e.$options,t=n.parent;if(t&&!n.abstract){for(;t.$options.abstract&&t.$parent;)t=t.$parent;t.$children.push(e)}e.$parent=t,e.$root=t?t.$root:e,e.$children=[],e.$refs={},e._watcher=null,e._inactive=null,e._directInactive=!1,e._isMounted=!1,e._isDestroyed=!1,e._isBeingDestroyed=!1}(n),function(e){e._events=Object.create(null),e._hasHookEvent=!1;var n=e.$options._parentListeners;n&&Yn(e,n)}(n),function(e){e._vnode=null,e._staticTrees=null;var n=e.$options,t=e.$vnode=n._parentVnode,o=t&&t.context;e.$slots=fn(n._renderChildren,o),e.$scopedSlots=r,e._c=function(n,t,o,r){return Fn(e,n,t,o,r,!1)},e.$createElement=function(n,t,o,r){return Fn(e,n,t,o,r,!0)};var a=t&&t.data;Ee(e,"$attrs",a&&a.attrs||r,null,!0),Ee(e,"$listeners",n._parentListeners||r,null,!0)}(n),nt(n,"beforeCreate"),function(e){var n=mn(e.$options.inject,e);n&&(Pe(!1),Object.keys(n).forEach((function(t){Ee(e,t,n[t])})),Pe(!0))}(n),gt(n),function(e){var n=e.$options.provide;n&&(e._provided="function"==typeof n?n.call(e):n)}(n),nt(n,"created"),n.$options.el&&n.$mount(n.$options.el)},function(e){var n={get:function(){return this._data}},t={get:function(){return this._props}};Object.defineProperty(e.prototype,"$data",n),Object.defineProperty(e.prototype,"$props",t),e.prototype.$set=Ie,e.prototype.$delete=Te,e.prototype.$watch=function(e,n,t){if(d(n))return yt(this,e,n,t);(t=t||{}).user=!0;var o=new mt(this,e,n,t);if(t.immediate)try{n.call(this,o.value)}catch(e){$e(e,this,'callback for immediate watcher "'+o.expression+'"')}return function(){o.teardown()}}}(Pt),function(e){var n=/^hook:/;e.prototype.$on=function(e,t){var o=this;if(Array.isArray(e))for(var r=0,a=e.length;r<a;r++)o.$on(e[r],t);else(o._events[e]||(o._events[e]=[])).push(t),n.test(e)&&(o._hasHookEvent=!0);return o},e.prototype.$once=function(e,n){var t=this;function o(){t.$off(e,o),n.apply(t,arguments)}return o.fn=n,t.$on(e,o),t},e.prototype.$off=function(e,n){var t=this;if(!arguments.length)return t._events=Object.create(null),t;if(Array.isArray(e)){for(var o=0,r=e.length;o<r;o++)t.$off(e[o],n);return t}var a,s=t._events[e];if(!s)return t;if(!n)return t._events[e]=null,t;for(var i=s.length;i--;)if((a=s[i])===n||a.fn===n){s.splice(i,1);break}return t},e.prototype.$emit=function(e){var n=this,t=n._events[e];if(t){t=t.length>1?T(t):t;for(var o=T(arguments,1),r='event handler for "'+e+'"',a=0,s=t.length;a<s;a++)qe(t[a],n,o,n,r)}return n}}(Pt),function(e){e.prototype._update=function(e,n){var t=this,o=t.$el,r=t._vnode,a=Qn(t);t._vnode=e,t.$el=r?t.__patch__(r,e):t.__patch__(t.$el,e,n,!1),a(),o&&(o.__vue__=null),t.$el&&(t.$el.__vue__=t),t.$vnode&&t.$parent&&t.$vnode===t.$parent._vnode&&(t.$parent.$el=t.$el)},e.prototype.$forceUpdate=function(){this._watcher&&this._watcher.update()},e.prototype.$destroy=function(){var e=this;if(!e._isBeingDestroyed){nt(e,"beforeDestroy"),e._isBeingDestroyed=!0;var n=e.$parent;!n||n._isBeingDestroyed||e.$options.abstract||_(n.$children,e),e._watcher&&e._watcher.teardown();for(var t=e._watchers.length;t--;)e._watchers[t].teardown();e._data.__ob__&&e._data.__ob__.vmCount--,e._isDestroyed=!0,e.__patch__(e._vnode,null),nt(e,"destroyed"),e.$off(),e.$el&&(e.$el.__vue__=null),e.$vnode&&(e.$vnode.parent=null)}}}(Pt),function(e){An(e.prototype),e.prototype.$nextTick=function(e){return tn(e,this)},e.prototype._render=function(){var e,n=this,t=n.$options,o=t.render,r=t._parentVnode;r&&(n.$scopedSlots=gn(r.data.scopedSlots,n.$slots,n.$scopedSlots)),n.$vnode=r;try{$n=n,e=o.call(n._renderProxy,n.$createElement)}catch(t){$e(t,n,"render"),e=n._vnode}finally{$n=null}return Array.isArray(e)&&1===e.length&&(e=e[0]),e instanceof he||(e=ve()),e.parent=r,e}}(Pt);var Ut=[String,RegExp,Array],zt={KeepAlive:{name:"keep-alive",abstract:!0,props:{include:Ut,exclude:Ut,max:[String,Number]},created:function(){this.cache=Object.create(null),this.keys=[]},destroyed:function(){for(var e in this.cache)Tt(this.cache,e,this.keys)},mounted:function(){var e=this;this.$watch("include",(function(n){It(e,(function(e){return Et(n,e)}))})),this.$watch("exclude",(function(n){It(e,(function(e){return!Et(n,e)}))}))},render:function(){var e=this.$slots.default,n=Wn(e),t=n&&n.componentOptions;if(t){var o=Ct(t),r=this.include,a=this.exclude;if(r&&(!o||!Et(r,o))||a&&o&&Et(a,o))return n;var s=this.cache,i=this.keys,c=null==n.key?t.Ctor.cid+(t.tag?"::"+t.tag:""):n.key;s[c]?(n.componentInstance=s[c].componentInstance,_(i,c),i.push(c)):(s[c]=n,i.push(c),this.max&&i.length>parseInt(this.max)&&Tt(s,i[0],i,this._vnode)),n.data.keepAlive=!0}return n||e&&e[0]}}};!function(e){var n={get:function(){return N}};Object.defineProperty(e,"config",n),e.util={warn:le,extend:U,mergeOptions:Le,defineReactive:Ee},e.set=Ie,e.delete=Te,e.nextTick=tn,e.observable=function(e){return Ce(e),e},e.options=Object.create(null),B.forEach((function(n){e.options[n+"s"]=Object.create(null)})),e.options._base=e,U(e.options.components,zt),function(e){e.use=function(e){var n=this._installedPlugins||(this._installedPlugins=[]);if(n.indexOf(e)>-1)return this;var t=T(arguments,1);return t.unshift(this),"function"==typeof e.install?e.install.apply(e,t):"function"==typeof e&&e.apply(null,t),n.push(e),this}}(e),function(e){e.mixin=function(e){return this.options=Le(this.options,e),this}}(e),St(e),function(e){B.forEach((function(n){e[n]=function(e,t){return t?("component"===n&&d(t)&&(t.name=t.name||e,t=this.options._base.extend(t)),"directive"===n&&"function"==typeof t&&(t={bind:t,update:t}),this.options[n+"s"][e]=t,t):this.options[n+"s"][e]}}))}(e)}(Pt),Object.defineProperty(Pt.prototype,"$isServer",{get:re}),Object.defineProperty(Pt.prototype,"$ssrContext",{get:function(){return this.$vnode&&this.$vnode.ssrContext}}),Object.defineProperty(Pt,"FunctionalRenderContext",{value:Dn}),Pt.version="2.6.12";var Ot=v("style,class"),At=v("input,textarea,option,select,progress"),Dt=v("contenteditable,draggable,spellcheck"),Mt=v("events,caret,typing,plaintext-only"),jt=v("allowfullscreen,async,autofocus,autoplay,checked,compact,controls,declare,default,defaultchecked,defaultmuted,defaultselected,defer,disabled,enabled,formnovalidate,hidden,indeterminate,inert,ismap,itemscope,loop,multiple,muted,nohref,noresize,noshade,novalidate,nowrap,open,pauseonexit,readonly,required,reversed,scoped,seamless,selected,sortable,translate,truespeed,typemustmatch,visible"),Lt="http://www.w3.org/1999/xlink",Bt=function(e){return":"===e.charAt(5)&&"xlink"===e.slice(0,5)},Rt=function(e){return Bt(e)?e.slice(6,e.length):""},Nt=function(e){return null==e||!1===e};function Ft(e){for(var n=e.data,t=e,o=e;s(o.componentInstance);)(o=o.componentInstance._vnode)&&o.data&&(n=Gt(o.data,n));for(;s(t=t.parent);)t&&t.data&&(n=Gt(n,t.data));return function(e,n){if(s(e)||s(n))return $t(e,qt(n));return""}(n.staticClass,n.class)}function Gt(e,n){return{staticClass:$t(e.staticClass,n.staticClass),class:s(e.class)?[e.class,n.class]:n.class}}function $t(e,n){return e?n?e+" "+n:e:n||""}function qt(e){return Array.isArray(e)?function(e){for(var n,t="",o=0,r=e.length;o<r;o++)s(n=qt(e[o]))&&""!==n&&(t&&(t+=" "),t+=n);return t}(e):l(e)?function(e){var n="";for(var t in e)e[t]&&(n&&(n+=" "),n+=t);return n}(e):"string"==typeof e?e:""}var Vt={svg:"http://www.w3.org/2000/svg",math:"http://www.w3.org/1998/Math/MathML"},Wt=v("html,body,base,head,link,meta,style,title,address,article,aside,footer,header,h1,h2,h3,h4,h5,h6,hgroup,nav,section,div,dd,dl,dt,figcaption,figure,picture,hr,img,li,main,ol,p,pre,ul,a,b,abbr,bdi,bdo,br,cite,code,data,dfn,em,i,kbd,mark,q,rp,rt,rtc,ruby,s,samp,small,span,strong,sub,sup,time,u,var,wbr,area,audio,map,track,video,embed,object,param,source,canvas,script,noscript,del,ins,caption,col,colgroup,table,thead,tbody,td,th,tr,button,datalist,fieldset,form,input,label,legend,meter,optgroup,option,output,progress,select,textarea,details,dialog,menu,menuitem,summary,content,element,shadow,template,blockquote,iframe,tfoot"),Ht=v("svg,animate,circle,clippath,cursor,defs,desc,ellipse,filter,font-face,foreignObject,g,glyph,image,line,marker,mask,missing-glyph,path,pattern,polygon,polyline,rect,switch,symbol,text,textpath,tspan,use,view",!0),Kt=function(e){return Wt(e)||Ht(e)};var Jt=Object.create(null);var Yt=v("text,number,password,search,email,tel,url");var Xt=Object.freeze({createElement:function(e,n){var t=document.createElement(e);return"select"!==e||n.data&&n.data.attrs&&void 0!==n.data.attrs.multiple&&t.setAttribute("multiple","multiple"),t},createElementNS:function(e,n){return document.createElementNS(Vt[e],n)},createTextNode:function(e){return document.createTextNode(e)},createComment:function(e){return document.createComment(e)},insertBefore:function(e,n,t){e.insertBefore(n,t)},removeChild:function(e,n){e.removeChild(n)},appendChild:function(e,n){e.appendChild(n)},parentNode:function(e){return e.parentNode},nextSibling:function(e){return e.nextSibling},tagName:function(e){return e.tagName},setTextContent:function(e,n){e.textContent=n},setStyleScope:function(e,n){e.setAttribute(n,"")}}),Qt={create:function(e,n){Zt(n)},update:function(e,n){e.data.ref!==n.data.ref&&(Zt(e,!0),Zt(n))},destroy:function(e){Zt(e,!0)}};function Zt(e,n){var t=e.data.ref;if(s(t)){var o=e.context,r=e.componentInstance||e.elm,a=o.$refs;n?Array.isArray(a[t])?_(a[t],r):a[t]===r&&(a[t]=void 0):e.data.refInFor?Array.isArray(a[t])?a[t].indexOf(r)<0&&a[t].push(r):a[t]=[r]:a[t]=r}}var eo=new he("",{},[]),no=["create","activate","update","remove","destroy"];function to(e,n){return e.key===n.key&&(e.tag===n.tag&&e.isComment===n.isComment&&s(e.data)===s(n.data)&&function(e,n){if("input"!==e.tag)return!0;var t,o=s(t=e.data)&&s(t=t.attrs)&&t.type,r=s(t=n.data)&&s(t=t.attrs)&&t.type;return o===r||Yt(o)&&Yt(r)}(e,n)||i(e.isAsyncPlaceholder)&&e.asyncFactory===n.asyncFactory&&a(n.asyncFactory.error))}function oo(e,n,t){var o,r,a={};for(o=n;o<=t;++o)s(r=e[o].key)&&(a[r]=o);return a}var ro={create:ao,update:ao,destroy:function(e){ao(e,eo)}};function ao(e,n){(e.data.directives||n.data.directives)&&function(e,n){var t,o,r,a=e===eo,s=n===eo,i=io(e.data.directives,e.context),c=io(n.data.directives,n.context),l=[],u=[];for(t in c)o=i[t],r=c[t],o?(r.oldValue=o.value,r.oldArg=o.arg,lo(r,"update",n,e),r.def&&r.def.componentUpdated&&u.push(r)):(lo(r,"bind",n,e),r.def&&r.def.inserted&&l.push(r));if(l.length){var d=function(){for(var t=0;t<l.length;t++)lo(l[t],"inserted",n,e)};a?ln(n,"insert",d):d()}u.length&&ln(n,"postpatch",(function(){for(var t=0;t<u.length;t++)lo(u[t],"componentUpdated",n,e)}));if(!a)for(t in i)c[t]||lo(i[t],"unbind",e,e,s)}(e,n)}var so=Object.create(null);function io(e,n){var t,o,r=Object.create(null);if(!e)return r;for(t=0;t<e.length;t++)(o=e[t]).modifiers||(o.modifiers=so),r[co(o)]=o,o.def=Be(n.$options,"directives",o.name);return r}function co(e){return e.rawName||e.name+"."+Object.keys(e.modifiers||{}).join(".")}function lo(e,n,t,o,r){var a=e.def&&e.def[n];if(a)try{a(t.elm,e,t,o,r)}catch(o){$e(o,t.context,"directive "+e.name+" "+n+" hook")}}var uo=[Qt,ro];function po(e,n){var t=n.componentOptions;if(!(s(t)&&!1===t.Ctor.options.inheritAttrs||a(e.data.attrs)&&a(n.data.attrs))){var o,r,i=n.elm,c=e.data.attrs||{},l=n.data.attrs||{};for(o in s(l.__ob__)&&(l=n.data.attrs=U({},l)),l)r=l[o],c[o]!==r&&mo(i,o,r);for(o in(Y||Q)&&l.value!==c.value&&mo(i,"value",l.value),c)a(l[o])&&(Bt(o)?i.removeAttributeNS(Lt,Rt(o)):Dt(o)||i.removeAttribute(o))}}function mo(e,n,t){e.tagName.indexOf("-")>-1?fo(e,n,t):jt(n)?Nt(t)?e.removeAttribute(n):(t="allowfullscreen"===n&&"EMBED"===e.tagName?"true":n,e.setAttribute(n,t)):Dt(n)?e.setAttribute(n,function(e,n){return Nt(n)||"false"===n?"false":"contenteditable"===e&&Mt(n)?n:"true"}(n,t)):Bt(n)?Nt(t)?e.removeAttributeNS(Lt,Rt(n)):e.setAttributeNS(Lt,n,t):fo(e,n,t)}function fo(e,n,t){if(Nt(t))e.removeAttribute(n);else{if(Y&&!X&&"TEXTAREA"===e.tagName&&"placeholder"===n&&""!==t&&!e.__ieph){var o=function(n){n.stopImmediatePropagation(),e.removeEventListener("input",o)};e.addEventListener("input",o),e.__ieph=!0}e.setAttribute(n,t)}}var ho={create:po,update:po};function go(e,n){var t=n.elm,o=n.data,r=e.data;if(!(a(o.staticClass)&&a(o.class)&&(a(r)||a(r.staticClass)&&a(r.class)))){var i=Ft(n),c=t._transitionClasses;s(c)&&(i=$t(i,qt(c))),i!==t._prevClass&&(t.setAttribute("class",i),t._prevClass=i)}}var vo,bo={create:go,update:go};function _o(e,n,t){var o=vo;return function r(){var a=n.apply(null,arguments);null!==a&&ko(e,r,t,o)}}var wo=Ke&&!(ee&&Number(ee[1])<=53);function yo(e,n,t,o){if(wo){var r=ct,a=n;n=a._wrapper=function(e){if(e.target===e.currentTarget||e.timeStamp>=r||e.timeStamp<=0||e.target.ownerDocument!==document)return a.apply(this,arguments)}}vo.addEventListener(e,n,te?{capture:t,passive:o}:t)}function ko(e,n,t,o){(o||vo).removeEventListener(e,n._wrapper||n,t)}function xo(e,n){if(!a(e.data.on)||!a(n.data.on)){var t=n.data.on||{},o=e.data.on||{};vo=n.elm,function(e){if(s(e.__r)){var n=Y?"change":"input";e[n]=[].concat(e.__r,e[n]||[]),delete e.__r}s(e.__c)&&(e.change=[].concat(e.__c,e.change||[]),delete e.__c)}(t),cn(t,o,yo,ko,_o,n.context),vo=void 0}}var Po,So={create:xo,update:xo};function Co(e,n){if(!a(e.data.domProps)||!a(n.data.domProps)){var t,o,r=n.elm,i=e.data.domProps||{},c=n.data.domProps||{};for(t in s(c.__ob__)&&(c=n.data.domProps=U({},c)),i)t in c||(r[t]="");for(t in c){if(o=c[t],"textContent"===t||"innerHTML"===t){if(n.children&&(n.children.length=0),o===i[t])continue;1===r.childNodes.length&&r.removeChild(r.childNodes[0])}if("value"===t&&"PROGRESS"!==r.tagName){r._value=o;var l=a(o)?"":String(o);Eo(r,l)&&(r.value=l)}else if("innerHTML"===t&&Ht(r.tagName)&&a(r.innerHTML)){(Po=Po||document.createElement("div")).innerHTML="<svg>"+o+"</svg>";for(var u=Po.firstChild;r.firstChild;)r.removeChild(r.firstChild);for(;u.firstChild;)r.appendChild(u.firstChild)}else if(o!==i[t])try{r[t]=o}catch(e){}}}}function Eo(e,n){return!e.composing&&("OPTION"===e.tagName||function(e,n){var t=!0;try{t=document.activeElement!==e}catch(e){}return t&&e.value!==n}(e,n)||function(e,n){var t=e.value,o=e._vModifiers;if(s(o)){if(o.number)return g(t)!==g(n);if(o.trim)return t.trim()!==n.trim()}return t!==n}(e,n))}var Io={create:Co,update:Co},To=k((function(e){var n={},t=/:(.+)/;return e.split(/;(?![^(]*\))/g).forEach((function(e){if(e){var o=e.split(t);o.length>1&&(n[o[0].trim()]=o[1].trim())}})),n}));function Uo(e){var n=zo(e.style);return e.staticStyle?U(e.staticStyle,n):n}function zo(e){return Array.isArray(e)?z(e):"string"==typeof e?To(e):e}var Oo,Ao=/^--/,Do=/\s*!important$/,Mo=function(e,n,t){if(Ao.test(n))e.style.setProperty(n,t);else if(Do.test(t))e.style.setProperty(E(n),t.replace(Do,""),"important");else{var o=Lo(n);if(Array.isArray(t))for(var r=0,a=t.length;r<a;r++)e.style[o]=t[r];else e.style[o]=t}},jo=["Webkit","Moz","ms"],Lo=k((function(e){if(Oo=Oo||document.createElement("div").style,"filter"!==(e=P(e))&&e in Oo)return e;for(var n=e.charAt(0).toUpperCase()+e.slice(1),t=0;t<jo.length;t++){var o=jo[t]+n;if(o in Oo)return o}}));function Bo(e,n){var t=n.data,o=e.data;if(!(a(t.staticStyle)&&a(t.style)&&a(o.staticStyle)&&a(o.style))){var r,i,c=n.elm,l=o.staticStyle,u=o.normalizedStyle||o.style||{},d=l||u,p=zo(n.data.style)||{};n.data.normalizedStyle=s(p.__ob__)?U({},p):p;var m=function(e,n){var t,o={};if(n)for(var r=e;r.componentInstance;)(r=r.componentInstance._vnode)&&r.data&&(t=Uo(r.data))&&U(o,t);(t=Uo(e.data))&&U(o,t);for(var a=e;a=a.parent;)a.data&&(t=Uo(a.data))&&U(o,t);return o}(n,!0);for(i in d)a(m[i])&&Mo(c,i,"");for(i in m)(r=m[i])!==d[i]&&Mo(c,i,null==r?"":r)}}var Ro={create:Bo,update:Bo},No=/\s+/;function Fo(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(No).forEach((function(n){return e.classList.add(n)})):e.classList.add(n);else{var t=" "+(e.getAttribute("class")||"")+" ";t.indexOf(" "+n+" ")<0&&e.setAttribute("class",(t+n).trim())}}function Go(e,n){if(n&&(n=n.trim()))if(e.classList)n.indexOf(" ")>-1?n.split(No).forEach((function(n){return e.classList.remove(n)})):e.classList.remove(n),e.classList.length||e.removeAttribute("class");else{for(var t=" "+(e.getAttribute("class")||"")+" ",o=" "+n+" ";t.indexOf(o)>=0;)t=t.replace(o," ");(t=t.trim())?e.setAttribute("class",t):e.removeAttribute("class")}}function $o(e){if(e){if("object"==typeof e){var n={};return!1!==e.css&&U(n,qo(e.name||"v")),U(n,e),n}return"string"==typeof e?qo(e):void 0}}var qo=k((function(e){return{enterClass:e+"-enter",enterToClass:e+"-enter-to",enterActiveClass:e+"-enter-active",leaveClass:e+"-leave",leaveToClass:e+"-leave-to",leaveActiveClass:e+"-leave-active"}})),Vo=W&&!X,Wo="transition",Ho="transitionend",Ko="animation",Jo="animationend";Vo&&(void 0===window.ontransitionend&&void 0!==window.onwebkittransitionend&&(Wo="WebkitTransition",Ho="webkitTransitionEnd"),void 0===window.onanimationend&&void 0!==window.onwebkitanimationend&&(Ko="WebkitAnimation",Jo="webkitAnimationEnd"));var Yo=W?window.requestAnimationFrame?window.requestAnimationFrame.bind(window):setTimeout:function(e){return e()};function Xo(e){Yo((function(){Yo(e)}))}function Qo(e,n){var t=e._transitionClasses||(e._transitionClasses=[]);t.indexOf(n)<0&&(t.push(n),Fo(e,n))}function Zo(e,n){e._transitionClasses&&_(e._transitionClasses,n),Go(e,n)}function er(e,n,t){var o=tr(e,n),r=o.type,a=o.timeout,s=o.propCount;if(!r)return t();var i="transition"===r?Ho:Jo,c=0,l=function(){e.removeEventListener(i,u),t()},u=function(n){n.target===e&&++c>=s&&l()};setTimeout((function(){c<s&&l()}),a+1),e.addEventListener(i,u)}var nr=/\b(transform|all)(,|$)/;function tr(e,n){var t,o=window.getComputedStyle(e),r=(o[Wo+"Delay"]||"").split(", "),a=(o[Wo+"Duration"]||"").split(", "),s=or(r,a),i=(o[Ko+"Delay"]||"").split(", "),c=(o[Ko+"Duration"]||"").split(", "),l=or(i,c),u=0,d=0;return"transition"===n?s>0&&(t="transition",u=s,d=a.length):"animation"===n?l>0&&(t="animation",u=l,d=c.length):d=(t=(u=Math.max(s,l))>0?s>l?"transition":"animation":null)?"transition"===t?a.length:c.length:0,{type:t,timeout:u,propCount:d,hasTransform:"transition"===t&&nr.test(o[Wo+"Property"])}}function or(e,n){for(;e.length<n.length;)e=e.concat(e);return Math.max.apply(null,n.map((function(n,t){return rr(n)+rr(e[t])})))}function rr(e){return 1e3*Number(e.slice(0,-1).replace(",","."))}function ar(e,n){var t=e.elm;s(t._leaveCb)&&(t._leaveCb.cancelled=!0,t._leaveCb());var o=$o(e.data.transition);if(!a(o)&&!s(t._enterCb)&&1===t.nodeType){for(var r=o.css,i=o.type,c=o.enterClass,u=o.enterToClass,d=o.enterActiveClass,p=o.appearClass,m=o.appearToClass,f=o.appearActiveClass,h=o.beforeEnter,v=o.enter,b=o.afterEnter,_=o.enterCancelled,w=o.beforeAppear,y=o.appear,k=o.afterAppear,x=o.appearCancelled,P=o.duration,S=Xn,C=Xn.$vnode;C&&C.parent;)S=C.context,C=C.parent;var E=!S._isMounted||!e.isRootInsert;if(!E||y||""===y){var I=E&&p?p:c,T=E&&f?f:d,U=E&&m?m:u,z=E&&w||h,O=E&&"function"==typeof y?y:v,A=E&&k||b,D=E&&x||_,M=g(l(P)?P.enter:P);0;var j=!1!==r&&!X,B=cr(O),R=t._enterCb=L((function(){j&&(Zo(t,U),Zo(t,T)),R.cancelled?(j&&Zo(t,I),D&&D(t)):A&&A(t),t._enterCb=null}));e.data.show||ln(e,"insert",(function(){var n=t.parentNode,o=n&&n._pending&&n._pending[e.key];o&&o.tag===e.tag&&o.elm._leaveCb&&o.elm._leaveCb(),O&&O(t,R)})),z&&z(t),j&&(Qo(t,I),Qo(t,T),Xo((function(){Zo(t,I),R.cancelled||(Qo(t,U),B||(ir(M)?setTimeout(R,M):er(t,i,R)))}))),e.data.show&&(n&&n(),O&&O(t,R)),j||B||R()}}}function sr(e,n){var t=e.elm;s(t._enterCb)&&(t._enterCb.cancelled=!0,t._enterCb());var o=$o(e.data.transition);if(a(o)||1!==t.nodeType)return n();if(!s(t._leaveCb)){var r=o.css,i=o.type,c=o.leaveClass,u=o.leaveToClass,d=o.leaveActiveClass,p=o.beforeLeave,m=o.leave,f=o.afterLeave,h=o.leaveCancelled,v=o.delayLeave,b=o.duration,_=!1!==r&&!X,w=cr(m),y=g(l(b)?b.leave:b);0;var k=t._leaveCb=L((function(){t.parentNode&&t.parentNode._pending&&(t.parentNode._pending[e.key]=null),_&&(Zo(t,u),Zo(t,d)),k.cancelled?(_&&Zo(t,c),h&&h(t)):(n(),f&&f(t)),t._leaveCb=null}));v?v(x):x()}function x(){k.cancelled||(!e.data.show&&t.parentNode&&((t.parentNode._pending||(t.parentNode._pending={}))[e.key]=e),p&&p(t),_&&(Qo(t,c),Qo(t,d),Xo((function(){Zo(t,c),k.cancelled||(Qo(t,u),w||(ir(y)?setTimeout(k,y):er(t,i,k)))}))),m&&m(t,k),_||w||k())}}function ir(e){return"number"==typeof e&&!isNaN(e)}function cr(e){if(a(e))return!1;var n=e.fns;return s(n)?cr(Array.isArray(n)?n[0]:n):(e._length||e.length)>1}function lr(e,n){!0!==n.data.show&&ar(n)}var ur=function(e){var n,t,o={},r=e.modules,l=e.nodeOps;for(n=0;n<no.length;++n)for(o[no[n]]=[],t=0;t<r.length;++t)s(r[t][no[n]])&&o[no[n]].push(r[t][no[n]]);function u(e){var n=l.parentNode(e);s(n)&&l.removeChild(n,e)}function d(e,n,t,r,a,c,u){if(s(e.elm)&&s(c)&&(e=c[u]=_e(e)),e.isRootInsert=!a,!function(e,n,t,r){var a=e.data;if(s(a)){var c=s(e.componentInstance)&&a.keepAlive;if(s(a=a.hook)&&s(a=a.init)&&a(e,!1),s(e.componentInstance))return p(e,n),m(t,e.elm,r),i(c)&&function(e,n,t,r){var a,i=e;for(;i.componentInstance;)if(i=i.componentInstance._vnode,s(a=i.data)&&s(a=a.transition)){for(a=0;a<o.activate.length;++a)o.activate[a](eo,i);n.push(i);break}m(t,e.elm,r)}(e,n,t,r),!0}}(e,n,t,r)){var d=e.data,h=e.children,v=e.tag;s(v)?(e.elm=e.ns?l.createElementNS(e.ns,v):l.createElement(v,e),b(e),f(e,h,n),s(d)&&g(e,n),m(t,e.elm,r)):i(e.isComment)?(e.elm=l.createComment(e.text),m(t,e.elm,r)):(e.elm=l.createTextNode(e.text),m(t,e.elm,r))}}function p(e,n){s(e.data.pendingInsert)&&(n.push.apply(n,e.data.pendingInsert),e.data.pendingInsert=null),e.elm=e.componentInstance.$el,h(e)?(g(e,n),b(e)):(Zt(e),n.push(e))}function m(e,n,t){s(e)&&(s(t)?l.parentNode(t)===e&&l.insertBefore(e,n,t):l.appendChild(e,n))}function f(e,n,t){if(Array.isArray(n)){0;for(var o=0;o<n.length;++o)d(n[o],t,e.elm,null,!0,n,o)}else c(e.text)&&l.appendChild(e.elm,l.createTextNode(String(e.text)))}function h(e){for(;e.componentInstance;)e=e.componentInstance._vnode;return s(e.tag)}function g(e,t){for(var r=0;r<o.create.length;++r)o.create[r](eo,e);s(n=e.data.hook)&&(s(n.create)&&n.create(eo,e),s(n.insert)&&t.push(e))}function b(e){var n;if(s(n=e.fnScopeId))l.setStyleScope(e.elm,n);else for(var t=e;t;)s(n=t.context)&&s(n=n.$options._scopeId)&&l.setStyleScope(e.elm,n),t=t.parent;s(n=Xn)&&n!==e.context&&n!==e.fnContext&&s(n=n.$options._scopeId)&&l.setStyleScope(e.elm,n)}function _(e,n,t,o,r,a){for(;o<=r;++o)d(t[o],a,e,n,!1,t,o)}function w(e){var n,t,r=e.data;if(s(r))for(s(n=r.hook)&&s(n=n.destroy)&&n(e),n=0;n<o.destroy.length;++n)o.destroy[n](e);if(s(n=e.children))for(t=0;t<e.children.length;++t)w(e.children[t])}function y(e,n,t){for(;n<=t;++n){var o=e[n];s(o)&&(s(o.tag)?(k(o),w(o)):u(o.elm))}}function k(e,n){if(s(n)||s(e.data)){var t,r=o.remove.length+1;for(s(n)?n.listeners+=r:n=function(e,n){function t(){0==--t.listeners&&u(e)}return t.listeners=n,t}(e.elm,r),s(t=e.componentInstance)&&s(t=t._vnode)&&s(t.data)&&k(t,n),t=0;t<o.remove.length;++t)o.remove[t](e,n);s(t=e.data.hook)&&s(t=t.remove)?t(e,n):n()}else u(e.elm)}function x(e,n,t,o){for(var r=t;r<o;r++){var a=n[r];if(s(a)&&to(e,a))return r}}function P(e,n,t,r,c,u){if(e!==n){s(n.elm)&&s(r)&&(n=r[c]=_e(n));var p=n.elm=e.elm;if(i(e.isAsyncPlaceholder))s(n.asyncFactory.resolved)?E(e.elm,n,t):n.isAsyncPlaceholder=!0;else if(i(n.isStatic)&&i(e.isStatic)&&n.key===e.key&&(i(n.isCloned)||i(n.isOnce)))n.componentInstance=e.componentInstance;else{var m,f=n.data;s(f)&&s(m=f.hook)&&s(m=m.prepatch)&&m(e,n);var g=e.children,v=n.children;if(s(f)&&h(n)){for(m=0;m<o.update.length;++m)o.update[m](e,n);s(m=f.hook)&&s(m=m.update)&&m(e,n)}a(n.text)?s(g)&&s(v)?g!==v&&function(e,n,t,o,r){var i,c,u,p=0,m=0,f=n.length-1,h=n[0],g=n[f],v=t.length-1,b=t[0],w=t[v],k=!r;for(0;p<=f&&m<=v;)a(h)?h=n[++p]:a(g)?g=n[--f]:to(h,b)?(P(h,b,o,t,m),h=n[++p],b=t[++m]):to(g,w)?(P(g,w,o,t,v),g=n[--f],w=t[--v]):to(h,w)?(P(h,w,o,t,v),k&&l.insertBefore(e,h.elm,l.nextSibling(g.elm)),h=n[++p],w=t[--v]):to(g,b)?(P(g,b,o,t,m),k&&l.insertBefore(e,g.elm,h.elm),g=n[--f],b=t[++m]):(a(i)&&(i=oo(n,p,f)),a(c=s(b.key)?i[b.key]:x(b,n,p,f))?d(b,o,e,h.elm,!1,t,m):to(u=n[c],b)?(P(u,b,o,t,m),n[c]=void 0,k&&l.insertBefore(e,u.elm,h.elm)):d(b,o,e,h.elm,!1,t,m),b=t[++m]);p>f?_(e,a(t[v+1])?null:t[v+1].elm,t,m,v,o):m>v&&y(n,p,f)}(p,g,v,t,u):s(v)?(s(e.text)&&l.setTextContent(p,""),_(p,null,v,0,v.length-1,t)):s(g)?y(g,0,g.length-1):s(e.text)&&l.setTextContent(p,""):e.text!==n.text&&l.setTextContent(p,n.text),s(f)&&s(m=f.hook)&&s(m=m.postpatch)&&m(e,n)}}}function S(e,n,t){if(i(t)&&s(e.parent))e.parent.data.pendingInsert=n;else for(var o=0;o<n.length;++o)n[o].data.hook.insert(n[o])}var C=v("attrs,class,staticClass,staticStyle,key");function E(e,n,t,o){var r,a=n.tag,c=n.data,l=n.children;if(o=o||c&&c.pre,n.elm=e,i(n.isComment)&&s(n.asyncFactory))return n.isAsyncPlaceholder=!0,!0;if(s(c)&&(s(r=c.hook)&&s(r=r.init)&&r(n,!0),s(r=n.componentInstance)))return p(n,t),!0;if(s(a)){if(s(l))if(e.hasChildNodes())if(s(r=c)&&s(r=r.domProps)&&s(r=r.innerHTML)){if(r!==e.innerHTML)return!1}else{for(var u=!0,d=e.firstChild,m=0;m<l.length;m++){if(!d||!E(d,l[m],t,o)){u=!1;break}d=d.nextSibling}if(!u||d)return!1}else f(n,l,t);if(s(c)){var h=!1;for(var v in c)if(!C(v)){h=!0,g(n,t);break}!h&&c.class&&rn(c.class)}}else e.data!==n.text&&(e.data=n.text);return!0}return function(e,n,t,r){if(!a(n)){var c,u=!1,p=[];if(a(e))u=!0,d(n,p);else{var m=s(e.nodeType);if(!m&&to(e,n))P(e,n,p,null,null,r);else{if(m){if(1===e.nodeType&&e.hasAttribute("data-server-rendered")&&(e.removeAttribute("data-server-rendered"),t=!0),i(t)&&E(e,n,p))return S(n,p,!0),e;c=e,e=new he(l.tagName(c).toLowerCase(),{},[],void 0,c)}var f=e.elm,g=l.parentNode(f);if(d(n,p,f._leaveCb?null:g,l.nextSibling(f)),s(n.parent))for(var v=n.parent,b=h(n);v;){for(var _=0;_<o.destroy.length;++_)o.destroy[_](v);if(v.elm=n.elm,b){for(var k=0;k<o.create.length;++k)o.create[k](eo,v);var x=v.data.hook.insert;if(x.merged)for(var C=1;C<x.fns.length;C++)x.fns[C]()}else Zt(v);v=v.parent}s(g)?y([e],0,0):s(e.tag)&&w(e)}}return S(n,p,u),n.elm}s(e)&&w(e)}}({nodeOps:Xt,modules:[ho,bo,So,Io,Ro,W?{create:lr,activate:lr,remove:function(e,n){!0!==e.data.show?sr(e,n):n()}}:{}].concat(uo)});X&&document.addEventListener("selectionchange",(function(){var e=document.activeElement;e&&e.vmodel&&br(e,"input")}));var dr={inserted:function(e,n,t,o){"select"===t.tag?(o.elm&&!o.elm._vOptions?ln(t,"postpatch",(function(){dr.componentUpdated(e,n,t)})):pr(e,n,t.context),e._vOptions=[].map.call(e.options,hr)):("textarea"===t.tag||Yt(e.type))&&(e._vModifiers=n.modifiers,n.modifiers.lazy||(e.addEventListener("compositionstart",gr),e.addEventListener("compositionend",vr),e.addEventListener("change",vr),X&&(e.vmodel=!0)))},componentUpdated:function(e,n,t){if("select"===t.tag){pr(e,n,t.context);var o=e._vOptions,r=e._vOptions=[].map.call(e.options,hr);if(r.some((function(e,n){return!M(e,o[n])})))(e.multiple?n.value.some((function(e){return fr(e,r)})):n.value!==n.oldValue&&fr(n.value,r))&&br(e,"change")}}};function pr(e,n,t){mr(e,n,t),(Y||Q)&&setTimeout((function(){mr(e,n,t)}),0)}function mr(e,n,t){var o=n.value,r=e.multiple;if(!r||Array.isArray(o)){for(var a,s,i=0,c=e.options.length;i<c;i++)if(s=e.options[i],r)a=j(o,hr(s))>-1,s.selected!==a&&(s.selected=a);else if(M(hr(s),o))return void(e.selectedIndex!==i&&(e.selectedIndex=i));r||(e.selectedIndex=-1)}}function fr(e,n){return n.every((function(n){return!M(n,e)}))}function hr(e){return"_value"in e?e._value:e.value}function gr(e){e.target.composing=!0}function vr(e){e.target.composing&&(e.target.composing=!1,br(e.target,"input"))}function br(e,n){var t=document.createEvent("HTMLEvents");t.initEvent(n,!0,!0),e.dispatchEvent(t)}function _r(e){return!e.componentInstance||e.data&&e.data.transition?e:_r(e.componentInstance._vnode)}var wr={model:dr,show:{bind:function(e,n,t){var o=n.value,r=(t=_r(t)).data&&t.data.transition,a=e.__vOriginalDisplay="none"===e.style.display?"":e.style.display;o&&r?(t.data.show=!0,ar(t,(function(){e.style.display=a}))):e.style.display=o?a:"none"},update:function(e,n,t){var o=n.value;!o!=!n.oldValue&&((t=_r(t)).data&&t.data.transition?(t.data.show=!0,o?ar(t,(function(){e.style.display=e.__vOriginalDisplay})):sr(t,(function(){e.style.display="none"}))):e.style.display=o?e.__vOriginalDisplay:"none")},unbind:function(e,n,t,o,r){r||(e.style.display=e.__vOriginalDisplay)}}},yr={name:String,appear:Boolean,css:Boolean,mode:String,type:String,enterClass:String,leaveClass:String,enterToClass:String,leaveToClass:String,enterActiveClass:String,leaveActiveClass:String,appearClass:String,appearActiveClass:String,appearToClass:String,duration:[Number,String,Object]};function kr(e){var n=e&&e.componentOptions;return n&&n.Ctor.options.abstract?kr(Wn(n.children)):e}function xr(e){var n={},t=e.$options;for(var o in t.propsData)n[o]=e[o];var r=t._parentListeners;for(var a in r)n[P(a)]=r[a];return n}function Pr(e,n){if(/\d-keep-alive$/.test(n.tag))return e("keep-alive",{props:n.componentOptions.propsData})}var Sr=function(e){return e.tag||Vn(e)},Cr=function(e){return"show"===e.name},Er={name:"transition",props:yr,abstract:!0,render:function(e){var n=this,t=this.$slots.default;if(t&&(t=t.filter(Sr)).length){0;var o=this.mode;0;var r=t[0];if(function(e){for(;e=e.parent;)if(e.data.transition)return!0}(this.$vnode))return r;var a=kr(r);if(!a)return r;if(this._leaving)return Pr(e,r);var s="__transition-"+this._uid+"-";a.key=null==a.key?a.isComment?s+"comment":s+a.tag:c(a.key)?0===String(a.key).indexOf(s)?a.key:s+a.key:a.key;var i=(a.data||(a.data={})).transition=xr(this),l=this._vnode,u=kr(l);if(a.data.directives&&a.data.directives.some(Cr)&&(a.data.show=!0),u&&u.data&&!function(e,n){return n.key===e.key&&n.tag===e.tag}(a,u)&&!Vn(u)&&(!u.componentInstance||!u.componentInstance._vnode.isComment)){var d=u.data.transition=U({},i);if("out-in"===o)return this._leaving=!0,ln(d,"afterLeave",(function(){n._leaving=!1,n.$forceUpdate()})),Pr(e,r);if("in-out"===o){if(Vn(a))return l;var p,m=function(){p()};ln(i,"afterEnter",m),ln(i,"enterCancelled",m),ln(d,"delayLeave",(function(e){p=e}))}}return r}}},Ir=U({tag:String,moveClass:String},yr);function Tr(e){e.elm._moveCb&&e.elm._moveCb(),e.elm._enterCb&&e.elm._enterCb()}function Ur(e){e.data.newPos=e.elm.getBoundingClientRect()}function zr(e){var n=e.data.pos,t=e.data.newPos,o=n.left-t.left,r=n.top-t.top;if(o||r){e.data.moved=!0;var a=e.elm.style;a.transform=a.WebkitTransform="translate("+o+"px,"+r+"px)",a.transitionDuration="0s"}}delete Ir.mode;var Or={Transition:Er,TransitionGroup:{props:Ir,beforeMount:function(){var e=this,n=this._update;this._update=function(t,o){var r=Qn(e);e.__patch__(e._vnode,e.kept,!1,!0),e._vnode=e.kept,r(),n.call(e,t,o)}},render:function(e){for(var n=this.tag||this.$vnode.data.tag||"span",t=Object.create(null),o=this.prevChildren=this.children,r=this.$slots.default||[],a=this.children=[],s=xr(this),i=0;i<r.length;i++){var c=r[i];if(c.tag)if(null!=c.key&&0!==String(c.key).indexOf("__vlist"))a.push(c),t[c.key]=c,(c.data||(c.data={})).transition=s;else;}if(o){for(var l=[],u=[],d=0;d<o.length;d++){var p=o[d];p.data.transition=s,p.data.pos=p.elm.getBoundingClientRect(),t[p.key]?l.push(p):u.push(p)}this.kept=e(n,null,l),this.removed=u}return e(n,null,a)},updated:function(){var e=this.prevChildren,n=this.moveClass||(this.name||"v")+"-move";e.length&&this.hasMove(e[0].elm,n)&&(e.forEach(Tr),e.forEach(Ur),e.forEach(zr),this._reflow=document.body.offsetHeight,e.forEach((function(e){if(e.data.moved){var t=e.elm,o=t.style;Qo(t,n),o.transform=o.WebkitTransform=o.transitionDuration="",t.addEventListener(Ho,t._moveCb=function e(o){o&&o.target!==t||o&&!/transform$/.test(o.propertyName)||(t.removeEventListener(Ho,e),t._moveCb=null,Zo(t,n))})}})))},methods:{hasMove:function(e,n){if(!Vo)return!1;if(this._hasMove)return this._hasMove;var t=e.cloneNode();e._transitionClasses&&e._transitionClasses.forEach((function(e){Go(t,e)})),Fo(t,n),t.style.display="none",this.$el.appendChild(t);var o=tr(t);return this.$el.removeChild(t),this._hasMove=o.hasTransform}}}};Pt.config.mustUseProp=function(e,n,t){return"value"===t&&At(e)&&"button"!==n||"selected"===t&&"option"===e||"checked"===t&&"input"===e||"muted"===t&&"video"===e},Pt.config.isReservedTag=Kt,Pt.config.isReservedAttr=Ot,Pt.config.getTagNamespace=function(e){return Ht(e)?"svg":"math"===e?"math":void 0},Pt.config.isUnknownElement=function(e){if(!W)return!0;if(Kt(e))return!1;if(e=e.toLowerCase(),null!=Jt[e])return Jt[e];var n=document.createElement(e);return e.indexOf("-")>-1?Jt[e]=n.constructor===window.HTMLUnknownElement||n.constructor===window.HTMLElement:Jt[e]=/HTMLUnknownElement/.test(n.toString())},U(Pt.options.directives,wr),U(Pt.options.components,Or),Pt.prototype.__patch__=W?ur:O,Pt.prototype.$mount=function(e,n){return function(e,n,t){var o;return e.$el=n,e.$options.render||(e.$options.render=ve),nt(e,"beforeMount"),o=function(){e._update(e._render(),t)},new mt(e,o,O,{before:function(){e._isMounted&&!e._isDestroyed&&nt(e,"beforeUpdate")}},!0),t=!1,null==e.$vnode&&(e._isMounted=!0,nt(e,"mounted")),e}(this,e=e&&W?function(e){if("string"==typeof e){var n=document.querySelector(e);return n||document.createElement("div")}return e}(e):void 0,n)},W&&setTimeout((function(){N.devtools&&ae&&ae.emit("init",Pt)}),0);var Ar=Pt;
/*!
  * vue-router v3.4.9
  * (c) 2020 Evan You
  * @license MIT
  */function Dr(e,n){for(var t in n)e[t]=n[t];return e}var Mr=/[!'()*]/g,jr=function(e){return"%"+e.charCodeAt(0).toString(16)},Lr=/%2C/g,Br=function(e){return encodeURIComponent(e).replace(Mr,jr).replace(Lr,",")};function Rr(e){try{return decodeURIComponent(e)}catch(e){0}return e}var Nr=function(e){return null==e||"object"==typeof e?e:String(e)};function Fr(e){var n={};return(e=e.trim().replace(/^(\?|#|&)/,""))?(e.split("&").forEach((function(e){var t=e.replace(/\+/g," ").split("="),o=Rr(t.shift()),r=t.length>0?Rr(t.join("=")):null;void 0===n[o]?n[o]=r:Array.isArray(n[o])?n[o].push(r):n[o]=[n[o],r]})),n):n}function Gr(e){var n=e?Object.keys(e).map((function(n){var t=e[n];if(void 0===t)return"";if(null===t)return Br(n);if(Array.isArray(t)){var o=[];return t.forEach((function(e){void 0!==e&&(null===e?o.push(Br(n)):o.push(Br(n)+"="+Br(e)))})),o.join("&")}return Br(n)+"="+Br(t)})).filter((function(e){return e.length>0})).join("&"):null;return n?"?"+n:""}var $r=/\/?$/;function qr(e,n,t,o){var r=o&&o.options.stringifyQuery,a=n.query||{};try{a=Vr(a)}catch(e){}var s={name:n.name||e&&e.name,meta:e&&e.meta||{},path:n.path||"/",hash:n.hash||"",query:a,params:n.params||{},fullPath:Kr(n,r),matched:e?Hr(e):[]};return t&&(s.redirectedFrom=Kr(t,r)),Object.freeze(s)}function Vr(e){if(Array.isArray(e))return e.map(Vr);if(e&&"object"==typeof e){var n={};for(var t in e)n[t]=Vr(e[t]);return n}return e}var Wr=qr(null,{path:"/"});function Hr(e){for(var n=[];e;)n.unshift(e),e=e.parent;return n}function Kr(e,n){var t=e.path,o=e.query;void 0===o&&(o={});var r=e.hash;return void 0===r&&(r=""),(t||"/")+(n||Gr)(o)+r}function Jr(e,n){return n===Wr?e===n:!!n&&(e.path&&n.path?e.path.replace($r,"")===n.path.replace($r,"")&&e.hash===n.hash&&Yr(e.query,n.query):!(!e.name||!n.name)&&(e.name===n.name&&e.hash===n.hash&&Yr(e.query,n.query)&&Yr(e.params,n.params)))}function Yr(e,n){if(void 0===e&&(e={}),void 0===n&&(n={}),!e||!n)return e===n;var t=Object.keys(e).sort(),o=Object.keys(n).sort();return t.length===o.length&&t.every((function(t,r){var a=e[t];if(o[r]!==t)return!1;var s=n[t];return null==a||null==s?a===s:"object"==typeof a&&"object"==typeof s?Yr(a,s):String(a)===String(s)}))}function Xr(e){for(var n=0;n<e.matched.length;n++){var t=e.matched[n];for(var o in t.instances){var r=t.instances[o],a=t.enteredCbs[o];if(r&&a){delete t.enteredCbs[o];for(var s=0;s<a.length;s++)r._isBeingDestroyed||a[s](r)}}}}var Qr={name:"RouterView",functional:!0,props:{name:{type:String,default:"default"}},render:function(e,n){var t=n.props,o=n.children,r=n.parent,a=n.data;a.routerView=!0;for(var s=r.$createElement,i=t.name,c=r.$route,l=r._routerViewCache||(r._routerViewCache={}),u=0,d=!1;r&&r._routerRoot!==r;){var p=r.$vnode?r.$vnode.data:{};p.routerView&&u++,p.keepAlive&&r._directInactive&&r._inactive&&(d=!0),r=r.$parent}if(a.routerViewDepth=u,d){var m=l[i],f=m&&m.component;return f?(m.configProps&&Zr(f,a,m.route,m.configProps),s(f,a,o)):s()}var h=c.matched[u],g=h&&h.components[i];if(!h||!g)return l[i]=null,s();l[i]={component:g},a.registerRouteInstance=function(e,n){var t=h.instances[i];(n&&t!==e||!n&&t===e)&&(h.instances[i]=n)},(a.hook||(a.hook={})).prepatch=function(e,n){h.instances[i]=n.componentInstance},a.hook.init=function(e){e.data.keepAlive&&e.componentInstance&&e.componentInstance!==h.instances[i]&&(h.instances[i]=e.componentInstance),Xr(c)};var v=h.props&&h.props[i];return v&&(Dr(l[i],{route:c,configProps:v}),Zr(g,a,c,v)),s(g,a,o)}};function Zr(e,n,t,o){var r=n.props=function(e,n){switch(typeof n){case"undefined":return;case"object":return n;case"function":return n(e);case"boolean":return n?e.params:void 0;default:0}}(t,o);if(r){r=n.props=Dr({},r);var a=n.attrs=n.attrs||{};for(var s in r)e.props&&s in e.props||(a[s]=r[s],delete r[s])}}function ea(e,n,t){var o=e.charAt(0);if("/"===o)return e;if("?"===o||"#"===o)return n+e;var r=n.split("/");t&&r[r.length-1]||r.pop();for(var a=e.replace(/^\//,"").split("/"),s=0;s<a.length;s++){var i=a[s];".."===i?r.pop():"."!==i&&r.push(i)}return""!==r[0]&&r.unshift(""),r.join("/")}function na(e){return e.replace(/\/\//g,"/")}var ta=Array.isArray||function(e){return"[object Array]"==Object.prototype.toString.call(e)},oa=va,ra=la,aa=function(e,n){return da(la(e,n),n)},sa=da,ia=ga,ca=new RegExp(["(\\\\.)","([\\/.])?(?:(?:\\:(\\w+)(?:\\(((?:\\\\.|[^\\\\()])+)\\))?|\\(((?:\\\\.|[^\\\\()])+)\\))([+*?])?|(\\*))"].join("|"),"g");function la(e,n){for(var t,o=[],r=0,a=0,s="",i=n&&n.delimiter||"/";null!=(t=ca.exec(e));){var c=t[0],l=t[1],u=t.index;if(s+=e.slice(a,u),a=u+c.length,l)s+=l[1];else{var d=e[a],p=t[2],m=t[3],f=t[4],h=t[5],g=t[6],v=t[7];s&&(o.push(s),s="");var b=null!=p&&null!=d&&d!==p,_="+"===g||"*"===g,w="?"===g||"*"===g,y=t[2]||i,k=f||h;o.push({name:m||r++,prefix:p||"",delimiter:y,optional:w,repeat:_,partial:b,asterisk:!!v,pattern:k?ma(k):v?".*":"[^"+pa(y)+"]+?"})}}return a<e.length&&(s+=e.substr(a)),s&&o.push(s),o}function ua(e){return encodeURI(e).replace(/[\/?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()}))}function da(e,n){for(var t=new Array(e.length),o=0;o<e.length;o++)"object"==typeof e[o]&&(t[o]=new RegExp("^(?:"+e[o].pattern+")$",ha(n)));return function(n,o){for(var r="",a=n||{},s=(o||{}).pretty?ua:encodeURIComponent,i=0;i<e.length;i++){var c=e[i];if("string"!=typeof c){var l,u=a[c.name];if(null==u){if(c.optional){c.partial&&(r+=c.prefix);continue}throw new TypeError('Expected "'+c.name+'" to be defined')}if(ta(u)){if(!c.repeat)throw new TypeError('Expected "'+c.name+'" to not repeat, but received `'+JSON.stringify(u)+"`");if(0===u.length){if(c.optional)continue;throw new TypeError('Expected "'+c.name+'" to not be empty')}for(var d=0;d<u.length;d++){if(l=s(u[d]),!t[i].test(l))throw new TypeError('Expected all "'+c.name+'" to match "'+c.pattern+'", but received `'+JSON.stringify(l)+"`");r+=(0===d?c.prefix:c.delimiter)+l}}else{if(l=c.asterisk?encodeURI(u).replace(/[?#]/g,(function(e){return"%"+e.charCodeAt(0).toString(16).toUpperCase()})):s(u),!t[i].test(l))throw new TypeError('Expected "'+c.name+'" to match "'+c.pattern+'", but received "'+l+'"');r+=c.prefix+l}}else r+=c}return r}}function pa(e){return e.replace(/([.+*?=^!:${}()[\]|\/\\])/g,"\\$1")}function ma(e){return e.replace(/([=!:$\/()])/g,"\\$1")}function fa(e,n){return e.keys=n,e}function ha(e){return e&&e.sensitive?"":"i"}function ga(e,n,t){ta(n)||(t=n||t,n=[]);for(var o=(t=t||{}).strict,r=!1!==t.end,a="",s=0;s<e.length;s++){var i=e[s];if("string"==typeof i)a+=pa(i);else{var c=pa(i.prefix),l="(?:"+i.pattern+")";n.push(i),i.repeat&&(l+="(?:"+c+l+")*"),a+=l=i.optional?i.partial?c+"("+l+")?":"(?:"+c+"("+l+"))?":c+"("+l+")"}}var u=pa(t.delimiter||"/"),d=a.slice(-u.length)===u;return o||(a=(d?a.slice(0,-u.length):a)+"(?:"+u+"(?=$))?"),a+=r?"$":o&&d?"":"(?="+u+"|$)",fa(new RegExp("^"+a,ha(t)),n)}function va(e,n,t){return ta(n)||(t=n||t,n=[]),t=t||{},e instanceof RegExp?function(e,n){var t=e.source.match(/\((?!\?)/g);if(t)for(var o=0;o<t.length;o++)n.push({name:o,prefix:null,delimiter:null,optional:!1,repeat:!1,partial:!1,asterisk:!1,pattern:null});return fa(e,n)}(e,n):ta(e)?function(e,n,t){for(var o=[],r=0;r<e.length;r++)o.push(va(e[r],n,t).source);return fa(new RegExp("(?:"+o.join("|")+")",ha(t)),n)}(e,n,t):function(e,n,t){return ga(la(e,t),n,t)}(e,n,t)}oa.parse=ra,oa.compile=aa,oa.tokensToFunction=sa,oa.tokensToRegExp=ia;var ba=Object.create(null);function _a(e,n,t){n=n||{};try{var o=ba[e]||(ba[e]=oa.compile(e));return"string"==typeof n.pathMatch&&(n[0]=n.pathMatch),o(n,{pretty:!0})}catch(e){return""}finally{delete n[0]}}function wa(e,n,t,o){var r="string"==typeof e?{path:e}:e;if(r._normalized)return r;if(r.name){var a=(r=Dr({},e)).params;return a&&"object"==typeof a&&(r.params=Dr({},a)),r}if(!r.path&&r.params&&n){(r=Dr({},r))._normalized=!0;var s=Dr(Dr({},n.params),r.params);if(n.name)r.name=n.name,r.params=s;else if(n.matched.length){var i=n.matched[n.matched.length-1].path;r.path=_a(i,s,n.path)}else 0;return r}var c=function(e){var n="",t="",o=e.indexOf("#");o>=0&&(n=e.slice(o),e=e.slice(0,o));var r=e.indexOf("?");return r>=0&&(t=e.slice(r+1),e=e.slice(0,r)),{path:e,query:t,hash:n}}(r.path||""),l=n&&n.path||"/",u=c.path?ea(c.path,l,t||r.append):l,d=function(e,n,t){void 0===n&&(n={});var o,r=t||Fr;try{o=r(e||"")}catch(e){o={}}for(var a in n){var s=n[a];o[a]=Array.isArray(s)?s.map(Nr):Nr(s)}return o}(c.query,r.query,o&&o.options.parseQuery),p=r.hash||c.hash;return p&&"#"!==p.charAt(0)&&(p="#"+p),{_normalized:!0,path:u,query:d,hash:p}}var ya,ka=function(){},xa={name:"RouterLink",props:{to:{type:[String,Object],required:!0},tag:{type:String,default:"a"},exact:Boolean,append:Boolean,replace:Boolean,activeClass:String,exactActiveClass:String,ariaCurrentValue:{type:String,default:"page"},event:{type:[String,Array],default:"click"}},render:function(e){var n=this,t=this.$router,o=this.$route,r=t.resolve(this.to,o,this.append),a=r.location,s=r.route,i=r.href,c={},l=t.options.linkActiveClass,u=t.options.linkExactActiveClass,d=null==l?"router-link-active":l,p=null==u?"router-link-exact-active":u,m=null==this.activeClass?d:this.activeClass,f=null==this.exactActiveClass?p:this.exactActiveClass,h=s.redirectedFrom?qr(null,wa(s.redirectedFrom),null,t):s;c[f]=Jr(o,h),c[m]=this.exact?c[f]:function(e,n){return 0===e.path.replace($r,"/").indexOf(n.path.replace($r,"/"))&&(!n.hash||e.hash===n.hash)&&function(e,n){for(var t in n)if(!(t in e))return!1;return!0}(e.query,n.query)}(o,h);var g=c[f]?this.ariaCurrentValue:null,v=function(e){Pa(e)&&(n.replace?t.replace(a,ka):t.push(a,ka))},b={click:Pa};Array.isArray(this.event)?this.event.forEach((function(e){b[e]=v})):b[this.event]=v;var _={class:c},w=!this.$scopedSlots.$hasNormal&&this.$scopedSlots.default&&this.$scopedSlots.default({href:i,route:s,navigate:v,isActive:c[m],isExactActive:c[f]});if(w){if(1===w.length)return w[0];if(w.length>1||!w.length)return 0===w.length?e():e("span",{},w)}if("a"===this.tag)_.on=b,_.attrs={href:i,"aria-current":g};else{var y=function e(n){var t;if(n)for(var o=0;o<n.length;o++){if("a"===(t=n[o]).tag)return t;if(t.children&&(t=e(t.children)))return t}}(this.$slots.default);if(y){y.isStatic=!1;var k=y.data=Dr({},y.data);for(var x in k.on=k.on||{},k.on){var P=k.on[x];x in b&&(k.on[x]=Array.isArray(P)?P:[P])}for(var S in b)S in k.on?k.on[S].push(b[S]):k.on[S]=v;var C=y.data.attrs=Dr({},y.data.attrs);C.href=i,C["aria-current"]=g}else _.on=b}return e(this.tag,_,this.$slots.default)}};function Pa(e){if(!(e.metaKey||e.altKey||e.ctrlKey||e.shiftKey||e.defaultPrevented||void 0!==e.button&&0!==e.button)){if(e.currentTarget&&e.currentTarget.getAttribute){var n=e.currentTarget.getAttribute("target");if(/\b_blank\b/i.test(n))return}return e.preventDefault&&e.preventDefault(),!0}}var Sa="undefined"!=typeof window;function Ca(e,n,t,o){var r=n||[],a=t||Object.create(null),s=o||Object.create(null);e.forEach((function(e){!function e(n,t,o,r,a,s){var i=r.path,c=r.name;0;var l=r.pathToRegexpOptions||{},u=function(e,n,t){t||(e=e.replace(/\/$/,""));if("/"===e[0])return e;if(null==n)return e;return na(n.path+"/"+e)}(i,a,l.strict);"boolean"==typeof r.caseSensitive&&(l.sensitive=r.caseSensitive);var d={path:u,regex:Ea(u,l),components:r.components||{default:r.component},instances:{},enteredCbs:{},name:c,parent:a,matchAs:s,redirect:r.redirect,beforeEnter:r.beforeEnter,meta:r.meta||{},props:null==r.props?{}:r.components?r.props:{default:r.props}};r.children&&r.children.forEach((function(r){var a=s?na(s+"/"+r.path):void 0;e(n,t,o,r,d,a)}));t[d.path]||(n.push(d.path),t[d.path]=d);if(void 0!==r.alias)for(var p=Array.isArray(r.alias)?r.alias:[r.alias],m=0;m<p.length;++m){0;var f={path:p[m],children:r.children};e(n,t,o,f,a,d.path||"/")}c&&(o[c]||(o[c]=d))}(r,a,s,e)}));for(var i=0,c=r.length;i<c;i++)"*"===r[i]&&(r.push(r.splice(i,1)[0]),c--,i--);return{pathList:r,pathMap:a,nameMap:s}}function Ea(e,n){return oa(e,[],n)}function Ia(e,n){var t=Ca(e),o=t.pathList,r=t.pathMap,a=t.nameMap;function s(e,t,s){var i=wa(e,t,!1,n),l=i.name;if(l){var u=a[l];if(!u)return c(null,i);var d=u.regex.keys.filter((function(e){return!e.optional})).map((function(e){return e.name}));if("object"!=typeof i.params&&(i.params={}),t&&"object"==typeof t.params)for(var p in t.params)!(p in i.params)&&d.indexOf(p)>-1&&(i.params[p]=t.params[p]);return i.path=_a(u.path,i.params),c(u,i,s)}if(i.path){i.params={};for(var m=0;m<o.length;m++){var f=o[m],h=r[f];if(Ta(h.regex,i.path,i.params))return c(h,i,s)}}return c(null,i)}function i(e,t){var o=e.redirect,r="function"==typeof o?o(qr(e,t,null,n)):o;if("string"==typeof r&&(r={path:r}),!r||"object"!=typeof r)return c(null,t);var i=r,l=i.name,u=i.path,d=t.query,p=t.hash,m=t.params;if(d=i.hasOwnProperty("query")?i.query:d,p=i.hasOwnProperty("hash")?i.hash:p,m=i.hasOwnProperty("params")?i.params:m,l){a[l];return s({_normalized:!0,name:l,query:d,hash:p,params:m},void 0,t)}if(u){var f=function(e,n){return ea(e,n.parent?n.parent.path:"/",!0)}(u,e);return s({_normalized:!0,path:_a(f,m),query:d,hash:p},void 0,t)}return c(null,t)}function c(e,t,o){return e&&e.redirect?i(e,o||t):e&&e.matchAs?function(e,n,t){var o=s({_normalized:!0,path:_a(t,n.params)});if(o){var r=o.matched,a=r[r.length-1];return n.params=o.params,c(a,n)}return c(null,n)}(0,t,e.matchAs):qr(e,t,o,n)}return{match:s,addRoutes:function(e){Ca(e,o,r,a)}}}function Ta(e,n,t){var o=n.match(e);if(!o)return!1;if(!t)return!0;for(var r=1,a=o.length;r<a;++r){var s=e.keys[r-1];s&&(t[s.name||"pathMatch"]="string"==typeof o[r]?Rr(o[r]):o[r])}return!0}var Ua=Sa&&window.performance&&window.performance.now?window.performance:Date;function za(){return Ua.now().toFixed(3)}var Oa=za();function Aa(){return Oa}function Da(e){return Oa=e}var Ma=Object.create(null);function ja(){"scrollRestoration"in window.history&&(window.history.scrollRestoration="manual");var e=window.location.protocol+"//"+window.location.host,n=window.location.href.replace(e,""),t=Dr({},window.history.state);return t.key=Aa(),window.history.replaceState(t,"",n),window.addEventListener("popstate",Ra),function(){window.removeEventListener("popstate",Ra)}}function La(e,n,t,o){if(e.app){var r=e.options.scrollBehavior;r&&e.app.$nextTick((function(){var a=function(){var e=Aa();if(e)return Ma[e]}(),s=r.call(e,n,t,o?a:null);s&&("function"==typeof s.then?s.then((function(e){qa(e,a)})).catch((function(e){0})):qa(s,a))}))}}function Ba(){var e=Aa();e&&(Ma[e]={x:window.pageXOffset,y:window.pageYOffset})}function Ra(e){Ba(),e.state&&e.state.key&&Da(e.state.key)}function Na(e){return Ga(e.x)||Ga(e.y)}function Fa(e){return{x:Ga(e.x)?e.x:window.pageXOffset,y:Ga(e.y)?e.y:window.pageYOffset}}function Ga(e){return"number"==typeof e}var $a=/^#\d/;function qa(e,n){var t,o="object"==typeof e;if(o&&"string"==typeof e.selector){var r=$a.test(e.selector)?document.getElementById(e.selector.slice(1)):document.querySelector(e.selector);if(r){var a=e.offset&&"object"==typeof e.offset?e.offset:{};n=function(e,n){var t=document.documentElement.getBoundingClientRect(),o=e.getBoundingClientRect();return{x:o.left-t.left-n.x,y:o.top-t.top-n.y}}(r,a={x:Ga((t=a).x)?t.x:0,y:Ga(t.y)?t.y:0})}else Na(e)&&(n=Fa(e))}else o&&Na(e)&&(n=Fa(e));n&&("scrollBehavior"in document.documentElement.style?window.scrollTo({left:n.x,top:n.y,behavior:e.behavior}):window.scrollTo(n.x,n.y))}var Va,Wa=Sa&&((-1===(Va=window.navigator.userAgent).indexOf("Android 2.")&&-1===Va.indexOf("Android 4.0")||-1===Va.indexOf("Mobile Safari")||-1!==Va.indexOf("Chrome")||-1!==Va.indexOf("Windows Phone"))&&window.history&&"function"==typeof window.history.pushState);function Ha(e,n){Ba();var t=window.history;try{if(n){var o=Dr({},t.state);o.key=Aa(),t.replaceState(o,"",e)}else t.pushState({key:Da(za())},"",e)}catch(t){window.location[n?"replace":"assign"](e)}}function Ka(e){Ha(e,!0)}function Ja(e,n,t){var o=function(r){r>=e.length?t():e[r]?n(e[r],(function(){o(r+1)})):o(r+1)};o(0)}var Ya={redirected:2,aborted:4,cancelled:8,duplicated:16};function Xa(e,n){return Za(e,n,Ya.redirected,'Redirected when going from "'+e.fullPath+'" to "'+function(e){if("string"==typeof e)return e;if("path"in e)return e.path;var n={};return es.forEach((function(t){t in e&&(n[t]=e[t])})),JSON.stringify(n,null,2)}(n)+'" via a navigation guard.')}function Qa(e,n){return Za(e,n,Ya.cancelled,'Navigation cancelled from "'+e.fullPath+'" to "'+n.fullPath+'" with a new navigation.')}function Za(e,n,t,o){var r=new Error(o);return r._isRouter=!0,r.from=e,r.to=n,r.type=t,r}var es=["params","query","hash"];function ns(e){return Object.prototype.toString.call(e).indexOf("Error")>-1}function ts(e,n){return ns(e)&&e._isRouter&&(null==n||e.type===n)}function os(e){return function(n,t,o){var r=!1,a=0,s=null;rs(e,(function(e,n,t,i){if("function"==typeof e&&void 0===e.cid){r=!0,a++;var c,l=is((function(n){var r;((r=n).__esModule||ss&&"Module"===r[Symbol.toStringTag])&&(n=n.default),e.resolved="function"==typeof n?n:ya.extend(n),t.components[i]=n,--a<=0&&o()})),u=is((function(e){var n="Failed to resolve async component "+i+": "+e;s||(s=ns(e)?e:new Error(n),o(s))}));try{c=e(l,u)}catch(e){u(e)}if(c)if("function"==typeof c.then)c.then(l,u);else{var d=c.component;d&&"function"==typeof d.then&&d.then(l,u)}}})),r||o()}}function rs(e,n){return as(e.map((function(e){return Object.keys(e.components).map((function(t){return n(e.components[t],e.instances[t],e,t)}))})))}function as(e){return Array.prototype.concat.apply([],e)}var ss="function"==typeof Symbol&&"symbol"==typeof Symbol.toStringTag;function is(e){var n=!1;return function(){for(var t=[],o=arguments.length;o--;)t[o]=arguments[o];if(!n)return n=!0,e.apply(this,t)}}var cs=function(e,n){this.router=e,this.base=function(e){if(!e)if(Sa){var n=document.querySelector("base");e=(e=n&&n.getAttribute("href")||"/").replace(/^https?:\/\/[^\/]+/,"")}else e="/";"/"!==e.charAt(0)&&(e="/"+e);return e.replace(/\/$/,"")}(n),this.current=Wr,this.pending=null,this.ready=!1,this.readyCbs=[],this.readyErrorCbs=[],this.errorCbs=[],this.listeners=[]};function ls(e,n,t,o){var r=rs(e,(function(e,o,r,a){var s=function(e,n){"function"!=typeof e&&(e=ya.extend(e));return e.options[n]}(e,n);if(s)return Array.isArray(s)?s.map((function(e){return t(e,o,r,a)})):t(s,o,r,a)}));return as(o?r.reverse():r)}function us(e,n){if(n)return function(){return e.apply(n,arguments)}}cs.prototype.listen=function(e){this.cb=e},cs.prototype.onReady=function(e,n){this.ready?e():(this.readyCbs.push(e),n&&this.readyErrorCbs.push(n))},cs.prototype.onError=function(e){this.errorCbs.push(e)},cs.prototype.transitionTo=function(e,n,t){var o,r=this;try{o=this.router.match(e,this.current)}catch(e){throw this.errorCbs.forEach((function(n){n(e)})),e}var a=this.current;this.confirmTransition(o,(function(){r.updateRoute(o),n&&n(o),r.ensureURL(),r.router.afterHooks.forEach((function(e){e&&e(o,a)})),r.ready||(r.ready=!0,r.readyCbs.forEach((function(e){e(o)})))}),(function(e){t&&t(e),e&&!r.ready&&(ts(e,Ya.redirected)&&a===Wr||(r.ready=!0,r.readyErrorCbs.forEach((function(n){n(e)}))))}))},cs.prototype.confirmTransition=function(e,n,t){var o=this,r=this.current;this.pending=e;var a,s,i=function(e){!ts(e)&&ns(e)&&(o.errorCbs.length?o.errorCbs.forEach((function(n){n(e)})):console.error(e)),t&&t(e)},c=e.matched.length-1,l=r.matched.length-1;if(Jr(e,r)&&c===l&&e.matched[c]===r.matched[l])return this.ensureURL(),i(((s=Za(a=r,e,Ya.duplicated,'Avoided redundant navigation to current location: "'+a.fullPath+'".')).name="NavigationDuplicated",s));var u=function(e,n){var t,o=Math.max(e.length,n.length);for(t=0;t<o&&e[t]===n[t];t++);return{updated:n.slice(0,t),activated:n.slice(t),deactivated:e.slice(t)}}(this.current.matched,e.matched),d=u.updated,p=u.deactivated,m=u.activated,f=[].concat(function(e){return ls(e,"beforeRouteLeave",us,!0)}(p),this.router.beforeHooks,function(e){return ls(e,"beforeRouteUpdate",us)}(d),m.map((function(e){return e.beforeEnter})),os(m)),h=function(n,t){if(o.pending!==e)return i(Qa(r,e));try{n(e,r,(function(n){!1===n?(o.ensureURL(!0),i(function(e,n){return Za(e,n,Ya.aborted,'Navigation aborted from "'+e.fullPath+'" to "'+n.fullPath+'" via a navigation guard.')}(r,e))):ns(n)?(o.ensureURL(!0),i(n)):"string"==typeof n||"object"==typeof n&&("string"==typeof n.path||"string"==typeof n.name)?(i(Xa(r,e)),"object"==typeof n&&n.replace?o.replace(n):o.push(n)):t(n)}))}catch(e){i(e)}};Ja(f,h,(function(){Ja(function(e){return ls(e,"beforeRouteEnter",(function(e,n,t,o){return function(e,n,t){return function(o,r,a){return e(o,r,(function(e){"function"==typeof e&&(n.enteredCbs[t]||(n.enteredCbs[t]=[]),n.enteredCbs[t].push(e)),a(e)}))}}(e,t,o)}))}(m).concat(o.router.resolveHooks),h,(function(){if(o.pending!==e)return i(Qa(r,e));o.pending=null,n(e),o.router.app&&o.router.app.$nextTick((function(){Xr(e)}))}))}))},cs.prototype.updateRoute=function(e){this.current=e,this.cb&&this.cb(e)},cs.prototype.setupListeners=function(){},cs.prototype.teardown=function(){this.listeners.forEach((function(e){e()})),this.listeners=[],this.current=Wr,this.pending=null};var ds=function(e){function n(n,t){e.call(this,n,t),this._startLocation=ps(this.base)}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router,t=n.options.scrollBehavior,o=Wa&&t;o&&this.listeners.push(ja());var r=function(){var t=e.current,r=ps(e.base);e.current===Wr&&r===e._startLocation||e.transitionTo(r,(function(e){o&&La(n,e,t,!0)}))};window.addEventListener("popstate",r),this.listeners.push((function(){window.removeEventListener("popstate",r)}))}},n.prototype.go=function(e){window.history.go(e)},n.prototype.push=function(e,n,t){var o=this,r=this.current;this.transitionTo(e,(function(e){Ha(na(o.base+e.fullPath)),La(o.router,e,r,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var o=this,r=this.current;this.transitionTo(e,(function(e){Ka(na(o.base+e.fullPath)),La(o.router,e,r,!1),n&&n(e)}),t)},n.prototype.ensureURL=function(e){if(ps(this.base)!==this.current.fullPath){var n=na(this.base+this.current.fullPath);e?Ha(n):Ka(n)}},n.prototype.getCurrentLocation=function(){return ps(this.base)},n}(cs);function ps(e){var n=window.location.pathname;return e&&0===n.toLowerCase().indexOf(e.toLowerCase())&&(n=n.slice(e.length)),(n||"/")+window.location.search+window.location.hash}var ms=function(e){function n(n,t,o){e.call(this,n,t),o&&function(e){var n=ps(e);if(!/^\/#/.test(n))return window.location.replace(na(e+"/#"+n)),!0}(this.base)||fs()}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.setupListeners=function(){var e=this;if(!(this.listeners.length>0)){var n=this.router.options.scrollBehavior,t=Wa&&n;t&&this.listeners.push(ja());var o=function(){var n=e.current;fs()&&e.transitionTo(hs(),(function(o){t&&La(e.router,o,n,!0),Wa||bs(o.fullPath)}))},r=Wa?"popstate":"hashchange";window.addEventListener(r,o),this.listeners.push((function(){window.removeEventListener(r,o)}))}},n.prototype.push=function(e,n,t){var o=this,r=this.current;this.transitionTo(e,(function(e){vs(e.fullPath),La(o.router,e,r,!1),n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var o=this,r=this.current;this.transitionTo(e,(function(e){bs(e.fullPath),La(o.router,e,r,!1),n&&n(e)}),t)},n.prototype.go=function(e){window.history.go(e)},n.prototype.ensureURL=function(e){var n=this.current.fullPath;hs()!==n&&(e?vs(n):bs(n))},n.prototype.getCurrentLocation=function(){return hs()},n}(cs);function fs(){var e=hs();return"/"===e.charAt(0)||(bs("/"+e),!1)}function hs(){var e=window.location.href,n=e.indexOf("#");return n<0?"":e=e.slice(n+1)}function gs(e){var n=window.location.href,t=n.indexOf("#");return(t>=0?n.slice(0,t):n)+"#"+e}function vs(e){Wa?Ha(gs(e)):window.location.hash=e}function bs(e){Wa?Ka(gs(e)):window.location.replace(gs(e))}var _s=function(e){function n(n,t){e.call(this,n,t),this.stack=[],this.index=-1}return e&&(n.__proto__=e),n.prototype=Object.create(e&&e.prototype),n.prototype.constructor=n,n.prototype.push=function(e,n,t){var o=this;this.transitionTo(e,(function(e){o.stack=o.stack.slice(0,o.index+1).concat(e),o.index++,n&&n(e)}),t)},n.prototype.replace=function(e,n,t){var o=this;this.transitionTo(e,(function(e){o.stack=o.stack.slice(0,o.index).concat(e),n&&n(e)}),t)},n.prototype.go=function(e){var n=this,t=this.index+e;if(!(t<0||t>=this.stack.length)){var o=this.stack[t];this.confirmTransition(o,(function(){var e=n.current;n.index=t,n.updateRoute(o),n.router.afterHooks.forEach((function(n){n&&n(o,e)}))}),(function(e){ts(e,Ya.duplicated)&&(n.index=t)}))}},n.prototype.getCurrentLocation=function(){var e=this.stack[this.stack.length-1];return e?e.fullPath:"/"},n.prototype.ensureURL=function(){},n}(cs),ws=function(e){void 0===e&&(e={}),this.app=null,this.apps=[],this.options=e,this.beforeHooks=[],this.resolveHooks=[],this.afterHooks=[],this.matcher=Ia(e.routes||[],this);var n=e.mode||"hash";switch(this.fallback="history"===n&&!Wa&&!1!==e.fallback,this.fallback&&(n="hash"),Sa||(n="abstract"),this.mode=n,n){case"history":this.history=new ds(this,e.base);break;case"hash":this.history=new ms(this,e.base,this.fallback);break;case"abstract":this.history=new _s(this,e.base);break;default:0}},ys={currentRoute:{configurable:!0}};function ks(e,n){return e.push(n),function(){var t=e.indexOf(n);t>-1&&e.splice(t,1)}}ws.prototype.match=function(e,n,t){return this.matcher.match(e,n,t)},ys.currentRoute.get=function(){return this.history&&this.history.current},ws.prototype.init=function(e){var n=this;if(this.apps.push(e),e.$once("hook:destroyed",(function(){var t=n.apps.indexOf(e);t>-1&&n.apps.splice(t,1),n.app===e&&(n.app=n.apps[0]||null),n.app||n.history.teardown()})),!this.app){this.app=e;var t=this.history;if(t instanceof ds||t instanceof ms){var o=function(e){t.setupListeners(),function(e){var o=t.current,r=n.options.scrollBehavior;Wa&&r&&"fullPath"in e&&La(n,e,o,!1)}(e)};t.transitionTo(t.getCurrentLocation(),o,o)}t.listen((function(e){n.apps.forEach((function(n){n._route=e}))}))}},ws.prototype.beforeEach=function(e){return ks(this.beforeHooks,e)},ws.prototype.beforeResolve=function(e){return ks(this.resolveHooks,e)},ws.prototype.afterEach=function(e){return ks(this.afterHooks,e)},ws.prototype.onReady=function(e,n){this.history.onReady(e,n)},ws.prototype.onError=function(e){this.history.onError(e)},ws.prototype.push=function(e,n,t){var o=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){o.history.push(e,n,t)}));this.history.push(e,n,t)},ws.prototype.replace=function(e,n,t){var o=this;if(!n&&!t&&"undefined"!=typeof Promise)return new Promise((function(n,t){o.history.replace(e,n,t)}));this.history.replace(e,n,t)},ws.prototype.go=function(e){this.history.go(e)},ws.prototype.back=function(){this.go(-1)},ws.prototype.forward=function(){this.go(1)},ws.prototype.getMatchedComponents=function(e){var n=e?e.matched?e:this.resolve(e).route:this.currentRoute;return n?[].concat.apply([],n.matched.map((function(e){return Object.keys(e.components).map((function(n){return e.components[n]}))}))):[]},ws.prototype.resolve=function(e,n,t){var o=wa(e,n=n||this.history.current,t,this),r=this.match(o,n),a=r.redirectedFrom||r.fullPath;return{location:o,route:r,href:function(e,n,t){var o="hash"===t?"#"+n:n;return e?na(e+"/"+o):o}(this.history.base,a,this.mode),normalizedTo:o,resolved:r}},ws.prototype.addRoutes=function(e){this.matcher.addRoutes(e),this.history.current!==Wr&&this.history.transitionTo(this.history.getCurrentLocation())},Object.defineProperties(ws.prototype,ys),ws.install=function e(n){if(!e.installed||ya!==n){e.installed=!0,ya=n;var t=function(e){return void 0!==e},o=function(e,n){var o=e.$options._parentVnode;t(o)&&t(o=o.data)&&t(o=o.registerRouteInstance)&&o(e,n)};n.mixin({beforeCreate:function(){t(this.$options.router)?(this._routerRoot=this,this._router=this.$options.router,this._router.init(this),n.util.defineReactive(this,"_route",this._router.history.current)):this._routerRoot=this.$parent&&this.$parent._routerRoot||this,o(this,this)},destroyed:function(){o(this)}}),Object.defineProperty(n.prototype,"$router",{get:function(){return this._routerRoot._router}}),Object.defineProperty(n.prototype,"$route",{get:function(){return this._routerRoot._route}}),n.component("RouterView",Qr),n.component("RouterLink",xa);var r=n.config.optionMergeStrategies;r.beforeRouteEnter=r.beforeRouteLeave=r.beforeRouteUpdate=r.created}},ws.version="3.4.9",ws.isNavigationFailure=ts,ws.NavigationFailureType=Ya,Sa&&window.Vue&&window.Vue.use(ws);var xs=ws;t(50),t(209),t(211),t(143),t(144),t(73),t(213),t(51);function Ps(e){e.locales&&Object.keys(e.locales).forEach((function(n){e.locales[n].path=n})),Object.freeze(e)}t(190),t(147),t(30),t(192),t(72),t(42),t(62),t(89);function Ss(e){return(Ss="function"==typeof Symbol&&"symbol"==typeof Symbol.iterator?function(e){return typeof e}:function(e){return e&&"function"==typeof Symbol&&e.constructor===Symbol&&e!==Symbol.prototype?"symbol":typeof e})(e)}var Cs=t(44),Es={NotFound:function(){return t.e(11).then(t.bind(null,389))},Layout:function(){return Promise.all([t.e(0),t.e(2)]).then(t.bind(null,388))}},Is={"v-f2ef6f48":function(){return t.e(12).then(t.bind(null,393))},"v-5dc9dbff":function(){return t.e(13).then(t.bind(null,394))},"v-b3507e40":function(){return t.e(6).then(t.bind(null,395))},"v-73585e3c":function(){return t.e(14).then(t.bind(null,396))},"v-77afd55f":function(){return t.e(15).then(t.bind(null,397))},"v-147b6d82":function(){return t.e(7).then(t.bind(null,398))},"v-f8f918a8":function(){return t.e(16).then(t.bind(null,399))},"v-7c409ac2":function(){return t.e(17).then(t.bind(null,400))},"v-84157232":function(){return t.e(18).then(t.bind(null,401))},"v-7f2f1742":function(){return t.e(19).then(t.bind(null,402))},"v-113a25c1":function(){return t.e(20).then(t.bind(null,403))},"v-3ece3291":function(){return t.e(21).then(t.bind(null,404))},"v-ef0bd702":function(){return t.e(22).then(t.bind(null,405))},"v-92b0af42":function(){return t.e(23).then(t.bind(null,406))},"v-b7b7c982":function(){return t.e(24).then(t.bind(null,407))},"v-6971be1f":function(){return t.e(25).then(t.bind(null,408))},"v-5f486ce8":function(){return t.e(26).then(t.bind(null,409))},"v-3266a61f":function(){return t.e(27).then(t.bind(null,410))},"v-6cf8170c":function(){return t.e(8).then(t.bind(null,411))},"v-87056da8":function(){return t.e(28).then(t.bind(null,412))},"v-e7947532":function(){return t.e(29).then(t.bind(null,413))},"v-0d494d6c":function(){return t.e(30).then(t.bind(null,414))},"v-20079582":function(){return t.e(31).then(t.bind(null,415))},"v-4fa6a05f":function(){return t.e(9).then(t.bind(null,416))},"v-f05138fa":function(){return t.e(32).then(t.bind(null,417))},"v-d0b99468":function(){return t.e(33).then(t.bind(null,418))},"v-7bbae69f":function(){return t.e(34).then(t.bind(null,419))},"v-20f290cb":function(){return t.e(35).then(t.bind(null,420))},"v-a98bfbc2":function(){return t.e(36).then(t.bind(null,421))},"v-104d39de":function(){return t.e(37).then(t.bind(null,422))},"v-39b38d02":function(){return t.e(38).then(t.bind(null,423))},"v-6f0a5512":function(){return t.e(39).then(t.bind(null,424))},"v-ef86c67a":function(){return t.e(40).then(t.bind(null,425))},"v-2e3168b2":function(){return t.e(41).then(t.bind(null,426))},"v-4578197f":function(){return t.e(42).then(t.bind(null,427))},"v-08acd132":function(){return t.e(43).then(t.bind(null,428))},"v-dd9c338e":function(){return t.e(44).then(t.bind(null,429))},"v-e5c0bb42":function(){return t.e(45).then(t.bind(null,430))},"v-592d247f":function(){return t.e(46).then(t.bind(null,431))},"v-12b90e11":function(){return t.e(47).then(t.bind(null,432))},"v-355617e7":function(){return t.e(48).then(t.bind(null,433))},"v-75a050c2":function(){return t.e(49).then(t.bind(null,434))},"v-5a820f7d":function(){return t.e(50).then(t.bind(null,435))}};function Ts(e){var n=Object.create(null);return function(t){return n[t]||(n[t]=e(t))}}var Us=/-(\w)/g,zs=Ts((function(e){return e.replace(Us,(function(e,n){return n?n.toUpperCase():""}))})),Os=/\B([A-Z])/g,As=Ts((function(e){return e.replace(Os,"-$1").toLowerCase()})),Ds=Ts((function(e){return e.charAt(0).toUpperCase()+e.slice(1)}));function Ms(e,n){if(n)return e(n)?e(n):n.includes("-")?e(Ds(zs(n))):e(Ds(n))||e(As(n))}var js=Object.assign({},Es,Is),Ls=function(e){return js[e]},Bs=function(e){return Is[e]},Rs=function(e){return Es[e]},Ns=function(e){return Ar.component(e)};function Fs(e){return Ms(Bs,e)}function Gs(e){return Ms(Rs,e)}function $s(e){return Ms(Ls,e)}function qs(e){return Ms(Ns,e)}function Vs(){for(var e=arguments.length,n=new Array(e),t=0;t<e;t++)n[t]=arguments[t];return Promise.all(n.filter((function(e){return e})).map(function(){var e=Object(o.a)(regeneratorRuntime.mark((function e(n){var t;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:if(qs(n)||!$s(n)){e.next=5;break}return e.next=3,$s(n)();case 3:t=e.sent,Ar.component(n,t.default);case 5:case"end":return e.stop()}}),e)})));return function(n){return e.apply(this,arguments)}}()))}function Ws(e,n){"undefined"!=typeof window&&window.__VUEPRESS__&&(window.__VUEPRESS__[e]=n)}t(108),t(185);var Hs=t(102),Ks=t(177),Js=t.n(Ks),Ys={created:function(){if(this.siteMeta=this.$site.headTags.filter((function(e){return"meta"===Object(Hs.a)(e,1)[0]})).map((function(e){var n=Object(Hs.a)(e,2);n[0];return n[1]})),this.$ssrContext){var e=this.getMergedMetaTags();this.$ssrContext.title=this.$title,this.$ssrContext.lang=this.$lang,this.$ssrContext.pageMeta=(n=e)?n.map((function(e){var n="<meta";return Object.keys(e).forEach((function(t){n+=" ".concat(t,'="').concat(e[t],'"')})),n+">"})).join("\n    "):"",this.$ssrContext.canonicalLink=Qs(this.$canonicalUrl)}var n},mounted:function(){this.currentMetaTags=Object(Cs.a)(document.querySelectorAll("meta")),this.updateMeta(),this.updateCanonicalLink()},methods:{updateMeta:function(){document.title=this.$title,document.documentElement.lang=this.$lang;var e=this.getMergedMetaTags();this.currentMetaTags=Zs(e,this.currentMetaTags)},getMergedMetaTags:function(){var e=this.$page.frontmatter.meta||[];return Js()([{name:"description",content:this.$description}],e,this.siteMeta,ei)},updateCanonicalLink:function(){Xs(),this.$canonicalUrl&&document.head.insertAdjacentHTML("beforeend",Qs(this.$canonicalUrl))}},watch:{$page:function(){this.updateMeta(),this.updateCanonicalLink()}},beforeDestroy:function(){Zs(null,this.currentMetaTags),Xs()}};function Xs(){var e=document.querySelector("link[rel='canonical']");e&&e.remove()}function Qs(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"";return e?'<link href="'.concat(e,'" rel="canonical" />'):""}function Zs(e,n){if(n&&Object(Cs.a)(n).filter((function(e){return e.parentNode===document.head})).forEach((function(e){return document.head.removeChild(e)})),e)return e.map((function(e){var n=document.createElement("meta");return Object.keys(e).forEach((function(t){n.setAttribute(t,e[t])})),document.head.appendChild(n),n}))}function ei(e){for(var n=0,t=["name","property","itemprop"];n<t.length;n++){var o=t[n];if(e.hasOwnProperty(o))return e[o]+o}return JSON.stringify(e)}t(178);var ni=t(69),ti=t.n(ni),oi={mounted:function(){window.addEventListener("scroll",this.onScroll)},methods:{onScroll:ti()((function(){this.setActiveHash()}),300),setActiveHash:function(){for(var e=this,n=[].slice.call(document.querySelectorAll(".sidebar-link")),t=[].slice.call(document.querySelectorAll(".header-anchor")).filter((function(e){return n.some((function(n){return n.hash===e.hash}))})),o=Math.max(window.pageYOffset,document.documentElement.scrollTop,document.body.scrollTop),r=Math.max(document.documentElement.scrollHeight,document.body.scrollHeight),a=window.innerHeight+o,s=0;s<t.length;s++){var i=t[s],c=t[s+1],l=0===s&&0===o||o>=i.parentElement.offsetTop+10&&(!c||o<c.parentElement.offsetTop-10),u=decodeURIComponent(this.$route.hash);if(l&&u!==decodeURIComponent(i.hash)){var d=i;if(a===r)for(var p=s+1;p<t.length;p++)if(u===decodeURIComponent(t[p].hash))return;return this.$vuepress.$set("disableScrollBehavior",!0),void this.$router.replace(decodeURIComponent(d.hash),(function(){e.$nextTick((function(){e.$vuepress.$set("disableScrollBehavior",!1)}))}))}}}},beforeDestroy:function(){window.removeEventListener("scroll",this.onScroll)}},ri=(t(90),t(70)),ai=t.n(ri),si={mounted:function(){var e=this;ai.a.configure({showSpinner:!1}),this.$router.beforeEach((function(e,n,t){e.path===n.path||Ar.component(e.name)||ai.a.start(),t()})),this.$router.afterEach((function(){ai.a.done(),e.isSidebarOpen=!1}))}},ii=(t(100),t(311),Object.assign||function(e){for(var n=1;n<arguments.length;n++){var t=arguments[n];for(var o in t)Object.prototype.hasOwnProperty.call(t,o)&&(e[o]=t[o])}return e}),ci=function(e){return"IMG"===e.tagName},li=function(e){return e&&1===e.nodeType},ui=function(e){return".svg"===(e.currentSrc||e.src).substr(-4).toLowerCase()},di=function(e){try{return Array.isArray(e)?e.filter(ci):function(e){return NodeList.prototype.isPrototypeOf(e)}(e)?[].slice.call(e).filter(ci):li(e)?[e].filter(ci):"string"==typeof e?[].slice.call(document.querySelectorAll(e)).filter(ci):[]}catch(e){throw new TypeError("The provided selector is invalid.\nExpects a CSS selector, a Node element, a NodeList or an array.\nSee: https://github.com/francoischalifour/medium-zoom")}},pi=function(e){var n=document.createElement("div");return n.classList.add("medium-zoom-overlay"),n.style.background=e,n},mi=function(e){var n=e.getBoundingClientRect(),t=n.top,o=n.left,r=n.width,a=n.height,s=e.cloneNode(),i=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,c=window.pageXOffset||document.documentElement.scrollLeft||document.body.scrollLeft||0;return s.removeAttribute("id"),s.style.position="absolute",s.style.top=t+i+"px",s.style.left=o+c+"px",s.style.width=r+"px",s.style.height=a+"px",s.style.transform="",s},fi=function(e,n){var t=ii({bubbles:!1,cancelable:!1,detail:void 0},n);if("function"==typeof window.CustomEvent)return new CustomEvent(e,t);var o=document.createEvent("CustomEvent");return o.initCustomEvent(e,t.bubbles,t.cancelable,t.detail),o};!function(e,n){void 0===n&&(n={});var t=n.insertAt;if(e&&"undefined"!=typeof document){var o=document.head||document.getElementsByTagName("head")[0],r=document.createElement("style");r.type="text/css","top"===t&&o.firstChild?o.insertBefore(r,o.firstChild):o.appendChild(r),r.styleSheet?r.styleSheet.cssText=e:r.appendChild(document.createTextNode(e))}}(".medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--opened .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s cubic-bezier(.2,0,.2,1)!important}.medium-zoom-image--hidden{visibility:hidden}.medium-zoom-image--opened{position:relative;cursor:pointer;cursor:zoom-out;will-change:transform}");var hi=function e(n){var t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},o=window.Promise||function(e){function n(){}e(n,n)},r=function(e){var n=e.target;n!==S?-1!==_.indexOf(n)&&h({target:n}):f()},a=function(){if(!y&&P.original){var e=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0;Math.abs(k-e)>x.scrollOffset&&setTimeout(f,150)}},s=function(e){var n=e.key||e.keyCode;"Escape"!==n&&"Esc"!==n&&27!==n||f()},i=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},n=e;if(e.background&&(S.style.background=e.background),e.container&&e.container instanceof Object&&(n.container=ii({},x.container,e.container)),e.template){var t=li(e.template)?e.template:document.querySelector(e.template);n.template=t}return x=ii({},x,n),_.forEach((function(e){e.dispatchEvent(fi("medium-zoom:update",{detail:{zoom:C}}))})),C},c=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};return e(ii({},x,n))},l=function(){for(var e=arguments.length,n=Array(e),t=0;t<e;t++)n[t]=arguments[t];var o=n.reduce((function(e,n){return[].concat(e,di(n))}),[]);return o.filter((function(e){return-1===_.indexOf(e)})).forEach((function(e){_.push(e),e.classList.add("medium-zoom-image")})),w.forEach((function(e){var n=e.type,t=e.listener,r=e.options;o.forEach((function(e){e.addEventListener(n,t,r)}))})),C},u=function(){for(var e=arguments.length,n=Array(e),t=0;t<e;t++)n[t]=arguments[t];P.zoomed&&f();var o=n.length>0?n.reduce((function(e,n){return[].concat(e,di(n))}),[]):_;return o.forEach((function(e){e.classList.remove("medium-zoom-image"),e.dispatchEvent(fi("medium-zoom:detach",{detail:{zoom:C}}))})),_=_.filter((function(e){return-1===o.indexOf(e)})),C},d=function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return _.forEach((function(o){o.addEventListener("medium-zoom:"+e,n,t)})),w.push({type:"medium-zoom:"+e,listener:n,options:t}),C},p=function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};return _.forEach((function(o){o.removeEventListener("medium-zoom:"+e,n,t)})),w=w.filter((function(t){return!(t.type==="medium-zoom:"+e&&t.listener.toString()===n.toString())})),C},m=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},n=e.target,t=function(){var e={width:document.documentElement.clientWidth,height:document.documentElement.clientHeight,left:0,top:0,right:0,bottom:0},n=void 0,t=void 0;if(x.container)if(x.container instanceof Object)n=(e=ii({},e,x.container)).width-e.left-e.right-2*x.margin,t=e.height-e.top-e.bottom-2*x.margin;else{var o=(li(x.container)?x.container:document.querySelector(x.container)).getBoundingClientRect(),r=o.width,a=o.height,s=o.left,i=o.top;e=ii({},e,{width:r,height:a,left:s,top:i})}n=n||e.width-2*x.margin,t=t||e.height-2*x.margin;var c=P.zoomedHd||P.original,l=ui(c)?n:c.naturalWidth||n,u=ui(c)?t:c.naturalHeight||t,d=c.getBoundingClientRect(),p=d.top,m=d.left,f=d.width,h=d.height,g=Math.min(l,n)/f,v=Math.min(u,t)/h,b=Math.min(g,v),_="scale("+b+") translate3d("+((n-f)/2-m+x.margin+e.left)/b+"px, "+((t-h)/2-p+x.margin+e.top)/b+"px, 0)";P.zoomed.style.transform=_,P.zoomedHd&&(P.zoomedHd.style.transform=_)};return new o((function(e){if(n&&-1===_.indexOf(n))e(C);else{if(P.zoomed)e(C);else{if(n)P.original=n;else{if(!(_.length>0))return void e(C);var o=_;P.original=o[0]}if(P.original.dispatchEvent(fi("medium-zoom:open",{detail:{zoom:C}})),k=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,y=!0,P.zoomed=mi(P.original),document.body.appendChild(S),x.template){var r=li(x.template)?x.template:document.querySelector(x.template);P.template=document.createElement("div"),P.template.appendChild(r.content.cloneNode(!0)),document.body.appendChild(P.template)}if(document.body.appendChild(P.zoomed),window.requestAnimationFrame((function(){document.body.classList.add("medium-zoom--opened")})),P.original.classList.add("medium-zoom-image--hidden"),P.zoomed.classList.add("medium-zoom-image--opened"),P.zoomed.addEventListener("click",f),P.zoomed.addEventListener("transitionend",(function n(){y=!1,P.zoomed.removeEventListener("transitionend",n),P.original.dispatchEvent(fi("medium-zoom:opened",{detail:{zoom:C}})),e(C)})),P.original.getAttribute("data-zoom-src")){P.zoomedHd=P.zoomed.cloneNode(),P.zoomedHd.removeAttribute("srcset"),P.zoomedHd.removeAttribute("sizes"),P.zoomedHd.src=P.zoomed.getAttribute("data-zoom-src"),P.zoomedHd.onerror=function(){clearInterval(a),console.warn("Unable to reach the zoom image target "+P.zoomedHd.src),P.zoomedHd=null,t()};var a=setInterval((function(){P.zoomedHd.complete&&(clearInterval(a),P.zoomedHd.classList.add("medium-zoom-image--opened"),P.zoomedHd.addEventListener("click",f),document.body.appendChild(P.zoomedHd),t())}),10)}else if(P.original.hasAttribute("srcset")){P.zoomedHd=P.zoomed.cloneNode(),P.zoomedHd.removeAttribute("sizes"),P.zoomedHd.removeAttribute("loading");var s=P.zoomedHd.addEventListener("load",(function(){P.zoomedHd.removeEventListener("load",s),P.zoomedHd.classList.add("medium-zoom-image--opened"),P.zoomedHd.addEventListener("click",f),document.body.appendChild(P.zoomedHd),t()}))}else t()}}}))},f=function(){return new o((function(e){if(!y&&P.original){y=!0,document.body.classList.remove("medium-zoom--opened"),P.zoomed.style.transform="",P.zoomedHd&&(P.zoomedHd.style.transform=""),P.template&&(P.template.style.transition="opacity 150ms",P.template.style.opacity=0),P.original.dispatchEvent(fi("medium-zoom:close",{detail:{zoom:C}})),P.zoomed.addEventListener("transitionend",(function n(){P.original.classList.remove("medium-zoom-image--hidden"),document.body.removeChild(P.zoomed),P.zoomedHd&&document.body.removeChild(P.zoomedHd),document.body.removeChild(S),P.zoomed.classList.remove("medium-zoom-image--opened"),P.template&&document.body.removeChild(P.template),y=!1,P.zoomed.removeEventListener("transitionend",n),P.original.dispatchEvent(fi("medium-zoom:closed",{detail:{zoom:C}})),P.original=null,P.zoomed=null,P.zoomedHd=null,P.template=null,e(C)}))}else e(C)}))},h=function(){var e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},n=e.target;return P.original?f():m({target:n})},g=function(){return x},v=function(){return _},b=function(){return P.original},_=[],w=[],y=!1,k=0,x=t,P={original:null,zoomed:null,zoomedHd:null,template:null};"[object Object]"===Object.prototype.toString.call(n)?x=n:(n||"string"==typeof n)&&l(n),x=ii({margin:0,background:"#fff",scrollOffset:40,container:null,template:null},x);var S=pi(x.background);document.addEventListener("click",r),document.addEventListener("keyup",s),document.addEventListener("scroll",a),window.addEventListener("resize",f);var C={open:m,close:f,toggle:h,update:i,clone:c,attach:l,detach:u,on:d,off:p,getOptions:g,getImages:v,getZoomedImage:b};return C},gi={data:function(){return{zoom:null}},mounted:function(){this.updateZoom()},updated:function(){this.updateZoom()},methods:{updateZoom:function(){var e=this;setTimeout((function(){e.zoom&&e.zoom.detach(),e.zoom=hi(".theme-default-content :not(a) > img",{})}),1e3)}}},vi=t(120),bi=(t(195),{props:{parent:Object,code:String,options:{align:String,color:String,backgroundTransition:Boolean,backgroundColor:String,successText:String,staticIcon:Boolean}},data:function(){return{success:!1,originalBackground:null,originalTransition:null}},computed:{alignStyle:function(){var e={};return e[this.options.align]="7.5px",e},iconClass:function(){return this.options.staticIcon?"":"hover"}},mounted:function(){this.originalTransition=this.parent.style.transition,this.originalBackground=this.parent.style.background},beforeDestroy:function(){this.parent.style.transition=this.originalTransition,this.parent.style.background=this.originalBackground},methods:{hexToRgb:function(e){var n=/^#?([a-f\d]{2})([a-f\d]{2})([a-f\d]{2})$/i.exec(e);return n?{r:parseInt(n[1],16),g:parseInt(n[2],16),b:parseInt(n[3],16)}:null},copyToClipboard:function(e){var n=this;if(navigator.clipboard)navigator.clipboard.writeText(this.code).then((function(){n.setSuccessTransitions()}),(function(){}));else{var t=document.createElement("textarea");document.body.appendChild(t),t.value=this.code,t.select(),document.execCommand("Copy"),t.remove(),this.setSuccessTransitions()}},setSuccessTransitions:function(){var e=this;if(clearTimeout(this.successTimeout),this.options.backgroundTransition){this.parent.style.transition="background 350ms";var n=this.hexToRgb(this.options.backgroundColor);this.parent.style.background="rgba(".concat(n.r,", ").concat(n.g,", ").concat(n.b,", 0.1)")}this.success=!0,this.successTimeout=setTimeout((function(){e.options.backgroundTransition&&(e.parent.style.background=e.originalBackground,e.parent.style.transition=e.originalTransition),e.success=!1}),500)}}}),_i=(t(315),t(17)),wi=Object(_i.a)(bi,(function(){var e=this,n=e.$createElement,t=e._self._c||n;return t("div",{staticClass:"code-copy"},[t("svg",{class:e.iconClass,style:e.alignStyle,attrs:{xmlns:"http://www.w3.org/2000/svg",width:"24",height:"24",viewBox:"0 0 24 24"},on:{click:e.copyToClipboard}},[t("path",{attrs:{fill:"none",d:"M0 0h24v24H0z"}}),e._v(" "),t("path",{attrs:{fill:e.options.color,d:"M16 1H4c-1.1 0-2 .9-2 2v14h2V3h12V1zm-1 4l6 6v10c0 1.1-.9 2-2 2H7.99C6.89 23 6 22.1 6 21l.01-14c0-1.1.89-2 1.99-2h7zm-1 7h5.5L14 6.5V12z"}})]),e._v(" "),t("span",{class:e.success?"success":"",style:e.alignStyle},[e._v("\n        "+e._s(e.options.successText)+"\n    ")])])}),[],!1,null,"49140617",null).exports,yi=(t(316),[Ys,oi,si,gi,{updated:function(){this.update()},methods:{update:function(){setTimeout((function(){document.querySelectorAll('div[class*="language-"] pre').forEach((function(e){if(!e.classList.contains("code-copy-added")){var n=new(Ar.extend(wi));n.options=Object(vi.a)({},{align:"bottom",color:"#27b1ff",backgroundTransition:!0,backgroundColor:"#0075b8",successText:"Copied!",staticIcon:!1}),n.code=e.innerText,n.parent=e,n.$mount(),e.classList.add("code-copy-added"),e.appendChild(n.$el)}}))}),100)}}}]),ki={name:"GlobalLayout",computed:{layout:function(){var e=this.getLayout();return Ws("layout",e),Ar.component(e)}},methods:{getLayout:function(){if(this.$page.path){var e=this.$page.frontmatter.layout;return e&&(this.$vuepress.getLayoutAsyncComponent(e)||this.$vuepress.getVueComponent(e))?e:"Layout"}return"NotFound"}}},xi=Object(_i.a)(ki,(function(){var e=this.$createElement;return(this._self._c||e)(this.layout,{tag:"component"})}),[],!1,null,null,null).exports;!function(e,n,t){var o;switch(n){case"components":e[n]||(e[n]={}),Object.assign(e[n],t);break;case"mixins":e[n]||(e[n]=[]),(o=e[n]).push.apply(o,Object(Cs.a)(t));break;default:throw new Error("Unknown option name.")}}(xi,"mixins",yi);var Pi=[{name:"v-f2ef6f48",path:"/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-f2ef6f48").then(t)}},{path:"/index.html",redirect:"/"},{name:"v-5dc9dbff",path:"/_Footer.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-5dc9dbff").then(t)}},{name:"v-b3507e40",path:"/about/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-b3507e40").then(t)}},{path:"/about/index.html",redirect:"/about/"},{name:"v-73585e3c",path:"/intro/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-73585e3c").then(t)}},{path:"/intro/index.html",redirect:"/intro/"},{name:"v-77afd55f",path:"/intro/_Footer.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-77afd55f").then(t)}},{name:"v-147b6d82",path:"/intro/architecture.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-147b6d82").then(t)}},{name:"v-f8f918a8",path:"/operation/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-f8f918a8").then(t)}},{path:"/operation/index.html",redirect:"/operation/"},{name:"v-7c409ac2",path:"/operation/damocles-manager-config.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-7c409ac2").then(t)}},{name:"v-84157232",path:"/operation/damocles-worker-config.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-84157232").then(t)}},{name:"v-7f2f1742",path:"/operation/migrate-sectors.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-7f2f1742").then(t)}},{name:"v-113a25c1",path:"/operation/poster.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-113a25c1").then(t)}},{name:"v-3ece3291",path:"/operation/snapup.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-3ece3291").then(t)}},{name:"v-ef0bd702",path:"/operation/task-flow.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-ef0bd702").then(t)}},{name:"v-92b0af42",path:"/operation/task-management.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-92b0af42").then(t)}},{name:"v-b7b7c982",path:"/operation/worker-config-guide.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-b7b7c982").then(t)}},{name:"v-6971be1f",path:"/questions.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-6971be1f").then(t)}},{name:"v-5f486ce8",path:"/zh/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-5f486ce8").then(t)}},{path:"/zh/index.html",redirect:"/zh/"},{name:"v-3266a61f",path:"/zh/TODO.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-3266a61f").then(t)}},{name:"v-6cf8170c",path:"/zh/about/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-6cf8170c").then(t)}},{path:"/zh/about/index.html",redirect:"/zh/about/"},{name:"v-87056da8",path:"/zh/developer/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-87056da8").then(t)}},{path:"/zh/developer/index.html",redirect:"/zh/developer/"},{name:"v-e7947532",path:"/zh/developer/concept.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-e7947532").then(t)}},{name:"v-0d494d6c",path:"/zh/intro/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-0d494d6c").then(t)}},{path:"/zh/intro/index.html",redirect:"/zh/intro/"},{name:"v-20079582",path:"/zh/intro/_Footer.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-20079582").then(t)}},{name:"v-4fa6a05f",path:"/zh/intro/architecture.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-4fa6a05f").then(t)}},{name:"v-f05138fa",path:"/zh/intro/getting-started.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-f05138fa").then(t)}},{name:"v-d0b99468",path:"/zh/operation/",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-d0b99468").then(t)}},{path:"/zh/operation/index.html",redirect:"/zh/operation/"},{name:"v-7bbae69f",path:"/zh/operation/custom-algo.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-7bbae69f").then(t)}},{name:"v-20f290cb",path:"/zh/operation/damocles-manager-config.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-20f290cb").then(t)}},{name:"v-a98bfbc2",path:"/zh/operation/damocles-worker-config.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-a98bfbc2").then(t)}},{name:"v-104d39de",path:"/zh/operation/faq.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-104d39de").then(t)}},{name:"v-39b38d02",path:"/zh/operation/hugeTLB.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-39b38d02").then(t)}},{name:"v-6f0a5512",path:"/zh/operation/metrics.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-6f0a5512").then(t)}},{name:"v-ef86c67a",path:"/zh/operation/migrate-miner.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-ef86c67a").then(t)}},{name:"v-2e3168b2",path:"/zh/operation/migrate-sectors.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-2e3168b2").then(t)}},{name:"v-4578197f",path:"/zh/operation/poster.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-4578197f").then(t)}},{name:"v-08acd132",path:"/zh/operation/processors-config-example.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-08acd132").then(t)}},{name:"v-dd9c338e",path:"/zh/operation/quick-start.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-dd9c338e").then(t)}},{name:"v-e5c0bb42",path:"/zh/operation/sector-rebuild.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-e5c0bb42").then(t)}},{name:"v-592d247f",path:"/zh/operation/snapup.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-592d247f").then(t)}},{name:"v-12b90e11",path:"/zh/operation/task-flow.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-12b90e11").then(t)}},{name:"v-355617e7",path:"/zh/operation/task-management.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-355617e7").then(t)}},{name:"v-75a050c2",path:"/zh/operation/unseal.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-75a050c2").then(t)}},{name:"v-5a820f7d",path:"/zh/operation/worker-config-guide.html",component:xi,beforeEnter:function(e,n,t){Vs("Layout","v-5a820f7d").then(t)}},{path:"*",component:xi}],Si={title:"Damocles",description:"Damocles, formerly known as Venus Power Service, is THE Filecoin storage power solution.",base:"/",headTags:[["link",{rel:"icon",href:"/assets/damocles-icon.png"}],["script",{async:!0,src:"https://www.googletagmanager.com/gtag/js?id=G-C1545RTPE5"}],["script",{},["window.dataLayer = window.dataLayer || [];\nfunction gtag(){dataLayer.push(arguments);}\ngtag('js', new Date());\ngtag('config', 'G-C1545RTPE5');"]]],pages:[{title:"Home",frontmatter:{home:!0,heroImage:"/assets/damocles-hero.jpg",actionText:"Get Started →",actionLink:"/intro/",footer:"MIT Apache dual Licensed"},regularPath:"/",relativePath:"README.md",key:"v-f2ef6f48",path:"/",lastUpdated:"10/23/2023, 7:59:15 AM",headersStr:null,content:"Revamped Job Scheduling\n\nReimagined task scheduling where a manager passively listen to the sealing progress of workers, which ultimately results in 30% increase of sealing efficiency\n\n\nPrivate SaaS\n\nThrough Damocles' original architecture and broad selection of configurations, SPs can quickly organize a pool of workers into effectively a private SaaS (sealing-as-a-service) that can be used to seal for multiple miner_ids at the same time\n\n\nOptimized Snapdeal\n\nSP can batch import existing CC sectors as local candidate sectors. As storage deals come in, workers would go and grab candidate sectors by themselves and snap deal onto those candidate sectors minizing manual intervention",normalizedContent:"revamped job scheduling\n\nreimagined task scheduling where a manager passively listen to the sealing progress of workers, which ultimately results in 30% increase of sealing efficiency\n\n\nprivate saas\n\nthrough damocles' original architecture and broad selection of configurations, sps can quickly organize a pool of workers into effectively a private saas (sealing-as-a-service) that can be used to seal for multiple miner_ids at the same time\n\n\noptimized snapdeal\n\nsp can batch import existing cc sectors as local candidate sectors. as storage deals come in, workers would go and grab candidate sectors by themselves and snap deal onto those candidate sectors minizing manual intervention",charsets:{}},{frontmatter:{},regularPath:"/_Footer.html",relativePath:"_Footer.md",key:"v-5dc9dbff",path:"/_Footer.html",lastUpdated:"10/23/2023, 7:59:15 AM",headersStr:null,content:"See something missing? Have tips to share? File an issue, and we'll follow up as soon as possible. (If you have write permission in this repo, feel free to edit directly.)",normalizedContent:"see something missing? have tips to share? file an issue, and we'll follow up as soon as possible. (if you have write permission in this repo, feel free to edit directly.)",charsets:{}},{frontmatter:{},regularPath:"/about/",relativePath:"about/README.md",key:"v-b3507e40",path:"/about/",headers:[{level:2,title:"Mission, Vision, Value",slug:"mission-vision-value",normalizedTitle:"mission, vision, value",charIndex:2},{level:2,title:"Contacts",slug:"contacts",normalizedTitle:"contacts",charIndex:192},{level:2,title:"Resources",slug:"resources",normalizedTitle:"resources",charIndex:278}],lastUpdated:"10/23/2023, 7:59:15 AM",headersStr:"Mission, Vision, Value Contacts Resources",content:"# Mission, Vision, Value\n\n * ❗️Mission: push ecosystem forward by driving Filcoin infrastructure\n * 🌏 Vision: democratize Filecoin ecosystem participation\n * ☯️ Value: Open and trustless\n\n\n# Contacts\n\n * Ask us any questions: #fil-venus, #fil-venus-cn, venus@ipfsforce.com\n\n\n# Resources\n\n * venus-docs: venus.filecoin.io\n * VenusHub: venushub.io\n * Github: damocles, venus, droplet\n * Social Media: Twitter, Wechat\n * Security audit report: link",normalizedContent:"# mission, vision, value\n\n * ❗️mission: push ecosystem forward by driving filcoin infrastructure\n * 🌏 vision: democratize filecoin ecosystem participation\n * ☯️ value: open and trustless\n\n\n# contacts\n\n * ask us any questions: #fil-venus, #fil-venus-cn, venus@ipfsforce.com\n\n\n# resources\n\n * venus-docs: venus.filecoin.io\n * venushub: venushub.io\n * github: damocles, venus, droplet\n * social media: twitter, wechat\n * security audit report: link",charsets:{}},{title:"What is damocles?",frontmatter:{},regularPath:"/intro/",relativePath:"intro/README.md",key:"v-73585e3c",path:"/intro/",headers:[{level:2,title:"What is damocles?",slug:"what-is-damocles",normalizedTitle:"what is damocles?",charIndex:2},{level:2,title:"Features",slug:"features",normalizedTitle:"features",charIndex:187},{level:3,title:"Complete revamp of existing job scheduling",slug:"complete-revamp-of-existing-job-scheduling",normalizedTitle:"complete revamp of existing job scheduling",charIndex:200},{level:3,title:"Horizontal scaling of your workers",slug:"horizontal-scaling-of-your-workers",normalizedTitle:"horizontal scaling of your workers",charIndex:689},{level:3,title:"Decoupling of Post workers",slug:"decoupling-of-post-workers",normalizedTitle:"decoupling of post workers",charIndex:1009},{level:3,title:"Pooling of worker resources",slug:"pooling-of-worker-resources",normalizedTitle:"pooling of worker resources",charIndex:1203},{level:3,title:"Customized sealing tasks",slug:"customized-sealing-tasks",normalizedTitle:"customized sealing tasks",charIndex:1541}],lastUpdated:"10/23/2023, 7:59:15 AM",headersStr:"What is damocles? Features Complete revamp of existing job scheduling Horizontal scaling of your workers Decoupling of Post workers Pooling of worker resources Customized sealing tasks",content:"# What is damocles?\n\ndamocles is the next generation venus-sealer, which offers a configurable, expandable and customizable storage power solution for the Filecoin storage providers.\n\n\n# Features\n\n\n# Complete revamp of existing job scheduling\n\nManaging sealing pipeline traditionally not only requires extensive knowledge across sealing mechanism and hardware specifications, but also demand a lot of devOps attentions for maintenance and troubleshooting. By allowing damocles-workers to change the state machine of a sealing sector, instead of having an active and centralized job scheduling model, damocles-cluster introduced a new model that worker itself actively grabs new tasks.\n\n\n# Horizontal scaling of your workers\n\nWith extensive configurations damocles supports to optimize your setups, a storage provider can easily take the config file of one worker and apply it to another worker with same hardware specification. Thus enabling ultra fast expansion of a storage provider’s sealing capacity.\n\n\n# Decoupling of Post workers\n\ndamocles supports worker machines to be setup as dedicated Post workers, relieve the headache that sealing tasks may compete resources with essential post tasks.\n\n\n# Pooling of worker resources\n\nThanks to the new scheduling model, storage power services (sealing pipeline and Post workers) by damocles can be used to serve multiple nodes (miner_id). The pooling of workers essentially allows storage providers more flexibility in their deployment and saves hardware resources and operation overhead.\n\n\n# Customized sealing tasks\n\nIf storage provider has extensive background in optimizing a particular phase of sealing task, maintaining a forked reference implementation of Filecoin is no longer as damocles allows your custom code to be plugged into the sealing pipeline directly through simple configuration.",normalizedContent:"# what is damocles?\n\ndamocles is the next generation venus-sealer, which offers a configurable, expandable and customizable storage power solution for the filecoin storage providers.\n\n\n# features\n\n\n# complete revamp of existing job scheduling\n\nmanaging sealing pipeline traditionally not only requires extensive knowledge across sealing mechanism and hardware specifications, but also demand a lot of devops attentions for maintenance and troubleshooting. by allowing damocles-workers to change the state machine of a sealing sector, instead of having an active and centralized job scheduling model, damocles-cluster introduced a new model that worker itself actively grabs new tasks.\n\n\n# horizontal scaling of your workers\n\nwith extensive configurations damocles supports to optimize your setups, a storage provider can easily take the config file of one worker and apply it to another worker with same hardware specification. thus enabling ultra fast expansion of a storage provider’s sealing capacity.\n\n\n# decoupling of post workers\n\ndamocles supports worker machines to be setup as dedicated post workers, relieve the headache that sealing tasks may compete resources with essential post tasks.\n\n\n# pooling of worker resources\n\nthanks to the new scheduling model, storage power services (sealing pipeline and post workers) by damocles can be used to serve multiple nodes (miner_id). the pooling of workers essentially allows storage providers more flexibility in their deployment and saves hardware resources and operation overhead.\n\n\n# customized sealing tasks\n\nif storage provider has extensive background in optimizing a particular phase of sealing task, maintaining a forked reference implementation of filecoin is no longer as damocles allows your custom code to be plugged into the sealing pipeline directly through simple configuration.",charsets:{}},{frontmatter:{},regularPath:"/intro/_Footer.html",relativePath:"intro/_Footer.md",key:"v-77afd55f",path:"/intro/_Footer.html",lastUpdated:"10/23/2023, 7:59:15 AM",headersStr:null,content:"See something missing? Have tips to share? File an issue, and we'll follow up as soon as possible. (If you have write permission in this repo, feel free to edit directly.)",normalizedContent:"see something missing? have tips to share? file an issue, and we'll follow up as soon as possible. (if you have write permission in this repo, feel free to edit directly.)",charsets:{}},{title:"Background",frontmatter:{},regularPath:"/intro/architecture.html",relativePath:"intro/architecture.md",key:"v-147b6d82",path:"/intro/architecture.html",headers:[{level:2,title:"Background",slug:"background",normalizedTitle:"background",charIndex:2},{level:2,title:"Architecture of venus-cluster",slug:"architecture-of-venus-cluster",normalizedTitle:"architecture of venus-cluster",charIndex:891},{level:3,title:"Sealing pipeline optimization",slug:"sealing-pipeline-optimization",normalizedTitle:"sealing pipeline optimization",charIndex:1359},{level:3,title:"devOps optimization",slug:"devops-optimization",normalizedTitle:"devops optimization",charIndex:1976},{level:3,title:"Configuration architecture",slug:"configuration-architecture",normalizedTitle:"configuration architecture",charIndex:2577}],lastUpdated:"10/23/2023, 7:59:15 AM",headersStr:"Background Architecture of venus-cluster Sealing pipeline optimization devOps optimization Configuration architecture",content:"# Background\n\nThere have long been problems regarding how storage provider can maintain an efficient and stable storage power services (sealing pipeline and PoSt workers). To name a few...\n\n * Job scheduling issues where a centralized job scheduler would clog your sealing pipeline with too many tasks, wasting disk IO and network resources on unnecessary data transfer or fail to utilize your hardware to its full capacity\n * Growth expansion issues where a storage provider would struggle in scaling its sealing pipeline and finding the optimal setup for newly added machines\n * Custom code issues where a storage provider with programming background have to maintain a forked version of the reference implementation of Filecoin\n\nvenus-cluster is born given the above premises along with other pressing issues to help storage provider to alleviate and address these exact pain points.\n\n\n# Architecture of venus-cluster\n\nThe sealer subsystem, which is responsible for the growth and maintenance of storage power, is traditionally composed of either lotus-miner + lotus-worker or venus-sealer + venus-worker. In a Venus storage system, the sealer subsystem will be replaced by venus-sector-manager + venus-worker. Note that venus-worker compiled from venus-cluster project is completely different from the venus-worker compiled from venus-sealer project.\n\n\n# Sealing pipeline optimization\n\nThere are a few key concepts of venus-cluster that fundamentally changed the architecture of a sealing pipeline that one has to be aware of.\n\n * One worker machine handles all phases of sealing where a sector will go through all phases of sealing on one worker machine\n * Passive job scheduling where sector-manager passively receives task updates from worker while workers themselves actively seek out tasks to run\n * Configurable resource management where the computation and storage resources could be planned and isolated for each sealing tasks through extensive configurations\n\n\n# devOps optimization\n\nWith architectural changes of venus-cluster, it presents new streamlined ways of managing your storage system.\n\n * Decoupling of Post tasks where you can deploy dedicated Post worker to avoid resource competition with your sealing tasks\n * Uniformed and easy deployment where you can take configuration file of one worker and apply it to a new worker with same hardware specification to achieve same sealing capacity as the previous worker\n * Fault tolerance and quicker diagnosis where smarter retry mechanism is implemented and locate errors faster in your sealing pipeline\n\n\n# Configuration architecture\n\n\n\nvenus-cluster allows extensive configurations of worker’s planning and isolation of its computational resources (CPU, RAM, GPU) and its storage resources (SSD, NVMe). Proper configuration of computational resources and storage resources would contribute to a highly efficient sealing pipeline. Most notably, [[processors]] configures the isolation of the computational resources on a venus-worker and [[sealing_thread]] configures the task control and isolation of storage resources on a venus-worker. This means that when you have one [[processors.pc1]] configured, venus-worker would allocate the configured computational resources for use of this pc1 process only. Similarly, when you have one [[sealing_thread]] configured, venus-worker will allocate a section of your storage resources for storing all temporary files when sealing a sector.",normalizedContent:"# background\n\nthere have long been problems regarding how storage provider can maintain an efficient and stable storage power services (sealing pipeline and post workers). to name a few...\n\n * job scheduling issues where a centralized job scheduler would clog your sealing pipeline with too many tasks, wasting disk io and network resources on unnecessary data transfer or fail to utilize your hardware to its full capacity\n * growth expansion issues where a storage provider would struggle in scaling its sealing pipeline and finding the optimal setup for newly added machines\n * custom code issues where a storage provider with programming background have to maintain a forked version of the reference implementation of filecoin\n\nvenus-cluster is born given the above premises along with other pressing issues to help storage provider to alleviate and address these exact pain points.\n\n\n# architecture of venus-cluster\n\nthe sealer subsystem, which is responsible for the growth and maintenance of storage power, is traditionally composed of either lotus-miner + lotus-worker or venus-sealer + venus-worker. in a venus storage system, the sealer subsystem will be replaced by venus-sector-manager + venus-worker. note that venus-worker compiled from venus-cluster project is completely different from the venus-worker compiled from venus-sealer project.\n\n\n# sealing pipeline optimization\n\nthere are a few key concepts of venus-cluster that fundamentally changed the architecture of a sealing pipeline that one has to be aware of.\n\n * one worker machine handles all phases of sealing where a sector will go through all phases of sealing on one worker machine\n * passive job scheduling where sector-manager passively receives task updates from worker while workers themselves actively seek out tasks to run\n * configurable resource management where the computation and storage resources could be planned and isolated for each sealing tasks through extensive configurations\n\n\n# devops optimization\n\nwith architectural changes of venus-cluster, it presents new streamlined ways of managing your storage system.\n\n * decoupling of post tasks where you can deploy dedicated post worker to avoid resource competition with your sealing tasks\n * uniformed and easy deployment where you can take configuration file of one worker and apply it to a new worker with same hardware specification to achieve same sealing capacity as the previous worker\n * fault tolerance and quicker diagnosis where smarter retry mechanism is implemented and locate errors faster in your sealing pipeline\n\n\n# configuration architecture\n\n\n\nvenus-cluster allows extensive configurations of worker’s planning and isolation of its computational resources (cpu, ram, gpu) and its storage resources (ssd, nvme). proper configuration of computational resources and storage resources would contribute to a highly efficient sealing pipeline. most notably, [[processors]] configures the isolation of the computational resources on a venus-worker and [[sealing_thread]] configures the task control and isolation of storage resources on a venus-worker. this means that when you have one [[processors.pc1]] configured, venus-worker would allocate the configured computational resources for use of this pc1 process only. similarly, when you have one [[sealing_thread]] configured, venus-worker will allocate a section of your storage resources for storing all temporary files when sealing a sector.",charsets:{}},{title:"Getting started with damocles",frontmatter:{},regularPath:"/operation/",relativePath:"operation/README.md",key:"v-f8f918a8",path:"/operation/",headers:[{level:2,title:"Getting started with damocles",slug:"getting-started-with-damocles",normalizedTitle:"getting started with damocles",charIndex:2},{level:2,title:"Preparation",slug:"preparation",normalizedTitle:"preparation",charIndex:111},{level:2,title:"Mock mode",slug:"mock-mode",normalizedTitle:"mock mode",charIndex:787},{level:3,title:"damocles-manager",slug:"damocles-manager",normalizedTitle:"damocles-manager",charIndex:438},{level:3,title:"damocles-worker",slug:"damocles-worker",normalizedTitle:"damocles-worker",charIndex:418},{level:2,title:"Production mode",slug:"production-mode",normalizedTitle:"production mode",charIndex:1648},{level:3,title:"damocles-manager",slug:"damocles-manager-2",normalizedTitle:"damocles-manager",charIndex:438},{level:3,title:"damocles-worker",slug:"damocles-worker-2",normalizedTitle:"damocles-worker",charIndex:418}],lastUpdated:"10/23/2023, 7:59:15 AM",headersStr:"Getting started with damocles Preparation Mock mode damocles-manager damocles-worker Production mode damocles-manager damocles-worker",content:"# Getting started with damocles\n\nThis guide will walk you through how to qiuckly get started with damocles\n\n\n# Preparation\n\n 1. Install 3rd party dependencies. Please refer to instrcutions here.\n\n 2. Download source code.\n\n$ git clone https://github.com/ipfs-force-community/damocles.git\n\n\n 3. Compile damocles component.\n\n$ cd damocles\n$ make all\n\n\nTIP\n\nAfter completion, you should be able to find binaries for both damocles-worker and damocles-manager.\n\n 4. Copy the binaries to your machine(s).\n\n 5. Copy ./damocles-worker/create-cgroup.sh to all damocles-worker machine and execute the script under same user which you are going to run damocles-worker.\n\nTIP\n\nThe script will generate cgroup for the user, which allows damocles-worker to allocate hardware resources accordingly.\n\n\n# Mock mode\n\nBy default, starting a set of mock instance can be done by a series of commands.\n\n\n# damocles-manager\n\nFor example, start damocles-manager with a dummy miner actor t010000 and schedule sealing jobs with sector size of 2KiB.\n\n$ ./dist/bin/damocles-manager mock --miner=10000 --sector-size=2KiB\n\n\nTIP\n\n./mock/start_smgr.sh could also be used to do this.\n\n\n# damocles-worker\n\n 1. Init both sealing and permanent storage.\n\n$ ./dist/bin/damocles-worker store sealing-init -l ./mock-tmp/store1 ./mock-tmp/store2 ./mock-tmp/store3\n\n$ ./dist/bin/damocles-worker store file-init -l ./mock-tmp/remote\n\n\nTIP\n\n./mock/cleanup_store.sh could also be used to do this.\n\n 2. Start damocles-worker in mock mode.\n\n$ ./dist/bin/damocles-worker daemon -c ./damocles-worker/assets/damocles-worker.mock.toml\n\n\nTIP\n\n./mock/start_worker.sh could also be used to do this.\n\n\n# Production mode\n\n\n# damocles-manager\n\n 1. Init working directories.\n\n$ ./dist/bin/damocles-manager daemon init\n\n\n 2. Configure ~/.damocles-manager/sector-manager.cfg per your use case.\n\nTIP\n\nFor more details on what each configuration does, please refer to this document.\n\n 3. Start damocles-manager.\n\n$ ./dist/bin/damocles-manager daemon run\n\n\n\n# damocles-worker\n\n 1. Init sealing path for unsealed sector(s).\n\n$ ./dist/bin/damocles-worker store sealing-init -l <dir1> <dir2> <dir3> <...>\n\n\n 2. Init permanent storage path for sealed sector(s).\n\n$ ./dist/bin/damocles-worker store file-init -l <dir1>\n\n\n 3. Configure your damocles-worker according to your planning of CPU cores, numa, zone and etc for each sealing phases.\n\nTIP\n\nFor more details on what each configuration does, please refer to this document.\n\n 4. Start damocles-worker.\n\n$ /path/to/damocles-worker daemon -c /path/to/damocles-worker.toml\n",normalizedContent:"# getting started with damocles\n\nthis guide will walk you through how to qiuckly get started with damocles\n\n\n# preparation\n\n 1. install 3rd party dependencies. please refer to instrcutions here.\n\n 2. download source code.\n\n$ git clone https://github.com/ipfs-force-community/damocles.git\n\n\n 3. compile damocles component.\n\n$ cd damocles\n$ make all\n\n\ntip\n\nafter completion, you should be able to find binaries for both damocles-worker and damocles-manager.\n\n 4. copy the binaries to your machine(s).\n\n 5. copy ./damocles-worker/create-cgroup.sh to all damocles-worker machine and execute the script under same user which you are going to run damocles-worker.\n\ntip\n\nthe script will generate cgroup for the user, which allows damocles-worker to allocate hardware resources accordingly.\n\n\n# mock mode\n\nby default, starting a set of mock instance can be done by a series of commands.\n\n\n# damocles-manager\n\nfor example, start damocles-manager with a dummy miner actor t010000 and schedule sealing jobs with sector size of 2kib.\n\n$ ./dist/bin/damocles-manager mock --miner=10000 --sector-size=2kib\n\n\ntip\n\n./mock/start_smgr.sh could also be used to do this.\n\n\n# damocles-worker\n\n 1. init both sealing and permanent storage.\n\n$ ./dist/bin/damocles-worker store sealing-init -l ./mock-tmp/store1 ./mock-tmp/store2 ./mock-tmp/store3\n\n$ ./dist/bin/damocles-worker store file-init -l ./mock-tmp/remote\n\n\ntip\n\n./mock/cleanup_store.sh could also be used to do this.\n\n 2. start damocles-worker in mock mode.\n\n$ ./dist/bin/damocles-worker daemon -c ./damocles-worker/assets/damocles-worker.mock.toml\n\n\ntip\n\n./mock/start_worker.sh could also be used to do this.\n\n\n# production mode\n\n\n# damocles-manager\n\n 1. init working directories.\n\n$ ./dist/bin/damocles-manager daemon init\n\n\n 2. configure ~/.damocles-manager/sector-manager.cfg per your use case.\n\ntip\n\nfor more details on what each configuration does, please refer to this document.\n\n 3. start damocles-manager.\n\n$ ./dist/bin/damocles-manager daemon run\n\n\n\n# damocles-worker\n\n 1. init sealing path for unsealed sector(s).\n\n$ ./dist/bin/damocles-worker store sealing-init -l <dir1> <dir2> <dir3> <...>\n\n\n 2. init permanent storage path for sealed sector(s).\n\n$ ./dist/bin/damocles-worker store file-init -l <dir1>\n\n\n 3. configure your damocles-worker according to your planning of cpu cores, numa, zone and etc for each sealing phases.\n\ntip\n\nfor more details on what each configuration does, please refer to this document.\n\n 4. start damocles-worker.\n\n$ /path/to/damocles-worker daemon -c /path/to/damocles-worker.toml\n",charsets:{}},{title:"Configuration of damocles-manager",frontmatter:{},regularPath:"/operation/damocles-manager-config.html",relativePath:"operation/damocles-manager-config.md",key:"v-7c409ac2",path:"/operation/damocles-manager-config.html",headers:[{level:2,title:"[Common]",slug:"common",normalizedTitle:"[common]",charIndex:281},{level:3,title:"[Common.API]",slug:"common-api",normalizedTitle:"[common.api]",charIndex:290},{level:3,title:"[Common.Plugins]",slug:"common-plugins",normalizedTitle:"[common.plugins]",charIndex:5416},{level:3,title:"[[Common.PieceStores]]",slug:"common-piecestores",normalizedTitle:"[[common.piecestores]]",charIndex:534},{level:3,title:"[[Common.PersistStores]]",slug:"common-persiststores",normalizedTitle:"[[common.persiststores]]",charIndex:691},{level:3,title:"[Common.MongoKVStore] Deprecated",slug:"common-mongokvstore-deprecated",normalizedTitle:"[common.mongokvstore] deprecated",charIndex:10113},{level:3,title:"[Common.Proving]",slug:"common-proving",normalizedTitle:"[common.proving]",charIndex:1003},{level:3,title:"[Common.Proving.WorkerProver]",slug:"common-proving-workerprover",normalizedTitle:"[common.proving.workerprover]",charIndex:1109},{level:3,title:"[Common.DB]",slug:"common-db",normalizedTitle:"[common.db]",charIndex:939},{level:2,title:"[[Miners]]",slug:"miners",normalizedTitle:"[[miners]]",charIndex:1203},{level:3,title:"Main configuration item",slug:"main-configuration-item",normalizedTitle:"main configuration item",charIndex:14313},{level:3,title:"[Miners.Sector]",slug:"miners-sector",normalizedTitle:"[miners.sector]",charIndex:1229},{level:3,title:"[Miners.SnapUp]",slug:"miners-snapup",normalizedTitle:"[miners.snapup]",charIndex:1372},{level:3,title:"[Miners.Commitment]",slug:"miners-commitment",normalizedTitle:"[miners.commitment]",charIndex:1749},{level:3,title:"[Miners.Commitment.Pre]",slug:"miners-commitment-pre",normalizedTitle:"[miners.commitment.pre]",charIndex:1786},{level:3,title:"[Miners.Commitment.Prove]",slug:"miners-commitment-prove",normalizedTitle:"[miners.commitment.prove]",charIndex:2164},{level:3,title:"[Miners.Commitment.Terminate]",slug:"miners-commitment-terminate",normalizedTitle:"[miners.commitment.terminate]",charIndex:2546},{level:3,title:"[Miners.PoSt]",slug:"miners-post",normalizedTitle:"[miners.post]",charIndex:2935},{level:3,title:"[Miners.Proof]",slug:"miners-proof",normalizedTitle:"[miners.proof]",charIndex:3306},{level:3,title:"[Miners.Sealing]",slug:"miners-sealing",normalizedTitle:"[miners.sealing]",charIndex:3338},{level:3,title:"[Miners.Deal] Deprecated",slug:"miners-deal-deprecated",normalizedTitle:"[miners.deal] deprecated",charIndex:23797},{level:2,title:"A minimal working configuration file example",slug:"a-minimal-working-configuration-file-example",normalizedTitle:"a minimal working configuration file example",charIndex:23964}],headersStr:"[Common] [Common.API] [Common.Plugins] [[Common.PieceStores]] [[Common.PersistStores]] [Common.MongoKVStore] Deprecated [Common.Proving] [Common.Proving.WorkerProver] [Common.DB] [[Miners]] Main configuration item [Miners.Sector] [Miners.SnapUp] [Miners.Commitment] [Miners.Commitment.Pre] [Miners.Commitment.Prove] [Miners.Commitment.Terminate] [Miners.PoSt] [Miners.Proof] [Miners.Sealing] [Miners.Deal] Deprecated A minimal working configuration file example",content:'# Configuration of damocles-manager\n\ndamocles-manager is the main component for interacting with the chain and maintaining the sectors. Let\'s take a look at its configuration file structure.\n\nAfter initialization, we can get a copy of the default configuration:\n\n# Default config:\n[Common]\n[Common.API]\n#Gateway = ["/ip4/{api_host}/tcp/{api_port}"]\n#Token = "{some token}"\n#ChainEventInterval = "1m0s"\n#Chain = "/ip4/{api_host}/tcp/{api_port}"\n#Messager = "/ip4/{api_host}/tcp/{api_port}"\n#Market = "/ip4/{api_host}/tcp/{api_port}"\n#\n[[Common.PieceStores]]\n#Name = "{store_name}"\n#Path = "{store_path}"\n#Plugin = ""\n#PluginName = "s3store"\n[Common.PieceStores.Meta]\n#SomeKey = "SomeValue"\n#\n[[Common.PersistStores]]\n#Name = "{store_name}"\n#Path = "{store_path}"\n#Strict = false\n#ReadOnly = false\n#Weight = 0\n#AllowMiners = [1, 2]\n#DenyMiners = [3, 4]\n#Plugin = ""\n#PluginName = "s3store"\n[Common.PersistStores.Meta]\n#SomeKey = "SomeValue"\n[Common.DB]\n#Driver = "badger"\n[Common.DB.Badger]\n#BaseDir = ""\n[Common.Proving]\n#ParallelCheckLimit = 128\n#SingleCheckTimeout = "10m0s"\n#PartitionCheckTimeout = "20m0s"\n[Common.Proving.WorkerProver]\nJobMaxTry = 2\nHeartbeatTimeout = "15s"\nJobLifetime = "25h0m0s"\n\n[[Miners]]\n#Actor = 10086\n[Miners.Sector]\n#InitNumber = 0\n#MinNumber = 10\n#MaxNumber = 1000000\n#Enabled = true\n#EnableDeals = false\n#LifetimeDays = 540\n#Verbose = false\n[Miners.SnapUp]\n#Enabled = false\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#SendFund = true\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n#CleanupCCData = true\n#MessageConfidence = 15\n#ReleaseConfidence = 30\n[Miners.SnapUp.Retry]\n#MaxAttempts = 10\n#PollInterval = "3m0s"\n#APIFailureWait = "3m0s"\n#LocalFailureWait = "3m0s"\n[Miners.Commitment]\n#Confidence = 10\n[Miners.Commitment.Pre]\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#SendFund = true\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Pre.Batch]\n#Enabled = false\n#Threshold = 16\n#MaxWait = "1h0m0s"\n#CheckInterval = "1m0s"\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Prove]\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#SendFund = true\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Prove.Batch]\n#Enabled = false\n#Threshold = 16\n#MaxWait = "1h0m0s"\n#CheckInterval = "1m0s"\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Terminate]\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#SendFund = true\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Terminate.Batch]\n#Enabled = false\n#Threshold = 5\n#MaxWait = "1h0m0s"\n#CheckInterval = "1m0s"\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.PoSt]\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#Enabled = true\n#StrictCheck = true\n#Parallel = false\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n#Confidence = 10\n#SubmitConfidence = 0\n#ChallengeConfidence = 0\n#MaxRecoverSectorLimit = 0\n#MaxPartitionsPerPoStMessage = 0\n#MaxPartitionsPerRecoveryMessage = 0\n[Miners.Proof]\n#Enabled = false\n[Miners.Sealing]\n#SealingEpochDuration = 0\n#UseSyntheticPoRep = false\n#\n\n\nWe will break down each configurable item one by one.\n\n\n# [Common]\n\nCommon section includes common configuration, which is further divided into four sub-configuration items:\n\n\n# [Common.API]\n\nCommon.API is interface related configuration, its content includes:\n\n[Common.API]\n# Gateway service infos, required, string type\n# Fill in according to the actual situation of the service used\n# For each one contained, if the item is valid as a token-included-info-string ("{token}:{multiaddr}"), the token included would be used to construct the rpc client instead of the common token.\nGateway = ["/ip4/{api_host}/tcp/{api_port}"]\n\n# common token for services, required, string type\n# Fill in according to the actual situation of the service used\nToken = "{some token}"\n\n# Chain service info, optional, string type\n# Fill in according to the actual situation of the service used\n# If the field is valid as a token-included-info-string ("{token}:{multiaddr}"), the token included would be used to construct the rpc client instead of the common token.\n# If not set, use value of Gateway as default \nChain = "/ip4/{api_host}/tcp/{api_port}"\n\n# Message service info, optional, string type\n# Fill in according to the actual situation of the service used\n# If the field is valid as a token-included-info-string ("{token}:{multiaddr}"), the token included would be used to construct the rpc client instead of the common token.\n# If not set, use value of Gateway as default \nMessager = "/ip4/{api_host}/tcp/{api_port}"\n\n# Market service info, optional, string type\n# Fill in according to the actual situation of the service used\n# If the field is valid as a token-included-info-string ("{token}:{multiaddr}"), the token included would be used to construct the rpc client instead of the common token.\n# If not set, use value of Gateway as default \nMarket = "/ip4/{api_host}/tcp/{api_port}"\n\n# Interval time for detecting chain height changes, optional, duration type\n# Default is 1min\n#ChainEventInterval = "1m0s"\n\n\n\n# [Common.Plugins]\n\nCommon.Plugins is used for configuring the plugin path of damocles-manager.\n\n[Common.Plugins]\n# Path where plugins are stored, optional，string type\n# Default is an empty string, which means that no plugin will be loaded.\n# It is recommended to use absolute paths.\nDir = ""\n\n\n\n# [[Common.PieceStores]]\n\nCommon.PieceStores is used for configuring local deal piece data. When there is available offline deal, you can configure this item to avoid getting the deal piece data through public network traffic.\n\nEach local store directory should correspond to a Common.PieceStores configuration block.\n\n# Basic configuration example\n\n[[Common.PieceStores]]\n# name, optional, string type\n# The default is the absolute path corresponding to the path\n#Name = "remote-store1"\n\n# path, required, string type\nPath = "/mnt/mass/piece1"\n\n# Plugin path, optional, string type\n# default is null\n# If you would like to use a custom storage scheme, you can write a golang plugin that meets the requirements and set it here.\n# Deprecated, Please use PluginName instead.\n#Plugin = "path/to/objstore-plugin"\n\n# Plugin path, optional, string type\n# default is empty string\n# If you would like to use a custom storage scheme, you can write a golang plugin that meets the requirements and set it here.\n#PluginName = "s3store"\n\n# Meta information, optional items, dictionary type\n# The internal value is in the format of Key = "Value"\n# Default value is null\n# Used to support different types of storage schemes\n[Common.PieceStores.Meta]\n#SomeKey = "SomeValue"\n#\n\n\n\n# [[Common.PersistStores]]\n\nCommon.PersistStores is used to configure sector persistent data stores. It corresponds to the attached concept in damocles-worker.\n\nSimilar to Common.PieceStores, each persistent store directory should correspond to a Common.PersistStores configuration block.\n\n# Basic configuration example\n\n[[Common.PersistStores]]\n# name, optional, string type\n## Default is the absolute path corresponding to the path\n#Name = "remote-store1"\n\n# path, required, string type\n# It is recommended to use absolute paths\nPath = "/mnt/remote/10.0.0.14/store"\n\n# read only, optional, boolean\n# Default is false\n# From v0.4.0 and above, the persistent storage allocation logic goes to damocles-manager\n# This configuration can be used to set whether you can continue to write to the storage\n#ReadOnly = false\n\n# optional, boolean\n# Default is false\n# Whether to validate if the Path is a regular file. If set to True, an error will be raised if the Path is a non-regular file, such as a symbolic link.\n#Strict = false\n\n# weight, optional, number type\n# Default is 1\n# When the filled value is 0, it is equivalent to 1\n# From v0.4.0 and above, the persistent storage allocation logic goes to damocles-manager\n# This configuration can be used to set the weight allocation ratio between multiple persistent stores\n# The probability of each persistent stores being selected is `weight / sum`, where `sum` is the sum of the weights of all available persistent stores.\n# Example: Configure 3 persistent stores, the weights are 2, 1, 1. The probability of being selected is 50%, 25%, 25% respectively\n#Weight = 1\n\n# Plugin path, optional, string type\n# default is null\n# If you want to use a custom storage scheme, you can write a golang plugin that meets the requirements and set it here.\n# Deprecated, Please use PluginName instead.\n#Plugin = "path/to/objstore-plugin"\n\n# List of miner IDs that are allowed, optional, numeric array type\n# default is null\n# When not set, it is regarded as allowing all miner IDs; when set, it is equivalent to a whitelist, which allows only listed miner IDs\n# If a miner ID appears in AllowMiners and DenyMiners at the same time, DenyMiners will take effect first, which is considered blacklisted\n#AllowMiners = [1, 2]\n\n# List of miner IDs that are being denied, optional, numeric array type\n# default is null\n# When not set, it is regarded as not rejecting any miner number; when set, it is equivalent to a blacklist, which will deny the listed miner ID\n# If a miner ID appears in both AllowMiners and DenyMiners, DenyMiners will take effect first, which is considered blacklisted\n#DenyMiners = [3, 4]\n\n# Plugin path, optional, string type\n# default is empty string\n# If you would like to use a custom storage scheme, you can write a golang plugin that meets the requirements and set it here.\n#PluginName = "s3store"\n\n# Meta information, optional items, dictionary type\n# The internal value is in the format of Key = "Value"\n# Default value is null\n# Used to support the preparation of different types of storage schemes, currently has no effect\n[Common.PersistStores.Meta]\n#SomeKey = "SomeValue"\n#\n\n\n\n# [Common.MongoKVStore] Deprecated\n\nCommon.MongoKVStore is used to configure whether damocles-manager use MongoDB as KV database during sealing.\n\n# Basic configuration example\n\n[Common.MongoKVStore]\n# Switch of Mongo KV, optional, boolean type\n# default is false \nEnable = true\n# DSN of Mongo, when `Enable` is true, `DSN` is required, string type.\nDSN = "mongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000"\n# DatabaseName of Mongo, when `Enable` is true, `DatabaseName` is required, string type.\nDatabaseName = "test"\n\n\n\n# [Common.Proving]\n\nProving used to control the number of proving parallels and check timeouts.\n\nexample:\n\n# Maximum number of sector checks to run in parallel, optional, time type\n# Default is 128. (0 = unlimited)\n# WARNING: Setting this value too high may make the node crash by running out of stack\n# WARNING: Setting this value too low may make sector challenge reading much slower, resulting in failed PoSt due\n# to late submission.\n#ParallelCheckLimit = 128\n# Maximum amount of time a proving pre-check can take for a sector, optional, time type\n# Default is 10m0s.\n# If the check times out the sector will be skipped\n# WARNING: Setting this value too low risks in sectors being skipped even though they are accessible, just reading the test challenge took longer than this timeout\n# WARNING: Setting this value too high risks missing PoSt deadline in case IO operations related to this sector are blocked (e.g. in case of disconnected NFS mount)\n#SingleCheckTimeout = "10m0s"\n# Maximum amount of time a proving pre-check can take for an entire partition, optional time type\n# Default is 20m0s.\n# If the check times out, sectors in the partition which didn\'t get checked on time will be skipped\n# WARNING: Setting this value too low risks in sectors being skipped even though they are accessible, just reading the test challenge took longer than this timeout\n# WARNING: Setting this value too high risks missing PoSt deadline in case IO operations related to this partition are blocked or slow\n#PartitionCheckTimeout = "20m0s"\n\n\n\n# [Common.Proving.WorkerProver]\n\nUsed to configure the worker prover module\n\nexample:\n\n# The maximum number of attempts of the WindowPoSt job, optional, number type\n# Default is 2\n# job that exceeds the JobMaxTry number can only be re-executed by manual reset\nJobMaxTry = 2\n# The timeout of the WindowPoSt job\'s heartbeat, optional, time type\n# Default is 15s\n# jobs that have not sent a heartbeat for more than this time will be set to fail and retried\nHeartbeatTimeout = "15s"\n# The heartbeat timeout of the WindowPoSt job, optional, time type\n# Default is 25h\n# WindowPoSt jobs created longer than this time will be deleted\nJobLifetime = "25h0m0s"\n\n\n\n# [Common.DB]\n\nCommon.DB is used to configure KV database used by damocles-manager during sealing. Currently, the badger local database and mongo database are supported.\n\n# Basic configuration example:\n\n[Common.DB]\n# Specify database, optional, string type\n# Default is badger\n# All options: badger | mongo | plugin\nDriver = "badger"\n[Common.DB.Badger]\n# Basedir of the badger, optional, string type\n# Default value is empty string\n# Use home dir (default is ~/.damocles-manager) to store badger database files if BaseDir is empty string\n#BaseDir = ""\n[Common.DB.Mongo]\n# DSN of Mongo, when `Enable` is true, `DSN` is required, string type.\nDSN = "mongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000"\n# DatabaseName of Mongo, when `Enable` is true, `DatabaseName` is required, string type.\nDatabaseName = "test"\n[Common.DB.Plugin]\n# The plugin name, optional，string type\n# Default is null\nPluginName = "redis"\n# Metadata, optional, dict type\n# The internal value is in the format of Key = "Value"\n# Default is null\n# The Metadata will be pass to the constructor of the plugin\n[Common.DB.Plugin.Meta]\n#SomeKey = "SomeValue"\n#\n\n\n\n# [[Miners]]\n\nMiners is an important configuration item, which is used to define behavior and policy for a certain SP.\n\ndamocles is designed to support multiple SPs with the same set of components. This is reflected in damocles-manager, which you can set multiple Miners configuration blocks as needed.\n\n\n# Main configuration item\n\n[[Miners]]\n# `SP` actor id, required, numeric type\nActor = 10086\n\n\nIn addition to the main configuration, Miners also contains a number of different sub-configuration blocks, let\'s go through them one by one\n\n\n# [Miners.Sector]\n\nPolicy used to control sector allocation.\n\n[Miners.Sector]\n# Sector start number, optional, number type\n# Default value is 0\n# Deprecated\nInitNumber = 0\n\n# Minimum sector number, optional, number type\n# Default value is null\n# Compared with InitNumber, when this is set,\n# 1. At any time, the allocator will not give a sector number less than or equal to this value.\n# 2. The value of this item can be adjusted during cluster operation.\n# Increase the config value, the assignment result will always follow the description of 1).\n# Lowering the config value usually has no effect.\n#\n# When this item is not set, if InitNumber is a non-zero value, it is equivalent to this item.\n#MinNumber = 10\n\n# Sector number upper limit, optional, number type\n# The default value is null, which means no upper limit\n#MaxNumber = 1000000\n\n# Whether to allow allocation of sectors, optional, boolean type\n# The default value is true, that is, the allocation is enabled\n# If set to false, do not seal\n#Enabled = true\n\n# Whether to allow allocation of deals, optional, boolean\n# Default is false\n#EnableDeals = false\n\n# The life cycle of the CC sector, the unit is days, optional, number type\n# Default is 540\n#LifetimeDays = 540\n\n# Sector log verbosity of related modules, optional items, boolean type\n# The default value is false, which simplifies the log output\n#Verbose = false\n\n\n\n# [Miners.SnapUp]\n\nProduction strategy for controlling SnapDeal\n\n[Miners.SnapUp]\n# Whether to enable, optional, boolean type\n# Default is false\n#Enabled = false\n\n# Sender address, required if enabled, address type\n#Sender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# Whether to send the necessary funds from Sender when submitting the on-chain message, optional, boolean type\n# Default value is true\n# If set to false, founds from miner address\n#SendFund = true\n\n# Gas estimate multiplier for a single commit message, optional, floating point type\n# Default is 1.2\n#GasOverEstimation = 1.2\n\n# Gas premium multiplier for a single commit message, optional, floating point type\n# Default is 0.0, which means no effect\n#GasOverPremium = 0.0\n\n# FeeCap limit for a single message, optional, FIL value type\n# Default is 5 nanoFIL\n#GasFeeCap = "5 nanoFIL"\n\n# Deprecated\n#MaxFeeCap = ""\n\n# Whether to delete the original cc sector data after the snapdeal message is on the chain\n# Default is true\n#CleanupCCData = true\n\n# The confident height for message on-chain, optional, number type\n# Default is 15\n#MessageConfidence = 15\n\n# The confident height to release old data storage space, optional, number type\n# Default is 30\n#ReleaseConfidence = 30\n\n# SnapUp retry policy\n[Miners.SnapUp.Retry]\n\n# maximum number of retries, optional, number type\n# The default is NULL, which means no limit\n#MaxAttempts = 10\n\n# Status polling interval, optional, duration type\n# Default is 3min\n#PollInterval = "3m0s"\n\n# API interface exception retry interval, optional, duration type\n# Default is 3min\n#APIFailureWait = "3m0s"\n\n# Retry interval for local exceptions, such as local database exceptions, local storage exceptions, etc., optional, duration type\n# Default is 3min\n#LocalFailureWait = "3m0s"\n\n\n\n# [Miners.Commitment]\n\nCommon section for configuring PoRep message sending policies.\n\n[Miners.Commitment]\n# Height of the message that is considered stable, optional, number type\n# Default is 10\n#Confidence = 10\n\n\n\n# [Miners.Commitment.Pre]\n\nStrategy for configuring PreCommit message sending\n\n[Miners.Commitment.Pre]\n# Whether to use the necessary funds from Sender when sending the  message on-chain, optional, boolean type\n# Default value is true\n#SendFund = true\n\n# Sender address, required, address type\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# Gas estimate multiplier for a single message, optional, floating point type\n# Default is 1.2\n#GasOverEstimation = 1.2\n\n# Gas premium multiplier for a single commit message, optional, floating point type\n# Default is 0.0, which means no effect\n#GasOverPremium = 0.0\n\n# FeeCap limit for a single message, optional, FIL value type\n# Default is 5 nanoFIL\n#GasFeeCap = "5 nanoFIL"\n\n# Deprecated\n#MaxFeeCap = "5 nanoFIL"\n\n# Aggregate message sending configuration blocks\n[Miners.Commitment.Pre.Batch]\n# Whether to enable aggregate messages, optional, boolean type\n# The default value is false, i.e. not enabled\n#Enabled = false\n\n# Minimum number of message to aggregate, optional, number type\n# The default value is 16, that is, the minimum number of aggregates is 16\n#Threshold = 16\n\n# Maximum waiting time, optional, time type\n# The default value is 1h, that is, the maximum wait time is 1 hour\n#MaxWait = "1h0m0s"\n\n# Check interval, optional, time type\n# The default value is 1min, that is, every 1min to check whether the aggregation conditions are met\n#CheckInterval = "1m0s"\n\n# Gas estimation multiplier of aggregate messages, optional, floating point type\n# Default is 1.2\n#GasOverEstimation = 1.2\n\n# Gas premium multiplier for a single commit message, optional, floating point type\n# Default is 0.0, which means no effect\n#GasOverPremium = 0.0\n\n# FeeCap limit for a single message, optional, FIL value type\n# Default is 5 nanoFIL\n#GasFeeCap = "5 nanoFIL"\n\n# Deprecated\n#MaxFeeCap = "5 nanoFIL"\n\n\n\n# [Miners.Commitment.Prove]\n\nThe strategy used to configure ProveCommit message sending, its configuration items and functions are exactly the same as those in Miners.Commitment.Pre.\n\n\n# [Miners.Commitment.Terminate]\n\nThe strategy used to configure TerminateSectors message submission, its configuration items and functions are basically the same as those in Miners.Commitment.Pre. In practice, such messages are not sent as frequently. It is recommended to use single message sending mode. When using aggregate sending mode, Threshold is recommended to be configured with a smaller value to ensure that messages get on-chain in time.\n\n\n# [Miners.PoSt]\n\nOptions for configuring WindowPoSt.\n\n[Miners.Post]\n# Sender address, required, address type\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# Whether to enable, optional, boolean type\n# Default value is true\n#Enabled = true\n\n# Whether to perform strong verification on sector files, optional, boolean type\n# Default value is true\n# When enabled, in addition to checking the existence of the file, it will also try to read some information, such as metadata, etc.\n#StrictCheck = true\n\n# Switch for enable parallel proofs generations, optional, boolean type\n# Default value is false\n# When enabled, we will start generating partitions inside one deadline parallelly\n# BE NOTICED: this can take effect only when multiple ext-provers are set\n#Parallel = false\n\n# Gas estimation multiplier of WindowPoSt message, optional, floating point type\n# Default is 1.2\n#GasOverEstimation = 1.2\n\n# Gas premium multiplier for a single commit message, optional, floating point type\n# Default is 0.0, which means no effect\n#GasOverPremium = 0.0\n\n# FeeCap limit for a single message, optional, FIL value type\n# Default is 5 nanoFIL\n#GasFeeCap = "5 nanoFIL"\n\n# Deprecated\n#MaxFeeCap = "5 nanoFIL"\n\n# Height of the message that is considered stable, optional, number type\n# Default is 10\n#Confidence = 10\n\n# Stable height to submit WindowPoSt proofs, optional, number type\n# This value determines how many epochs to wait for the chain to enter a stable state, so that we could start submiting proofs\n# The first epoch we can submit is deadline.Open + SubmitConfidence\n# The smaller this value is set, the earlier it will start, but at the same time, more likely to send windowPost to a fork\n# When set to 0, the default value of 4 will be used\n#SubmitConfidence = 0\n\n# Stable height to start WindowPoSt, optional, number type\n# This value determines how many epochs to wait for the chain to enter a stable state, and the WindowPoSt task can be started\n# The first epoch we can start is deadline.Challenge + ChallengeConfidence\n# The smaller this value is set, the earlier it will start, but at the same time, more likely to send windowPost to a fork\n# When set to 0, the default value of 10 will be used\n#ChallengeConfidence = 0\n\n# The maximum limit of the sectors included in one recovering check, optional, number type\n# Default value is 0\n# When set to 0, no limit\n#MaxRecoverSectorLimit = 0\n\n# The maximum number of Partitions allowed in a single PoSt message, optional, number type\n# Default value is 0\n# When set to 0, the default maximum value will be used\n#MaxPartitionsPerPoStMessage = 0\n\n# The maximum number of Partitions allowed in a single Recover message, optional, number type\n# Default value is 0\n# When set to 0, no limit\n#MaxPartitionsPerRecoveryMessage = 0\n\n\n\n# [Miners.Proof]\n\nUsed to configure WinningPoSt Proof related policies\n\n[Miners.Proof]\n# Whether to enable, optional, boolean type\n# Default is false\n#Enabled = false\n\n\n\n# [Miners.Sealing]\n\nUsed to configure sealing related policies\n\n[Miners.Sealing]\n# Sealing need how many epochs to achieve，when select deals in sealing, deals start epoch will be\n# required to later than current-epoch + sealing-duration ，optional，integer\n# Default is zero, means no config\n#SealingEpochDuration = 0\n#\n# weather to use SyntheticPoRep , optional, boolean type\n# default is false\n#UseSyntheticPoRep = false\n\n\n\n# [Miners.Deal] Deprecated\n\nUsed to configure deal related policies.\n\n[Miners.Deal]\n# Whether to enable, optional, boolean type\n# Default is false\n#Enabled = false\n\n\n\n# A minimal working configuration file example\n\nLet\'s have a look at an example of starting a damocles-manager that could supports a SP\'s operation,\n\n[Common]\n[Common.API]\nGateway = ["/ip4/{api_host}/tcp/{api_port}"]\nToken = "{some token}"\nChain = "/ip4/{api_host}/tcp/{api_port}"\nMessager = "/ip4/{api_host}/tcp/{api_port}"\nMarket = "/ip4/{api_host}/tcp/{api_port}"\n\n[[Common.PieceStores]]\nPath = "{store_path}"\n\n[[Common.PersistStores]]\nName = "{store_name1}"\nPath = "{store_path1}"\n\n[[Common.PersistStores]]\nName = "{store_name2}"\nPath = "{store_path2}"\n\n[[Common.PersistStores]]\nName = "{store_name3}"\nPath = "{store_path3}"\n\n[[Common.PersistStores]]\nName = "{store_name4}"\nPath = "{store_path4}"\n\n[[Miners]]\nActor = 10086\n[Miners.Sector]\nInitNumber = 1000\nEnabled = true\nEnableDeals = true\n\n[Miners.Commitment]\n[Miners.Commitment.Pre]\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n[Miners.Commitment.Pre.Batch]\nEnabled = false\n\n[Miners.Commitment.Prove]\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n[Miners.Commitment.Prove.Batch]\nEnabled = true\n\n[Miners.Post]\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\nEnabled = true\n\n[Miners.Proof]\nEnabled = true\n\n\n\nThis activates an instance of damocles-manager that...\n\n * With 1 local PieceStore\n * With 4 local persistent stores\n * Enables sector allocation, which initial number is 1000\n * Disables aggregated PreCommit\n * Enables aggregated ProveCommit\n * Enables WinningPost module\n * Enables deal',normalizedContent:'# configuration of damocles-manager\n\ndamocles-manager is the main component for interacting with the chain and maintaining the sectors. let\'s take a look at its configuration file structure.\n\nafter initialization, we can get a copy of the default configuration:\n\n# default config:\n[common]\n[common.api]\n#gateway = ["/ip4/{api_host}/tcp/{api_port}"]\n#token = "{some token}"\n#chaineventinterval = "1m0s"\n#chain = "/ip4/{api_host}/tcp/{api_port}"\n#messager = "/ip4/{api_host}/tcp/{api_port}"\n#market = "/ip4/{api_host}/tcp/{api_port}"\n#\n[[common.piecestores]]\n#name = "{store_name}"\n#path = "{store_path}"\n#plugin = ""\n#pluginname = "s3store"\n[common.piecestores.meta]\n#somekey = "somevalue"\n#\n[[common.persiststores]]\n#name = "{store_name}"\n#path = "{store_path}"\n#strict = false\n#readonly = false\n#weight = 0\n#allowminers = [1, 2]\n#denyminers = [3, 4]\n#plugin = ""\n#pluginname = "s3store"\n[common.persiststores.meta]\n#somekey = "somevalue"\n[common.db]\n#driver = "badger"\n[common.db.badger]\n#basedir = ""\n[common.proving]\n#parallelchecklimit = 128\n#singlechecktimeout = "10m0s"\n#partitionchecktimeout = "20m0s"\n[common.proving.workerprover]\njobmaxtry = 2\nheartbeattimeout = "15s"\njoblifetime = "25h0m0s"\n\n[[miners]]\n#actor = 10086\n[miners.sector]\n#initnumber = 0\n#minnumber = 10\n#maxnumber = 1000000\n#enabled = true\n#enabledeals = false\n#lifetimedays = 540\n#verbose = false\n[miners.snapup]\n#enabled = false\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#sendfund = true\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n#cleanupccdata = true\n#messageconfidence = 15\n#releaseconfidence = 30\n[miners.snapup.retry]\n#maxattempts = 10\n#pollinterval = "3m0s"\n#apifailurewait = "3m0s"\n#localfailurewait = "3m0s"\n[miners.commitment]\n#confidence = 10\n[miners.commitment.pre]\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#sendfund = true\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.pre.batch]\n#enabled = false\n#threshold = 16\n#maxwait = "1h0m0s"\n#checkinterval = "1m0s"\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.prove]\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#sendfund = true\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.prove.batch]\n#enabled = false\n#threshold = 16\n#maxwait = "1h0m0s"\n#checkinterval = "1m0s"\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.terminate]\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#sendfund = true\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.terminate.batch]\n#enabled = false\n#threshold = 5\n#maxwait = "1h0m0s"\n#checkinterval = "1m0s"\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.post]\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#enabled = true\n#strictcheck = true\n#parallel = false\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n#confidence = 10\n#submitconfidence = 0\n#challengeconfidence = 0\n#maxrecoversectorlimit = 0\n#maxpartitionsperpostmessage = 0\n#maxpartitionsperrecoverymessage = 0\n[miners.proof]\n#enabled = false\n[miners.sealing]\n#sealingepochduration = 0\n#usesyntheticporep = false\n#\n\n\nwe will break down each configurable item one by one.\n\n\n# [common]\n\ncommon section includes common configuration, which is further divided into four sub-configuration items:\n\n\n# [common.api]\n\ncommon.api is interface related configuration, its content includes:\n\n[common.api]\n# gateway service infos, required, string type\n# fill in according to the actual situation of the service used\n# for each one contained, if the item is valid as a token-included-info-string ("{token}:{multiaddr}"), the token included would be used to construct the rpc client instead of the common token.\ngateway = ["/ip4/{api_host}/tcp/{api_port}"]\n\n# common token for services, required, string type\n# fill in according to the actual situation of the service used\ntoken = "{some token}"\n\n# chain service info, optional, string type\n# fill in according to the actual situation of the service used\n# if the field is valid as a token-included-info-string ("{token}:{multiaddr}"), the token included would be used to construct the rpc client instead of the common token.\n# if not set, use value of gateway as default \nchain = "/ip4/{api_host}/tcp/{api_port}"\n\n# message service info, optional, string type\n# fill in according to the actual situation of the service used\n# if the field is valid as a token-included-info-string ("{token}:{multiaddr}"), the token included would be used to construct the rpc client instead of the common token.\n# if not set, use value of gateway as default \nmessager = "/ip4/{api_host}/tcp/{api_port}"\n\n# market service info, optional, string type\n# fill in according to the actual situation of the service used\n# if the field is valid as a token-included-info-string ("{token}:{multiaddr}"), the token included would be used to construct the rpc client instead of the common token.\n# if not set, use value of gateway as default \nmarket = "/ip4/{api_host}/tcp/{api_port}"\n\n# interval time for detecting chain height changes, optional, duration type\n# default is 1min\n#chaineventinterval = "1m0s"\n\n\n\n# [common.plugins]\n\ncommon.plugins is used for configuring the plugin path of damocles-manager.\n\n[common.plugins]\n# path where plugins are stored, optional，string type\n# default is an empty string, which means that no plugin will be loaded.\n# it is recommended to use absolute paths.\ndir = ""\n\n\n\n# [[common.piecestores]]\n\ncommon.piecestores is used for configuring local deal piece data. when there is available offline deal, you can configure this item to avoid getting the deal piece data through public network traffic.\n\neach local store directory should correspond to a common.piecestores configuration block.\n\n# basic configuration example\n\n[[common.piecestores]]\n# name, optional, string type\n# the default is the absolute path corresponding to the path\n#name = "remote-store1"\n\n# path, required, string type\npath = "/mnt/mass/piece1"\n\n# plugin path, optional, string type\n# default is null\n# if you would like to use a custom storage scheme, you can write a golang plugin that meets the requirements and set it here.\n# deprecated, please use pluginname instead.\n#plugin = "path/to/objstore-plugin"\n\n# plugin path, optional, string type\n# default is empty string\n# if you would like to use a custom storage scheme, you can write a golang plugin that meets the requirements and set it here.\n#pluginname = "s3store"\n\n# meta information, optional items, dictionary type\n# the internal value is in the format of key = "value"\n# default value is null\n# used to support different types of storage schemes\n[common.piecestores.meta]\n#somekey = "somevalue"\n#\n\n\n\n# [[common.persiststores]]\n\ncommon.persiststores is used to configure sector persistent data stores. it corresponds to the attached concept in damocles-worker.\n\nsimilar to common.piecestores, each persistent store directory should correspond to a common.persiststores configuration block.\n\n# basic configuration example\n\n[[common.persiststores]]\n# name, optional, string type\n## default is the absolute path corresponding to the path\n#name = "remote-store1"\n\n# path, required, string type\n# it is recommended to use absolute paths\npath = "/mnt/remote/10.0.0.14/store"\n\n# read only, optional, boolean\n# default is false\n# from v0.4.0 and above, the persistent storage allocation logic goes to damocles-manager\n# this configuration can be used to set whether you can continue to write to the storage\n#readonly = false\n\n# optional, boolean\n# default is false\n# whether to validate if the path is a regular file. if set to true, an error will be raised if the path is a non-regular file, such as a symbolic link.\n#strict = false\n\n# weight, optional, number type\n# default is 1\n# when the filled value is 0, it is equivalent to 1\n# from v0.4.0 and above, the persistent storage allocation logic goes to damocles-manager\n# this configuration can be used to set the weight allocation ratio between multiple persistent stores\n# the probability of each persistent stores being selected is `weight / sum`, where `sum` is the sum of the weights of all available persistent stores.\n# example: configure 3 persistent stores, the weights are 2, 1, 1. the probability of being selected is 50%, 25%, 25% respectively\n#weight = 1\n\n# plugin path, optional, string type\n# default is null\n# if you want to use a custom storage scheme, you can write a golang plugin that meets the requirements and set it here.\n# deprecated, please use pluginname instead.\n#plugin = "path/to/objstore-plugin"\n\n# list of miner ids that are allowed, optional, numeric array type\n# default is null\n# when not set, it is regarded as allowing all miner ids; when set, it is equivalent to a whitelist, which allows only listed miner ids\n# if a miner id appears in allowminers and denyminers at the same time, denyminers will take effect first, which is considered blacklisted\n#allowminers = [1, 2]\n\n# list of miner ids that are being denied, optional, numeric array type\n# default is null\n# when not set, it is regarded as not rejecting any miner number; when set, it is equivalent to a blacklist, which will deny the listed miner id\n# if a miner id appears in both allowminers and denyminers, denyminers will take effect first, which is considered blacklisted\n#denyminers = [3, 4]\n\n# plugin path, optional, string type\n# default is empty string\n# if you would like to use a custom storage scheme, you can write a golang plugin that meets the requirements and set it here.\n#pluginname = "s3store"\n\n# meta information, optional items, dictionary type\n# the internal value is in the format of key = "value"\n# default value is null\n# used to support the preparation of different types of storage schemes, currently has no effect\n[common.persiststores.meta]\n#somekey = "somevalue"\n#\n\n\n\n# [common.mongokvstore] deprecated\n\ncommon.mongokvstore is used to configure whether damocles-manager use mongodb as kv database during sealing.\n\n# basic configuration example\n\n[common.mongokvstore]\n# switch of mongo kv, optional, boolean type\n# default is false \nenable = true\n# dsn of mongo, when `enable` is true, `dsn` is required, string type.\ndsn = "mongodb://127.0.0.1:27017/?directconnection=true&serverselectiontimeoutms=2000"\n# databasename of mongo, when `enable` is true, `databasename` is required, string type.\ndatabasename = "test"\n\n\n\n# [common.proving]\n\nproving used to control the number of proving parallels and check timeouts.\n\nexample:\n\n# maximum number of sector checks to run in parallel, optional, time type\n# default is 128. (0 = unlimited)\n# warning: setting this value too high may make the node crash by running out of stack\n# warning: setting this value too low may make sector challenge reading much slower, resulting in failed post due\n# to late submission.\n#parallelchecklimit = 128\n# maximum amount of time a proving pre-check can take for a sector, optional, time type\n# default is 10m0s.\n# if the check times out the sector will be skipped\n# warning: setting this value too low risks in sectors being skipped even though they are accessible, just reading the test challenge took longer than this timeout\n# warning: setting this value too high risks missing post deadline in case io operations related to this sector are blocked (e.g. in case of disconnected nfs mount)\n#singlechecktimeout = "10m0s"\n# maximum amount of time a proving pre-check can take for an entire partition, optional time type\n# default is 20m0s.\n# if the check times out, sectors in the partition which didn\'t get checked on time will be skipped\n# warning: setting this value too low risks in sectors being skipped even though they are accessible, just reading the test challenge took longer than this timeout\n# warning: setting this value too high risks missing post deadline in case io operations related to this partition are blocked or slow\n#partitionchecktimeout = "20m0s"\n\n\n\n# [common.proving.workerprover]\n\nused to configure the worker prover module\n\nexample:\n\n# the maximum number of attempts of the windowpost job, optional, number type\n# default is 2\n# job that exceeds the jobmaxtry number can only be re-executed by manual reset\njobmaxtry = 2\n# the timeout of the windowpost job\'s heartbeat, optional, time type\n# default is 15s\n# jobs that have not sent a heartbeat for more than this time will be set to fail and retried\nheartbeattimeout = "15s"\n# the heartbeat timeout of the windowpost job, optional, time type\n# default is 25h\n# windowpost jobs created longer than this time will be deleted\njoblifetime = "25h0m0s"\n\n\n\n# [common.db]\n\ncommon.db is used to configure kv database used by damocles-manager during sealing. currently, the badger local database and mongo database are supported.\n\n# basic configuration example:\n\n[common.db]\n# specify database, optional, string type\n# default is badger\n# all options: badger | mongo | plugin\ndriver = "badger"\n[common.db.badger]\n# basedir of the badger, optional, string type\n# default value is empty string\n# use home dir (default is ~/.damocles-manager) to store badger database files if basedir is empty string\n#basedir = ""\n[common.db.mongo]\n# dsn of mongo, when `enable` is true, `dsn` is required, string type.\ndsn = "mongodb://127.0.0.1:27017/?directconnection=true&serverselectiontimeoutms=2000"\n# databasename of mongo, when `enable` is true, `databasename` is required, string type.\ndatabasename = "test"\n[common.db.plugin]\n# the plugin name, optional，string type\n# default is null\npluginname = "redis"\n# metadata, optional, dict type\n# the internal value is in the format of key = "value"\n# default is null\n# the metadata will be pass to the constructor of the plugin\n[common.db.plugin.meta]\n#somekey = "somevalue"\n#\n\n\n\n# [[miners]]\n\nminers is an important configuration item, which is used to define behavior and policy for a certain sp.\n\ndamocles is designed to support multiple sps with the same set of components. this is reflected in damocles-manager, which you can set multiple miners configuration blocks as needed.\n\n\n# main configuration item\n\n[[miners]]\n# `sp` actor id, required, numeric type\nactor = 10086\n\n\nin addition to the main configuration, miners also contains a number of different sub-configuration blocks, let\'s go through them one by one\n\n\n# [miners.sector]\n\npolicy used to control sector allocation.\n\n[miners.sector]\n# sector start number, optional, number type\n# default value is 0\n# deprecated\ninitnumber = 0\n\n# minimum sector number, optional, number type\n# default value is null\n# compared with initnumber, when this is set,\n# 1. at any time, the allocator will not give a sector number less than or equal to this value.\n# 2. the value of this item can be adjusted during cluster operation.\n# increase the config value, the assignment result will always follow the description of 1).\n# lowering the config value usually has no effect.\n#\n# when this item is not set, if initnumber is a non-zero value, it is equivalent to this item.\n#minnumber = 10\n\n# sector number upper limit, optional, number type\n# the default value is null, which means no upper limit\n#maxnumber = 1000000\n\n# whether to allow allocation of sectors, optional, boolean type\n# the default value is true, that is, the allocation is enabled\n# if set to false, do not seal\n#enabled = true\n\n# whether to allow allocation of deals, optional, boolean\n# default is false\n#enabledeals = false\n\n# the life cycle of the cc sector, the unit is days, optional, number type\n# default is 540\n#lifetimedays = 540\n\n# sector log verbosity of related modules, optional items, boolean type\n# the default value is false, which simplifies the log output\n#verbose = false\n\n\n\n# [miners.snapup]\n\nproduction strategy for controlling snapdeal\n\n[miners.snapup]\n# whether to enable, optional, boolean type\n# default is false\n#enabled = false\n\n# sender address, required if enabled, address type\n#sender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# whether to send the necessary funds from sender when submitting the on-chain message, optional, boolean type\n# default value is true\n# if set to false, founds from miner address\n#sendfund = true\n\n# gas estimate multiplier for a single commit message, optional, floating point type\n# default is 1.2\n#gasoverestimation = 1.2\n\n# gas premium multiplier for a single commit message, optional, floating point type\n# default is 0.0, which means no effect\n#gasoverpremium = 0.0\n\n# feecap limit for a single message, optional, fil value type\n# default is 5 nanofil\n#gasfeecap = "5 nanofil"\n\n# deprecated\n#maxfeecap = ""\n\n# whether to delete the original cc sector data after the snapdeal message is on the chain\n# default is true\n#cleanupccdata = true\n\n# the confident height for message on-chain, optional, number type\n# default is 15\n#messageconfidence = 15\n\n# the confident height to release old data storage space, optional, number type\n# default is 30\n#releaseconfidence = 30\n\n# snapup retry policy\n[miners.snapup.retry]\n\n# maximum number of retries, optional, number type\n# the default is null, which means no limit\n#maxattempts = 10\n\n# status polling interval, optional, duration type\n# default is 3min\n#pollinterval = "3m0s"\n\n# api interface exception retry interval, optional, duration type\n# default is 3min\n#apifailurewait = "3m0s"\n\n# retry interval for local exceptions, such as local database exceptions, local storage exceptions, etc., optional, duration type\n# default is 3min\n#localfailurewait = "3m0s"\n\n\n\n# [miners.commitment]\n\ncommon section for configuring porep message sending policies.\n\n[miners.commitment]\n# height of the message that is considered stable, optional, number type\n# default is 10\n#confidence = 10\n\n\n\n# [miners.commitment.pre]\n\nstrategy for configuring precommit message sending\n\n[miners.commitment.pre]\n# whether to use the necessary funds from sender when sending the  message on-chain, optional, boolean type\n# default value is true\n#sendfund = true\n\n# sender address, required, address type\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# gas estimate multiplier for a single message, optional, floating point type\n# default is 1.2\n#gasoverestimation = 1.2\n\n# gas premium multiplier for a single commit message, optional, floating point type\n# default is 0.0, which means no effect\n#gasoverpremium = 0.0\n\n# feecap limit for a single message, optional, fil value type\n# default is 5 nanofil\n#gasfeecap = "5 nanofil"\n\n# deprecated\n#maxfeecap = "5 nanofil"\n\n# aggregate message sending configuration blocks\n[miners.commitment.pre.batch]\n# whether to enable aggregate messages, optional, boolean type\n# the default value is false, i.e. not enabled\n#enabled = false\n\n# minimum number of message to aggregate, optional, number type\n# the default value is 16, that is, the minimum number of aggregates is 16\n#threshold = 16\n\n# maximum waiting time, optional, time type\n# the default value is 1h, that is, the maximum wait time is 1 hour\n#maxwait = "1h0m0s"\n\n# check interval, optional, time type\n# the default value is 1min, that is, every 1min to check whether the aggregation conditions are met\n#checkinterval = "1m0s"\n\n# gas estimation multiplier of aggregate messages, optional, floating point type\n# default is 1.2\n#gasoverestimation = 1.2\n\n# gas premium multiplier for a single commit message, optional, floating point type\n# default is 0.0, which means no effect\n#gasoverpremium = 0.0\n\n# feecap limit for a single message, optional, fil value type\n# default is 5 nanofil\n#gasfeecap = "5 nanofil"\n\n# deprecated\n#maxfeecap = "5 nanofil"\n\n\n\n# [miners.commitment.prove]\n\nthe strategy used to configure provecommit message sending, its configuration items and functions are exactly the same as those in miners.commitment.pre.\n\n\n# [miners.commitment.terminate]\n\nthe strategy used to configure terminatesectors message submission, its configuration items and functions are basically the same as those in miners.commitment.pre. in practice, such messages are not sent as frequently. it is recommended to use single message sending mode. when using aggregate sending mode, threshold is recommended to be configured with a smaller value to ensure that messages get on-chain in time.\n\n\n# [miners.post]\n\noptions for configuring windowpost.\n\n[miners.post]\n# sender address, required, address type\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# whether to enable, optional, boolean type\n# default value is true\n#enabled = true\n\n# whether to perform strong verification on sector files, optional, boolean type\n# default value is true\n# when enabled, in addition to checking the existence of the file, it will also try to read some information, such as metadata, etc.\n#strictcheck = true\n\n# switch for enable parallel proofs generations, optional, boolean type\n# default value is false\n# when enabled, we will start generating partitions inside one deadline parallelly\n# be noticed: this can take effect only when multiple ext-provers are set\n#parallel = false\n\n# gas estimation multiplier of windowpost message, optional, floating point type\n# default is 1.2\n#gasoverestimation = 1.2\n\n# gas premium multiplier for a single commit message, optional, floating point type\n# default is 0.0, which means no effect\n#gasoverpremium = 0.0\n\n# feecap limit for a single message, optional, fil value type\n# default is 5 nanofil\n#gasfeecap = "5 nanofil"\n\n# deprecated\n#maxfeecap = "5 nanofil"\n\n# height of the message that is considered stable, optional, number type\n# default is 10\n#confidence = 10\n\n# stable height to submit windowpost proofs, optional, number type\n# this value determines how many epochs to wait for the chain to enter a stable state, so that we could start submiting proofs\n# the first epoch we can submit is deadline.open + submitconfidence\n# the smaller this value is set, the earlier it will start, but at the same time, more likely to send windowpost to a fork\n# when set to 0, the default value of 4 will be used\n#submitconfidence = 0\n\n# stable height to start windowpost, optional, number type\n# this value determines how many epochs to wait for the chain to enter a stable state, and the windowpost task can be started\n# the first epoch we can start is deadline.challenge + challengeconfidence\n# the smaller this value is set, the earlier it will start, but at the same time, more likely to send windowpost to a fork\n# when set to 0, the default value of 10 will be used\n#challengeconfidence = 0\n\n# the maximum limit of the sectors included in one recovering check, optional, number type\n# default value is 0\n# when set to 0, no limit\n#maxrecoversectorlimit = 0\n\n# the maximum number of partitions allowed in a single post message, optional, number type\n# default value is 0\n# when set to 0, the default maximum value will be used\n#maxpartitionsperpostmessage = 0\n\n# the maximum number of partitions allowed in a single recover message, optional, number type\n# default value is 0\n# when set to 0, no limit\n#maxpartitionsperrecoverymessage = 0\n\n\n\n# [miners.proof]\n\nused to configure winningpost proof related policies\n\n[miners.proof]\n# whether to enable, optional, boolean type\n# default is false\n#enabled = false\n\n\n\n# [miners.sealing]\n\nused to configure sealing related policies\n\n[miners.sealing]\n# sealing need how many epochs to achieve，when select deals in sealing, deals start epoch will be\n# required to later than current-epoch + sealing-duration ，optional，integer\n# default is zero, means no config\n#sealingepochduration = 0\n#\n# weather to use syntheticporep , optional, boolean type\n# default is false\n#usesyntheticporep = false\n\n\n\n# [miners.deal] deprecated\n\nused to configure deal related policies.\n\n[miners.deal]\n# whether to enable, optional, boolean type\n# default is false\n#enabled = false\n\n\n\n# a minimal working configuration file example\n\nlet\'s have a look at an example of starting a damocles-manager that could supports a sp\'s operation,\n\n[common]\n[common.api]\ngateway = ["/ip4/{api_host}/tcp/{api_port}"]\ntoken = "{some token}"\nchain = "/ip4/{api_host}/tcp/{api_port}"\nmessager = "/ip4/{api_host}/tcp/{api_port}"\nmarket = "/ip4/{api_host}/tcp/{api_port}"\n\n[[common.piecestores]]\npath = "{store_path}"\n\n[[common.persiststores]]\nname = "{store_name1}"\npath = "{store_path1}"\n\n[[common.persiststores]]\nname = "{store_name2}"\npath = "{store_path2}"\n\n[[common.persiststores]]\nname = "{store_name3}"\npath = "{store_path3}"\n\n[[common.persiststores]]\nname = "{store_name4}"\npath = "{store_path4}"\n\n[[miners]]\nactor = 10086\n[miners.sector]\ninitnumber = 1000\nenabled = true\nenabledeals = true\n\n[miners.commitment]\n[miners.commitment.pre]\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n[miners.commitment.pre.batch]\nenabled = false\n\n[miners.commitment.prove]\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n[miners.commitment.prove.batch]\nenabled = true\n\n[miners.post]\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\nenabled = true\n\n[miners.proof]\nenabled = true\n\n\n\nthis activates an instance of damocles-manager that...\n\n * with 1 local piecestore\n * with 4 local persistent stores\n * enables sector allocation, which initial number is 1000\n * disables aggregated precommit\n * enables aggregated provecommit\n * enables winningpost module\n * enables deal',charsets:{cjk:!0}},{title:"Configurations of damocles-worker",frontmatter:{},regularPath:"/operation/damocles-worker-config.html",relativePath:"operation/damocles-worker-config.md",key:"v-84157232",path:"/operation/damocles-worker-config.html",headers:[{level:2,title:"[worker]",slug:"worker",normalizedTitle:"[worker]",charIndex:441},{level:3,title:"Basic configuration example",slug:"basic-configuration-example",normalizedTitle:"basic configuration example",charIndex:3167},{level:2,title:"[metrics]",slug:"metrics",normalizedTitle:"[metrics]",charIndex:534},{level:3,title:"Basic configuration example",slug:"basic-configuration-example-2",normalizedTitle:"basic configuration example",charIndex:3167},{level:2,title:"[sector_manager]",slug:"sector-manager",normalizedTitle:"[sector_manager]",charIndex:591},{level:3,title:"Basic configuration example",slug:"basic-configuration-example-3",normalizedTitle:"basic configuration example",charIndex:3167},{level:2,title:"[sealing]",slug:"sealing",normalizedTitle:"[sealing]",charIndex:867},{level:3,title:"Basic configuration example",slug:"basic-configuration-example-4",normalizedTitle:"basic configuration example",charIndex:3167},{level:3,title:"Special configuration example",slug:"special-configuration-example",normalizedTitle:"special configuration example",charIndex:8966},{level:2,title:"[[sealing_thread]]",slug:"sealing-thread",normalizedTitle:"[[sealing_thread]]",charIndex:1166},{level:3,title:"Basic configuration example",slug:"basic-configuration-example-5",normalizedTitle:"basic configuration example",charIndex:3167},{level:3,title:"Special configuration example",slug:"special-configuration-example-2",normalizedTitle:"special configuration example",charIndex:8966},{level:3,title:"sealing_thread configuration hot reload",slug:"sealing-thread-configuration-hot-reload",normalizedTitle:"sealing_thread configuration hot reload",charIndex:11929},{level:2,title:"[[attached]]",slug:"attached",normalizedTitle:"[[attached]]",charIndex:1717},{level:3,title:"Basic configuration example",slug:"basic-configuration-example-7",normalizedTitle:"basic configuration example",charIndex:3167},{level:2,title:"[processors]",slug:"processors",normalizedTitle:"[processors]",charIndex:15136},{level:3,title:"[processors.limitation.concurrent]",slug:"processors-limitation-concurrent",normalizedTitle:"[processors.limitation.concurrent]",charIndex:1788},{level:3,title:"[processors.limitation.staggered]",slug:"processors-limitation-staggered",normalizedTitle:"[processors.limitation.staggered]",charIndex:1870},{level:3,title:"[processors.ext_locks]",slug:"processors-ext-locks",normalizedTitle:"[processors.ext_locks]",charIndex:1935},{level:3,title:"[processors.statictreed]",slug:"processors-static-tree-d",normalizedTitle:"[processors.statictreed]",charIndex:null},{level:3,title:"[[processors.{stage_name}]]",slug:"processors-stage-name",normalizedTitle:"[[processors.{stage_name}]]",charIndex:17468},{level:2,title:"A minimal working configuration file example",slug:"a-minimal-working-configuration-file-example",normalizedTitle:"a minimal working configuration file example",charIndex:28683}],headersStr:"[worker] Basic configuration example [metrics] Basic configuration example [sector_manager] Basic configuration example [sealing] Basic configuration example Special configuration example [[sealing_thread]] Basic configuration example Special configuration example sealing_thread configuration hot reload [[attached]] Basic configuration example [processors] [processors.limitation.concurrent] [processors.limitation.staggered] [processors.ext_locks] [processors.statictreed] [[processors.{stage_name}]] A minimal working configuration file example",content:'# Configurations of damocles-worker\n\ndamocles-worker is the main execution body of data sealing. Let\'s take a look at its configuration file structure and configuration options.\n\nThe configuration file of damocles-worker is in toml format. It should be noted that in this format, lines starting with # will be regarded as comments and will not take effect.\n\nTaking a mock instance as an example, a basic configuration might look like this:\n\n[worker]\n# name = "worker-#1"\n# rpc_server.host = "192.168.1.100"\n# rpc_server.port = 17891\n\n[metrics]\n#enable = false\n#http_listen = "0.0.0.0:9000"\n\n[sector_manager]\nrpc_client.addr = "/ip4/127.0.0.1/tcp/1789"\n# rpc_client.headers = { User-Agent = "jsonrpc-core-client" }\n# piece_token = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiMS0xMjUiLCJwZXJtIjoic2lnbiIsImV4dCI6IiJ9.JenwgK0JZcxFDin3cyhBUN41VXNvYw-_0UUT2ZOohM0"\n\n[sealing]\n# allowed_miners = [10123, 10124, 10125]\n# allowed_sizes = ["32GiB", "64GiB"]\nenable_deals = true\n# disable_cc = true\n# max_deals = 3\n# min_deal_space = "8GiB"\nmax_retries = 3\n# seal_interval = "30s"\n# recover_interval = "60s"\n# rpc_polling_interval = "180s"\n# ignore_proof_check = false\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store1"\n# plan = "snapup"\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32GiB", "64GiB"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_deals = 3\n# sealing.min_deal_space = "8GiB"\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store2"\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store3"\n\n[[attached]]\n# name = "persist-store1"\nlocation = "./mock-tmp/remote"\n\n[processors.limitation.concurrent]\n# add_pieces = 5\n# pc1 = 3\n# pc2 = 1\n# c2 = 1\n\n[processors.limitation.staggered]\n# pc1 = "5min"\n# pc2 = "4min"\n\n[processors.ext_locks]\n# gpu1 = 1\n\n[processors.static_tree_d]\n# 2KiB = "./tmp/2k/sc-02-data-tree-d.dat"\n\n# fields for the add_pieces processor\n# [[processors.add_pieces]]\n\n# fields for tree_d processor\n[[processors.tree_d]]\n# auto_restart = true\n# inherit_envs = true\n\n# fields for pc1 processors\n[[processors.pc1]]\n# bin = "./dist/bin/damocles-worker-plugin-pc1"\n# args = ["--args-1", "1", --"args-2", "2"]\nnuma_preferred = 0\ncgroup.cpuset = "4-5"\nenvs = { RUST_LOG = "info" }\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "6-7"\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-13"\n# auto_restart = true\n# inherit_envs = true\n\n# fields for pc2 processors\n[[processors.pc2]]\n# cgroup.cpuset = "24-27"\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc2]]\ncgroup.cpuset = "28-31"\n# auto_restart = true\n# inherit_envs = true\n\n# fields for c2 processor\n[[processors.c2]]\ncgroup.cpuset = "32-47"\n# auto_restart = true\n# inherit_envs = true\n\n\nBelow we will break down the configurable items one by one.\n\n\n# [worker]\n\nThe worker configuration item is used to configure some basic information of this worker instance.\n\n\n# Basic configuration example\n\n[worker]\n# instance name, optional, string type\n# By default, the IP address of the network card used to connect to `damocles-manager` is used as the instance name\n# name = "worker-#1"\n\n# rpc service listening address, optional, string type\n# The default is "0.0.0.0", that is, listening to all addresses of the machine\n# rpc_server.host = "192.168.1.100"\n\n# rpc service listening port, optional, number type\n# Default is 17890\n# rpc_server.port = 17891\n\n# local pieces file directory, optional, string array type\n# if set, worker will load the piece file from the local file\n# otherwise it will load the remote piece file from damocles-manager\n# If the file "/path/to/{your_local_pieces_dir01, your_local_pieces_dir02, ...}/piece_file_name" does not exist, the worker will also load it from the damocles-manager\n# local_pieces_dirs = ["/path/to/your_local_pieces_dir01", "/path/to/your_local_pieces_dir02"]\n\n\nIn most cases, each field in this configuration item does not need to be manually configured.\n\nOnly in some special cases, such as:\n\n * If you would like to name each damocles-worker instance according to your own naming schemes\n * If you don\'t want to monitor all network card IPs and only allow local rpc requests\n * If multiple damocles-workers are deployed on one machine, you would like to avoid port conflicts so that they are distinguished\n * If damocles-worker can access the piece_store directory, you can set local_pieces_dir to load piece file locally\n\nAnd in other scenarios, you would need to manually configure the options here as needed.\n\n\n# [metrics]\n\nmetrics provides metrics recorder & exporter (prometheus) options.\n\n\n# Basic configuration example\n\n[metrics]\n# Switch of the prometheus exporter, optional，boolean\n# Default value: false\n# When enabled，field "http_listen" will be used to bootstrap a prometheus exporter\n#enable = false\n\n# prometheus exporter listen address, optional, socket address\n# Default value: "0.0.0.0:9000"\n#http_listen = "0.0.0.0:9000"\n\n\n\n# [sector_manager]\n\nsector_manager is used to configure damocles-manager related information so that damocles-worker can connect to the corresponding service correctly.\n\n\n# Basic configuration example\n\n[sector_manager]\n# The connection address used when constructing the rpc client, required, string type\n# Accepts both `multiaddr` format and url format such as `http://127.0.0.1:1789`, `ws://127.0.0.1:1789`\n# Normally, use the `multiaddr` format for consistency with other components\nrpc_client.addr = "/ip4/127.0.0.1/tcp/1789"\n\n# The http header information when constructing the rpc client, optional, dictionary type\n# default is null\n# rpc_client.headers = { User-Agent = "jsonrpc-core-client" }\n\n# The verification token carried when requesting deal piece data, optional, string type\n# default is null\n# This is usually set when this instance allows sealing of sectors with deal data\n# The value of this config item is usually the token value of the Venus service used\n# piece_token = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiMS0xMjUiLCJwZXJtIjoic2lnbiIsImV4dCI6IiJ9.JenwgK0JZcxFDin3cyhBUN41VXNvYw-_0UUT2ZOohM0"\n\n\n\n# [sealing]\n\nsealing is used to configure common parameter options in the sealing process.\n\n\n# Basic configuration example\n\n[sealing]\n# Allowed `SP`, optional, number array format\n# Defaults to null, allows tasks from any `SP`\n# After configuration, only sealing tasks from `SP` listed in the array can be carried out\n# allowed_miners = [10123, 10124, 10125]\n\n# Allowed sector size, optional, string array format\n# Defaults to null, allows sector tasks of any size\n# After configuration, only tasks that match the sector size listed in the array can be executed\n# allowed_sizes = ["32GiB", "64GiB"]\n\n# Whether to allow adding deals to the sector, optional, boolean type\n# Default is false\n# When set to true, the `piece_token` item in `sector_manager` usually needs to be set at the same time\n# enable_deals = true\n\n# Whether to disable cc sector, optional, boolean type\n# Default is false\n# When enable_deals is true, turn on this option, will keep waiting until a storage deal is obtained, instead of starting sealing cc sectors\n# disable_cc = true\n\n# The maximum number of deals allowed to be added to the sector, optional, number type\n# default is null\n# max_deals = 3\n\n# The minimum size of a deal to be filled in a sector, optional, byte string format\n# default is null\n# min_deal_space = "8GiB"\n\n# Number of retries when an error of `temp` type is encountered during the sealing process, optional, number format\n# default is 5\n# max_retries = 3\n\n# Retry interval when a `temp` type error is encountered during the sealing process, optional, time string format\n# The default is "30s", which means 30 seconds\n# recover_interval = "30s"\n\n# Interval for idle `sealing_thread` to apply for new sealing tasks, optional, time string format\n# The default is "30s", which means 30 seconds\n# seal_interval = "30s"\n\n# rpc status polling request interval, optional, time string format\n# The default is "180s", which means 180 seconds\n# During the sealing process, some links use the polling method to obtain non-real-time information, such as if a message is on-chain.\n# This value helps to avoid overly frequent requests consuming network resources\n# rpc_polling_interval = "180s"\n\n# Whether to skip the local verification step of proof, optional, boolean format\n# Default is false\n# Usually only set this during testing\n# ignore_proof_check = false\n\n# Number of retries when unable to request a task from `sector_manager`, optional, number format\n# default is 3\n# request_task_max_retries = 3\n\n\nThe configuration items in sealing usually have default items preset from experiences, which removes the need for manual configurations in most cases.\n\n\n# Special configuration example\n\n# 1. Test network, serving only specific SP\n\nallowed_miners = [2234, 2236, 2238]\n\n\n# 2. Large-scale clusters to reduce network usage\n\n# Among the recoverable exceptions, a considerable part is caused by network jitter, increase the interval of automatic recovery and reduce the frequency of requests\nrecover_interval = "90s"\n\n# Increases the interval time to reduces the request frequency\nrpc_polling_interval = "300s"\n\n\n# 3. Increase the possibility of self-recovery from errors during sealing tasks\n\n# Increase the number of autorecovery attempts\nmax_retries = 10\n\n# Increase the auto-recovery interval\nrecover_interval = "60s"\n\n\n\n# [[sealing_thread]]\n\nEach sealing_thread is used to configure a worker thread for a sector. Multiple sealing_thread configuration groups can exist in one configuration file.\n\nThe sealing_thread will inherit the configuration in [sealing]. Any sealing.* in the sealing_thread configuration will override the configuration of [sealing].\n\n\n# Basic configuration example\n\n[[sealing_thread]]\n# Sector data directory path, required, string type\n# It is recommended to use an absolute path, the data directory and the worker thread are binded in one-to-one relationship\nlocation = "/mnt/nvme1/store"\n\n# task type, optional, string type\n# Default value is null\n# All options: sealer | snapup | rebuild | unseal | wdpost, when left blank, the default is `sealer`\n# plan = "snapup"\n\n# Custom parameters of the sealing process, only applies to the current worker thread\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32GiB", "64GiB"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n# sealing.request_task_max_retries = 3\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\n\n\nThe number of sealing_thread and the corresponding sealing data directory paths need to be planned according to your specific situation.\n\nIn order to facilitate combination and matching, each sealing_thread can be configured with an independent sealing sub-item, which satisfies:\n\n * The naming, type and effect of configurable items are consistent with the common sealing items\n * Only applies to the current worker thread\n * When not configured use the value in the common sealing item\n\n\n# Special configuration example\n\n# 1. Two worker threads, each serving a different SP\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\nsealing.allowed_miners = [1357]\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\nsealing.allowed_miners = [2468]\n\n\n# 2. Two worker threads, each with different sector sizes\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\nsealing.allowed_sizes = ["32GiB"]\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\nsealing.allowed_sizes = ["64GiB"]\n\n\n\n# sealing_thread configuration hot reload\n\nBefore damocles v0.5.0, we could only update the configuration by modifying the sealing_thread configuration and restarting damocles-worker. It is very inconvenient in some scenarios, for example: during Sector rebuild we would like to be able to reload configuration without rebooting damocles-worker after modifying the plan configuration item of a sealing_thread.\n\nsealing_thread configuration hot reload is supported after v0.5.0. By creating a hot reload configuration file named config.toml under the location directory in a sealing_thread with definitions of the configuration file exactly the same as the content of [[sealing_thread]], the configuration items in this configuration file will override the corresponding [[sealing_thread]] configuration items in damocles-worker, and modifying this configuration file does not require restarting damocles-worker to take effect.\n\nThe sealing_thread in damocles-worker will check the config.toml file in the location directory before starting a new sector task. A reload or removal of the configuration will be triggered, if the content of the config.toml file is either changed or the file is deleted.\n\nNotice:\n\n * Hot reload of configuration file config.toml cannot override the location configuration item in sealing_thread.\n * damocles-worker main configuration file does not support hot reload.\n\n# Basic configuration example\n\n# /path/to/the_sealing_thread_location/config.toml\n\n# Task type, optional, string type\n# Default value is null\n# All options: sealer | snapup | rebuild | unseal | wdpost, if not filled, the default is sealer\n# plan = "rebuild"\n\n# Custom parameters of the sealing process, only effective on the current worker thread\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32GiB", "64GiB"]\n# sealing.enable_deals = true\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n# sealing.request_task_max_retries = 3\n\n\n\n# [[attached]]\n\nattached is used to configure where the sealed sector persistent data is saved. Multiple configuration entries are allowed at the same time.\n\n\n# Basic configuration example\n\n[attached]\n# name, optional, string type\n# The default is the absolute path corresponding to the location\n# name = "remote-store1"\n\n# path, required, string type\n# It is recommended to fill in the absolute path directly\nlocation = "/mnt/remote/10.0.0.14/store"\n\n# read only, optional, boolean\n# Default is false\n# readonly = true\n\n\n\nWith the need to coordinate storage location information between damocles-worker and damocles-manager in mind and the fact that in many cases the same persistent storage directory mount path on the damocles-worker machine and on the damocles-manager machine are not exactly the same, so we decided to use name as the basis for coordination.\n\nYou can also choose not to configure name on both sides of damocles-worker and damocles-manager during configuration if the mount path of the persistent storage directory is the same on all machines. In this case, both will use the absolute path as the name.\n\n\n# [processors]\n\nprocessors is used to configure sealing task executors, and some other information during sealing.\n\nThis configuration item is actually divided into three sub-items, and we shall analyze them one by one.\n\n\n# [processors.limitation.concurrent]\n\nWhen both [processors.limit] and [processors.limitation.concurrent] exist in the configuration file, the latter shall override.\n\nprocessors.limitation.concurrent is used to configure the number of parallel tasks for a specific sealing task. This is to reduce the contention of hardware resources at one given sealing stage.\n\nIt should be noted that when external processors are configured, the number of external processors and the total allowed concurrency will also affect the number of parallel tasks.\n\n# Basic configuration example\n\n[processors.limitation.concurrent]\n# Consurrency limit for add_pieces task, optional, number_type\n# add_pieces = 5\n\n# Consurrency limit for tree_d task, optional, number type\n# tree_d = 1\n\n# Concurrency limit for the pc1 task, optional, number type\n# pc1 = 3\n\n# Concurrency limit of the pc2 task, optional, number type\n# pc2 = 2\n\n# Concurrency limit for task c2, optional, number type\n# c2 = 1\n\n\nFor example, if pc2 = 2 is set, then at most two sectors can perform pc2 task at the same time.\n\n\n# [processors.limitation.staggered]\n\nprocessors.limitation.staggered is used to configure the time interval for staggered startup of parallel tasks during a given sealing phase. After configuring this item, when multiple tasks are scheduled at the same time for a given sealing phase, damocles-worker will start the tasks in sequence according to the configured time interval, so as to avoid the problem of resource shortage such as slowdown of disk IO caused by the simultaneous start of tasks.\n\n# Basic configuration example\n\n[processors.limitation.staggered]\n# The time interval for starting multiple pc1 tasks in sequence, optional, string type (e.g. "1s", "2min")\n# pc1 = "5min"\n# pc2 = "4min"\n\n\nFor example, if pc1 = "5min" is set, when two pc1 tasks are scheduled at the same time, the second task will be executed after 5 minutes into the execution of the first task.\n\n\n# [processors.ext_locks]\n\nprocessors.ext_locks is used to configure some custom lock restrictions, which is used in conjunction with the locks configuration item in [[processors.{stage_name}]]. This configuration item allows users to customize some restrictions and make different external processors subject to them.\n\n# Basic configuration example\n\n[processors.ext_locks]\n# some_name = some_number\n\n\n# Special configuration example\n\nprocessors.ext_locks by itself does not apply the lock.\n\n# one GPU, shared by pc2 and c2\n\n[processors.ext_locks]\ngpu=1\n\n[[processors.pc2]]\nlocks = ["gpu"]\n\n[[processors.c2]]\nlocks = ["gpu"]\n\n\nBy this way, pc2 c2 will each start an external processor, and the two will compete for the lock, which means that the two tasks will not happen at the same time.\n\n# Two GPU, shared by pc2 and c2\n\n[processors.ext_locks]\ngpu1 = 1\ngpu2 = 1\n\n[[processors.pc2]]\nlocks = ["gpu1"]\n\n[[processors.pc2]]\nlocks = ["gpu2"]\n\n[[processors.c2]]\nlocks = ["gpu1"]\n\n[[processors.c2]]\nlocks = ["gpu2"]\n\n\nIn this way, pc2 c2 will each start two external processors, which will create a two-two competition relationship, which allows to limit a GPU to only execute one of the sealing tasks.\n\n\n# [processors.static_tree_d]\n\nprocessors.static_tree_d is a configuration item introduced to improve the efficiency of cc sector.\n\nWhen a static file path is configured for the corresponding sector size, this file will be used directly as the tree_d file for cc sector without trying to generate it again.\n\n# Basic configuration example\n\n[processors.static_tree_d]\n2KiB = "/var/tmp/2k/sc-02-data-tree-d.dat"\n32GiB = "/var/tmp/32g/sc-02-data-tree-d.dat"\n64GiB = "/var/tmp/64g/sc-02-data-tree-d.dat"\n\n\n\n# [[processors.{stage_name}]]\n\nThis is the configuration group used to configure external processors.\n\nCurrently {stage_name} could be one of the following.\n\n * add_pieces for add pieces phase\n * tree_d for generation phase of Tree D\n * pc1 for the PreCommit1 phase\n * pc2 for the PreCommit2 phase\n * synth_proof for the synthetic proof phase\n * c2: for Commit2 phase\n * unseal: for unseal phase\n * transfer: used to customize the transfer method between local data and persistent data storage\n\nEach such configuration group translates to an external processor of the corresponding sealing phase to be started. If nothing is configured for one of the above {stage_name} and corresponding [[processors.{stage_name}]] line does not exist in the configuration file, then damocles-worker will not start a child process for this {stage_name}. damocles-worker will use the built-in processor code to directly execute the corresponding {stage_name} task in sealing_thread, which means that the concurrent number of {stage_name} tasks depends on the corresponding number of sealing_thread and concurrent number of {stage_name} as configured in [processors.limitation.concurrent]. By not configuring external processor saves extra steps such as serializing task parameters and task output, but one loses capabilities such as more powerful concurrency control, cgroup control, and custom proof algorithms. You are feel to choose according to your own needs.\n\nThe optional configuration items for [[processors.{stage_name}]] include:\n\n[[processors.pc1]]\n# Custom external processor executable file path, optional, string type\n# By default, the executable file path corresponding to the main process will be used, Execute the damocles-worker builtin processor\n# bin = "./dist/bin/damocles-worker-plugin-pc1"\n\n# Customize the parameters of the external processor, optional item, string array type\n# The default value is null, `damocles-worker`\'s own executor default parameters will be used\n# args = ["--args-1", "1", --"args-2", "2"]\n\n# Time to get ready of external child-processes, optional, time string\n# The default value is 5s\n# stable_wait = "5s"\n\n# numa affinity partition id, optional, number type\n# Default value is null, no affinity will be set\n# Need to fill in according to the host\'s numa partition\n# numa_preferred = 0\n\n# cpu core binding and limit options, optional, string type\n# The default value is null, no binding is set\n# The format of the value follows the standard cgroup.cpuset format\n# cgroup.cpuset = "4-5"\n\n# Additional environment variables for external processors, optional, dictionary type\n# Default value is null\n# envs = { RUST_LOG = "info" }\n\n# The maximum number of concurrent tasks allowed by this executor\n# The default value is null, unlimited, but whether the task is executed concurrently depends on the implementation of the external processor used\n# Mainly used in pc1 so that multiple parallel links can be used, which can effectively save resources such as shared memory and thread pools\n# concurrent = 4\n\n# Custom external limit lock name, optional, string array type\n# Default value is null\n# locks = ["gpu1"]\n\n# Whether to restart automatically when a child process exits, optional, bool type\n# Default is true\n# auto_restart = true\n\n# Whether to inherit the environment variables from the worker daemon process, optional, bool type\n# Default is true\n# inherit_envs = true\n\n\n# Basic configuration example\n\n[processors.limit]\nadd_pieces = 8\npc1 = 4\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-7"\nconcurrent = 2\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\nauto_restart = true\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-19"\nconcurrent = 2\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\nauto_restart = true\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }\nauto_restart = true\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-39"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "1" }\nauto_restart = true\n\n\n[[processors.c2]]\ncgroup.cpuset = "28-35"\nenvs = { CUDA_VISIBLE_DEVICES = "2,3" }\nauto_restart = true\n\n\n[[processors.tree_d]]\ncgroup.cpuset = "40-45"\nauto_restart = true\n\n\nThe above is an example processors.{stage_name} configuration for a 48C + 4GPU box. Under this configuration, it will start:\n\n * 2 pc1 external processors, both using MULTICORE_SDR mode, each with 8 cores, allowing 2 concurrent tasks, and memory allocation preferentially uses its configured numa partition\n * 2 pc2 external processors, each with 8 cores, each using one GPU\n * 1 c2 external processor, allocated 8 cores, using one GPU\n * 1 tree_d external processor with 6 cores allocated\n\n# Special configuration example\n\n# 1. Using user-defined closed source, algorithmically optimized c2 external processor\n\n[[processors.c2]]\nbin = "/usr/local/bin/damocles-worker-c2-optimized"\ncgroup.cpuset = "40-47"\nenvs = { CUDA_VISIBLE_DEVICES = "2,3" }\n\n\n# 2. c2 external processor using outsourced mode (remote computation)\n\n[[processors.c2]]\nbin = "/usr/local/bin/damocles-worker-c2-outsource"\nargs = ["--url", "/ip4/apis.filecoin.io/tcp/10086/https", "--timeout", "10s"]\nenvs = { LICENCE_PATH = "/var/tmp/c2.licence.dev" }\n\n\n# 3. Use CPU mode to make up for pc2 computing power when GPU is lacking\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-45"\n\n\n# 4. Under the optimal planning, the total amount of pc1 is odd and cannot be divided equally\n\n[processors.limitation.concurrent]\npc1 = 29\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-41"\nconcurrent = 14\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-92"\nconcurrent = 15\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n\n# 5. To prioritize numa 0 area for pc1\n\n[processors.limit]\npc1 = 29\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-47"\nconcurrent = 16\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-86"\nconcurrent = 13\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n\n# cgroup.cpuset Configuration Notes\n\n * For PC1 external processors with multicore SDR enabled, cgroup.cpuset should use the entirety of the CPU cores under one L3 cache as a base unit for configuration as much as possible. If you need to configure portion of CPU cores under one L3 cache, you must make sure that the number of CPU cores under each L3 cache is the same.\n   \n   According to rust-fil-proofs, when multicore sdr is enabled, the number of CPU cores and number of corresponding cores on single DIE must be an integer multiple relationship. If the external processor is using rust-fil-proofs or rust-fil-proofs based variants, this rule must be followed, otherwise it can be ignored. The default processor of damocles-worker is based on rust-fil-proofs.\n\n * The CPU cores in cgroup.cpuset should not span NUMA nodes as much as possible. Across NUMA nodes will make CPU access to memory slower. (damocles supports loading NUMA affinity hugepage memory files after v0.5.0, if this feature is enabled then you can allocate cpusets across NUMA nodes without concerns)\n   \n   If the configured CPU cores are on the same NUMA node, processors.{stage_name}.numa_preferred can be configured with the corresponding NUMA node id.\n\nUse damocles-worker-util to check CPU information\n\n./damocles-worker-util hwinfo\n\n\nOutput:\n\nCPU topology:\nMachine (503.55 GiB)\n├── Package (251.57 GiB) (*** *** *** 32-Core Processor)\n│ ├── NUMANode (#0 251.57 GiB)\n│ ├── L3 (#0 16 MiB)\n│ │ └── PU #0 + PU #1 + PU #2 + PU #3\n│ ├── L3 (#1 16 MiB)\n│ │ └── PU #4 + PU #5 + PU #6 + PU #7\n│ ├── L3 (#2 16 MiB)\n│ │ └── PU #8 + PU #9 + PU #10 + PU #11\n│ ├── L3 (#3 16 MiB)\n│ │ └── PU #12 + PU #13 + PU #14 + PU #15\n│ ├── L3 (#4 16 MiB)\n│ │ └── PU #16 + PU #17 + PU #18 + PU #19\n│ ├── L3 (#5 16 MiB)\n│ │ └── PU #20 + PU #21 + PU #22 + PU #23\n│ ├── L3 (#6 16 MiB)\n│ │ └── PU #24 + PU #25 + PU #26 + PU #27\n│ └── L3 (#7 16 MiB)\n│ └── PU #28 + PU #29 + PU #30 + PU #31\n└── Package (251.98 GiB) (*** *** *** 32-Core Processor)\n    ├── NUMANode (#1 251.98 GiB)\n    ├── L3 (#8 16 MiB)\n    │ └── PU #32 + PU #33 + PU #34 + PU #35\n    ├── L3 (#9 16 MiB)\n    │ └── PU #36 + PU #37 + PU #38 + PU #39\n    ├── L3 (#10 16 MiB)\n    │ └── PU #40 + PU #41 + PU #42 + PU #43\n    ├── L3 (#11 16 MiB)\n    │ └── PU #44 + PU #45 + PU #46 + PU #47\n    ├── L3 (#12 16 MiB)\n    │ └── PU #48 + PU #49 + PU #50 + PU #51\n    ├── L3 (#13 16 MiB)\n    │ └── PU #52 + PU #53 + PU #54 + PU #55\n    ├── L3 (#14 16 MiB)\n    │ └── PU #56 + PU #57 + PU #58 + PU #59\n    └── L3 (#15 16 MiB)\n        └── PU #60 + PU #61 + PU #62 + PU #63\n...\n\n\nFrom the output information, we can see that this machine has two NUMANodes, each NUMANode has 8 L3 Caches, and each L3 cache has 4 CPU cores.\n\n * CPU cores on NUMANode #0: 0-31\n * CPU cores on NUMANode #1: 31-63\n\nUsing this machine as an example, configuration like processors.pc1.cgroup.cpuset = "0-6" does not satisfy the rust-fil-proofs rule.\n\n 1. PU#0, PU#1, PU#2, PU#3, these 4 CPU cores belong to L3#0\n 2. PU#4, PU#5, PU#6, these 3 CPU cores belong to L3#1\n\nThe number of shared caches of is 2 (L3#0, L3#1), and the number of configured CPU cores is inconsistent, which does not meet the rust-fil-proofs rule and cannot be started. The correct configuration could be processors.pc1.cgroup.cpuset = "0-7".\n\n\n# A minimal working configuration file example\n\n[sector_manager]\nrpc_client.addr = "/ip4/{some_ip}/tcp/1789"\n\n# According to actual resource planning\n[[sealing_thread]]\nlocation = "{path to sealing store1}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store2}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store3}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store4}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store5}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store6}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store7}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store8}"\n\n\n[remote_store]\nname = "{remote store name}"\nlocation = "{path to remote store}"\n\n[processors.static_tree_d]\n32GiB = "{path to static tree_d for 32GiB}"\n64GiB = "{path to static tree_d for 64GiB}"\n\n# According to actual resource planning\n[processors.limit]\npc1 = 4\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-7"\nconcurrent = 2\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-19"\nconcurrent = 2\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-39"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "1" }\n\n\n[[processors.c2]]\ncgroup.cpuset = "28-35"\nenvs = { CUDA_VISIBLE_DEVICES = "2,3" }\n\n\n[[processors.tree_d]]\ncgroup.cpuset = "40-45"\n\n\nAfter planning according to the actual situation and filling in the corresponding information, the above is a minimal working configuration file that...\n\n * Does cc sector only\n * Optimized tree_d generation for 32GiB and 64GiB sectors\n * Integrated resource allocation\n\n# References to the Venus community test cases\n\nReference Example 1 Takeaways: PC1 is precisely limited to cores, and C2 is completed by gpuproxy, which has strong scalability. The disadvantage is that the configuration is complex, and the number of tasks needs to be adjusted according to the actual hardware specs\n\nReference Example 2 Takeaways: PC2 and C2 share 1 GPU, may cause some C2 tasks be backlogged\n\nReference Example 3 Takeaways: 2 sets of PC2 share GPU resources with 2 sets of C2 respectively\n\nReference Example 4 Takeaways: Suitable for lower end machines, creates 96G of swap space on NVMe, but this may cause some tasks to be slower',normalizedContent:'# configurations of damocles-worker\n\ndamocles-worker is the main execution body of data sealing. let\'s take a look at its configuration file structure and configuration options.\n\nthe configuration file of damocles-worker is in toml format. it should be noted that in this format, lines starting with # will be regarded as comments and will not take effect.\n\ntaking a mock instance as an example, a basic configuration might look like this:\n\n[worker]\n# name = "worker-#1"\n# rpc_server.host = "192.168.1.100"\n# rpc_server.port = 17891\n\n[metrics]\n#enable = false\n#http_listen = "0.0.0.0:9000"\n\n[sector_manager]\nrpc_client.addr = "/ip4/127.0.0.1/tcp/1789"\n# rpc_client.headers = { user-agent = "jsonrpc-core-client" }\n# piece_token = "eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyjuyw1lijoims0xmjuilcjwzxjtijoic2lnbiisimv4dci6iij9.jenwgk0jzcxfdin3cyhbun41vxnvyw-_0uut2zoohm0"\n\n[sealing]\n# allowed_miners = [10123, 10124, 10125]\n# allowed_sizes = ["32gib", "64gib"]\nenable_deals = true\n# disable_cc = true\n# max_deals = 3\n# min_deal_space = "8gib"\nmax_retries = 3\n# seal_interval = "30s"\n# recover_interval = "60s"\n# rpc_polling_interval = "180s"\n# ignore_proof_check = false\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store1"\n# plan = "snapup"\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32gib", "64gib"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_deals = 3\n# sealing.min_deal_space = "8gib"\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store2"\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store3"\n\n[[attached]]\n# name = "persist-store1"\nlocation = "./mock-tmp/remote"\n\n[processors.limitation.concurrent]\n# add_pieces = 5\n# pc1 = 3\n# pc2 = 1\n# c2 = 1\n\n[processors.limitation.staggered]\n# pc1 = "5min"\n# pc2 = "4min"\n\n[processors.ext_locks]\n# gpu1 = 1\n\n[processors.static_tree_d]\n# 2kib = "./tmp/2k/sc-02-data-tree-d.dat"\n\n# fields for the add_pieces processor\n# [[processors.add_pieces]]\n\n# fields for tree_d processor\n[[processors.tree_d]]\n# auto_restart = true\n# inherit_envs = true\n\n# fields for pc1 processors\n[[processors.pc1]]\n# bin = "./dist/bin/damocles-worker-plugin-pc1"\n# args = ["--args-1", "1", --"args-2", "2"]\nnuma_preferred = 0\ncgroup.cpuset = "4-5"\nenvs = { rust_log = "info" }\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "6-7"\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-13"\n# auto_restart = true\n# inherit_envs = true\n\n# fields for pc2 processors\n[[processors.pc2]]\n# cgroup.cpuset = "24-27"\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc2]]\ncgroup.cpuset = "28-31"\n# auto_restart = true\n# inherit_envs = true\n\n# fields for c2 processor\n[[processors.c2]]\ncgroup.cpuset = "32-47"\n# auto_restart = true\n# inherit_envs = true\n\n\nbelow we will break down the configurable items one by one.\n\n\n# [worker]\n\nthe worker configuration item is used to configure some basic information of this worker instance.\n\n\n# basic configuration example\n\n[worker]\n# instance name, optional, string type\n# by default, the ip address of the network card used to connect to `damocles-manager` is used as the instance name\n# name = "worker-#1"\n\n# rpc service listening address, optional, string type\n# the default is "0.0.0.0", that is, listening to all addresses of the machine\n# rpc_server.host = "192.168.1.100"\n\n# rpc service listening port, optional, number type\n# default is 17890\n# rpc_server.port = 17891\n\n# local pieces file directory, optional, string array type\n# if set, worker will load the piece file from the local file\n# otherwise it will load the remote piece file from damocles-manager\n# if the file "/path/to/{your_local_pieces_dir01, your_local_pieces_dir02, ...}/piece_file_name" does not exist, the worker will also load it from the damocles-manager\n# local_pieces_dirs = ["/path/to/your_local_pieces_dir01", "/path/to/your_local_pieces_dir02"]\n\n\nin most cases, each field in this configuration item does not need to be manually configured.\n\nonly in some special cases, such as:\n\n * if you would like to name each damocles-worker instance according to your own naming schemes\n * if you don\'t want to monitor all network card ips and only allow local rpc requests\n * if multiple damocles-workers are deployed on one machine, you would like to avoid port conflicts so that they are distinguished\n * if damocles-worker can access the piece_store directory, you can set local_pieces_dir to load piece file locally\n\nand in other scenarios, you would need to manually configure the options here as needed.\n\n\n# [metrics]\n\nmetrics provides metrics recorder & exporter (prometheus) options.\n\n\n# basic configuration example\n\n[metrics]\n# switch of the prometheus exporter, optional，boolean\n# default value: false\n# when enabled，field "http_listen" will be used to bootstrap a prometheus exporter\n#enable = false\n\n# prometheus exporter listen address, optional, socket address\n# default value: "0.0.0.0:9000"\n#http_listen = "0.0.0.0:9000"\n\n\n\n# [sector_manager]\n\nsector_manager is used to configure damocles-manager related information so that damocles-worker can connect to the corresponding service correctly.\n\n\n# basic configuration example\n\n[sector_manager]\n# the connection address used when constructing the rpc client, required, string type\n# accepts both `multiaddr` format and url format such as `http://127.0.0.1:1789`, `ws://127.0.0.1:1789`\n# normally, use the `multiaddr` format for consistency with other components\nrpc_client.addr = "/ip4/127.0.0.1/tcp/1789"\n\n# the http header information when constructing the rpc client, optional, dictionary type\n# default is null\n# rpc_client.headers = { user-agent = "jsonrpc-core-client" }\n\n# the verification token carried when requesting deal piece data, optional, string type\n# default is null\n# this is usually set when this instance allows sealing of sectors with deal data\n# the value of this config item is usually the token value of the venus service used\n# piece_token = "eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyjuyw1lijoims0xmjuilcjwzxjtijoic2lnbiisimv4dci6iij9.jenwgk0jzcxfdin3cyhbun41vxnvyw-_0uut2zoohm0"\n\n\n\n# [sealing]\n\nsealing is used to configure common parameter options in the sealing process.\n\n\n# basic configuration example\n\n[sealing]\n# allowed `sp`, optional, number array format\n# defaults to null, allows tasks from any `sp`\n# after configuration, only sealing tasks from `sp` listed in the array can be carried out\n# allowed_miners = [10123, 10124, 10125]\n\n# allowed sector size, optional, string array format\n# defaults to null, allows sector tasks of any size\n# after configuration, only tasks that match the sector size listed in the array can be executed\n# allowed_sizes = ["32gib", "64gib"]\n\n# whether to allow adding deals to the sector, optional, boolean type\n# default is false\n# when set to true, the `piece_token` item in `sector_manager` usually needs to be set at the same time\n# enable_deals = true\n\n# whether to disable cc sector, optional, boolean type\n# default is false\n# when enable_deals is true, turn on this option, will keep waiting until a storage deal is obtained, instead of starting sealing cc sectors\n# disable_cc = true\n\n# the maximum number of deals allowed to be added to the sector, optional, number type\n# default is null\n# max_deals = 3\n\n# the minimum size of a deal to be filled in a sector, optional, byte string format\n# default is null\n# min_deal_space = "8gib"\n\n# number of retries when an error of `temp` type is encountered during the sealing process, optional, number format\n# default is 5\n# max_retries = 3\n\n# retry interval when a `temp` type error is encountered during the sealing process, optional, time string format\n# the default is "30s", which means 30 seconds\n# recover_interval = "30s"\n\n# interval for idle `sealing_thread` to apply for new sealing tasks, optional, time string format\n# the default is "30s", which means 30 seconds\n# seal_interval = "30s"\n\n# rpc status polling request interval, optional, time string format\n# the default is "180s", which means 180 seconds\n# during the sealing process, some links use the polling method to obtain non-real-time information, such as if a message is on-chain.\n# this value helps to avoid overly frequent requests consuming network resources\n# rpc_polling_interval = "180s"\n\n# whether to skip the local verification step of proof, optional, boolean format\n# default is false\n# usually only set this during testing\n# ignore_proof_check = false\n\n# number of retries when unable to request a task from `sector_manager`, optional, number format\n# default is 3\n# request_task_max_retries = 3\n\n\nthe configuration items in sealing usually have default items preset from experiences, which removes the need for manual configurations in most cases.\n\n\n# special configuration example\n\n# 1. test network, serving only specific sp\n\nallowed_miners = [2234, 2236, 2238]\n\n\n# 2. large-scale clusters to reduce network usage\n\n# among the recoverable exceptions, a considerable part is caused by network jitter, increase the interval of automatic recovery and reduce the frequency of requests\nrecover_interval = "90s"\n\n# increases the interval time to reduces the request frequency\nrpc_polling_interval = "300s"\n\n\n# 3. increase the possibility of self-recovery from errors during sealing tasks\n\n# increase the number of autorecovery attempts\nmax_retries = 10\n\n# increase the auto-recovery interval\nrecover_interval = "60s"\n\n\n\n# [[sealing_thread]]\n\neach sealing_thread is used to configure a worker thread for a sector. multiple sealing_thread configuration groups can exist in one configuration file.\n\nthe sealing_thread will inherit the configuration in [sealing]. any sealing.* in the sealing_thread configuration will override the configuration of [sealing].\n\n\n# basic configuration example\n\n[[sealing_thread]]\n# sector data directory path, required, string type\n# it is recommended to use an absolute path, the data directory and the worker thread are binded in one-to-one relationship\nlocation = "/mnt/nvme1/store"\n\n# task type, optional, string type\n# default value is null\n# all options: sealer | snapup | rebuild | unseal | wdpost, when left blank, the default is `sealer`\n# plan = "snapup"\n\n# custom parameters of the sealing process, only applies to the current worker thread\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32gib", "64gib"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n# sealing.request_task_max_retries = 3\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\n\n\nthe number of sealing_thread and the corresponding sealing data directory paths need to be planned according to your specific situation.\n\nin order to facilitate combination and matching, each sealing_thread can be configured with an independent sealing sub-item, which satisfies:\n\n * the naming, type and effect of configurable items are consistent with the common sealing items\n * only applies to the current worker thread\n * when not configured use the value in the common sealing item\n\n\n# special configuration example\n\n# 1. two worker threads, each serving a different sp\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\nsealing.allowed_miners = [1357]\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\nsealing.allowed_miners = [2468]\n\n\n# 2. two worker threads, each with different sector sizes\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\nsealing.allowed_sizes = ["32gib"]\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\nsealing.allowed_sizes = ["64gib"]\n\n\n\n# sealing_thread configuration hot reload\n\nbefore damocles v0.5.0, we could only update the configuration by modifying the sealing_thread configuration and restarting damocles-worker. it is very inconvenient in some scenarios, for example: during sector rebuild we would like to be able to reload configuration without rebooting damocles-worker after modifying the plan configuration item of a sealing_thread.\n\nsealing_thread configuration hot reload is supported after v0.5.0. by creating a hot reload configuration file named config.toml under the location directory in a sealing_thread with definitions of the configuration file exactly the same as the content of [[sealing_thread]], the configuration items in this configuration file will override the corresponding [[sealing_thread]] configuration items in damocles-worker, and modifying this configuration file does not require restarting damocles-worker to take effect.\n\nthe sealing_thread in damocles-worker will check the config.toml file in the location directory before starting a new sector task. a reload or removal of the configuration will be triggered, if the content of the config.toml file is either changed or the file is deleted.\n\nnotice:\n\n * hot reload of configuration file config.toml cannot override the location configuration item in sealing_thread.\n * damocles-worker main configuration file does not support hot reload.\n\n# basic configuration example\n\n# /path/to/the_sealing_thread_location/config.toml\n\n# task type, optional, string type\n# default value is null\n# all options: sealer | snapup | rebuild | unseal | wdpost, if not filled, the default is sealer\n# plan = "rebuild"\n\n# custom parameters of the sealing process, only effective on the current worker thread\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32gib", "64gib"]\n# sealing.enable_deals = true\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n# sealing.request_task_max_retries = 3\n\n\n\n# [[attached]]\n\nattached is used to configure where the sealed sector persistent data is saved. multiple configuration entries are allowed at the same time.\n\n\n# basic configuration example\n\n[attached]\n# name, optional, string type\n# the default is the absolute path corresponding to the location\n# name = "remote-store1"\n\n# path, required, string type\n# it is recommended to fill in the absolute path directly\nlocation = "/mnt/remote/10.0.0.14/store"\n\n# read only, optional, boolean\n# default is false\n# readonly = true\n\n\n\nwith the need to coordinate storage location information between damocles-worker and damocles-manager in mind and the fact that in many cases the same persistent storage directory mount path on the damocles-worker machine and on the damocles-manager machine are not exactly the same, so we decided to use name as the basis for coordination.\n\nyou can also choose not to configure name on both sides of damocles-worker and damocles-manager during configuration if the mount path of the persistent storage directory is the same on all machines. in this case, both will use the absolute path as the name.\n\n\n# [processors]\n\nprocessors is used to configure sealing task executors, and some other information during sealing.\n\nthis configuration item is actually divided into three sub-items, and we shall analyze them one by one.\n\n\n# [processors.limitation.concurrent]\n\nwhen both [processors.limit] and [processors.limitation.concurrent] exist in the configuration file, the latter shall override.\n\nprocessors.limitation.concurrent is used to configure the number of parallel tasks for a specific sealing task. this is to reduce the contention of hardware resources at one given sealing stage.\n\nit should be noted that when external processors are configured, the number of external processors and the total allowed concurrency will also affect the number of parallel tasks.\n\n# basic configuration example\n\n[processors.limitation.concurrent]\n# consurrency limit for add_pieces task, optional, number_type\n# add_pieces = 5\n\n# consurrency limit for tree_d task, optional, number type\n# tree_d = 1\n\n# concurrency limit for the pc1 task, optional, number type\n# pc1 = 3\n\n# concurrency limit of the pc2 task, optional, number type\n# pc2 = 2\n\n# concurrency limit for task c2, optional, number type\n# c2 = 1\n\n\nfor example, if pc2 = 2 is set, then at most two sectors can perform pc2 task at the same time.\n\n\n# [processors.limitation.staggered]\n\nprocessors.limitation.staggered is used to configure the time interval for staggered startup of parallel tasks during a given sealing phase. after configuring this item, when multiple tasks are scheduled at the same time for a given sealing phase, damocles-worker will start the tasks in sequence according to the configured time interval, so as to avoid the problem of resource shortage such as slowdown of disk io caused by the simultaneous start of tasks.\n\n# basic configuration example\n\n[processors.limitation.staggered]\n# the time interval for starting multiple pc1 tasks in sequence, optional, string type (e.g. "1s", "2min")\n# pc1 = "5min"\n# pc2 = "4min"\n\n\nfor example, if pc1 = "5min" is set, when two pc1 tasks are scheduled at the same time, the second task will be executed after 5 minutes into the execution of the first task.\n\n\n# [processors.ext_locks]\n\nprocessors.ext_locks is used to configure some custom lock restrictions, which is used in conjunction with the locks configuration item in [[processors.{stage_name}]]. this configuration item allows users to customize some restrictions and make different external processors subject to them.\n\n# basic configuration example\n\n[processors.ext_locks]\n# some_name = some_number\n\n\n# special configuration example\n\nprocessors.ext_locks by itself does not apply the lock.\n\n# one gpu, shared by pc2 and c2\n\n[processors.ext_locks]\ngpu=1\n\n[[processors.pc2]]\nlocks = ["gpu"]\n\n[[processors.c2]]\nlocks = ["gpu"]\n\n\nby this way, pc2 c2 will each start an external processor, and the two will compete for the lock, which means that the two tasks will not happen at the same time.\n\n# two gpu, shared by pc2 and c2\n\n[processors.ext_locks]\ngpu1 = 1\ngpu2 = 1\n\n[[processors.pc2]]\nlocks = ["gpu1"]\n\n[[processors.pc2]]\nlocks = ["gpu2"]\n\n[[processors.c2]]\nlocks = ["gpu1"]\n\n[[processors.c2]]\nlocks = ["gpu2"]\n\n\nin this way, pc2 c2 will each start two external processors, which will create a two-two competition relationship, which allows to limit a gpu to only execute one of the sealing tasks.\n\n\n# [processors.static_tree_d]\n\nprocessors.static_tree_d is a configuration item introduced to improve the efficiency of cc sector.\n\nwhen a static file path is configured for the corresponding sector size, this file will be used directly as the tree_d file for cc sector without trying to generate it again.\n\n# basic configuration example\n\n[processors.static_tree_d]\n2kib = "/var/tmp/2k/sc-02-data-tree-d.dat"\n32gib = "/var/tmp/32g/sc-02-data-tree-d.dat"\n64gib = "/var/tmp/64g/sc-02-data-tree-d.dat"\n\n\n\n# [[processors.{stage_name}]]\n\nthis is the configuration group used to configure external processors.\n\ncurrently {stage_name} could be one of the following.\n\n * add_pieces for add pieces phase\n * tree_d for generation phase of tree d\n * pc1 for the precommit1 phase\n * pc2 for the precommit2 phase\n * synth_proof for the synthetic proof phase\n * c2: for commit2 phase\n * unseal: for unseal phase\n * transfer: used to customize the transfer method between local data and persistent data storage\n\neach such configuration group translates to an external processor of the corresponding sealing phase to be started. if nothing is configured for one of the above {stage_name} and corresponding [[processors.{stage_name}]] line does not exist in the configuration file, then damocles-worker will not start a child process for this {stage_name}. damocles-worker will use the built-in processor code to directly execute the corresponding {stage_name} task in sealing_thread, which means that the concurrent number of {stage_name} tasks depends on the corresponding number of sealing_thread and concurrent number of {stage_name} as configured in [processors.limitation.concurrent]. by not configuring external processor saves extra steps such as serializing task parameters and task output, but one loses capabilities such as more powerful concurrency control, cgroup control, and custom proof algorithms. you are feel to choose according to your own needs.\n\nthe optional configuration items for [[processors.{stage_name}]] include:\n\n[[processors.pc1]]\n# custom external processor executable file path, optional, string type\n# by default, the executable file path corresponding to the main process will be used, execute the damocles-worker builtin processor\n# bin = "./dist/bin/damocles-worker-plugin-pc1"\n\n# customize the parameters of the external processor, optional item, string array type\n# the default value is null, `damocles-worker`\'s own executor default parameters will be used\n# args = ["--args-1", "1", --"args-2", "2"]\n\n# time to get ready of external child-processes, optional, time string\n# the default value is 5s\n# stable_wait = "5s"\n\n# numa affinity partition id, optional, number type\n# default value is null, no affinity will be set\n# need to fill in according to the host\'s numa partition\n# numa_preferred = 0\n\n# cpu core binding and limit options, optional, string type\n# the default value is null, no binding is set\n# the format of the value follows the standard cgroup.cpuset format\n# cgroup.cpuset = "4-5"\n\n# additional environment variables for external processors, optional, dictionary type\n# default value is null\n# envs = { rust_log = "info" }\n\n# the maximum number of concurrent tasks allowed by this executor\n# the default value is null, unlimited, but whether the task is executed concurrently depends on the implementation of the external processor used\n# mainly used in pc1 so that multiple parallel links can be used, which can effectively save resources such as shared memory and thread pools\n# concurrent = 4\n\n# custom external limit lock name, optional, string array type\n# default value is null\n# locks = ["gpu1"]\n\n# whether to restart automatically when a child process exits, optional, bool type\n# default is true\n# auto_restart = true\n\n# whether to inherit the environment variables from the worker daemon process, optional, bool type\n# default is true\n# inherit_envs = true\n\n\n# basic configuration example\n\n[processors.limit]\nadd_pieces = 8\npc1 = 4\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-7"\nconcurrent = 2\nenvs = { fil_proofs_use_multicore_sdr = "1" }\nauto_restart = true\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-19"\nconcurrent = 2\nenvs = { fil_proofs_use_multicore_sdr = "1" }\nauto_restart = true\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0" }\nauto_restart = true\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-39"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "1" }\nauto_restart = true\n\n\n[[processors.c2]]\ncgroup.cpuset = "28-35"\nenvs = { cuda_visible_devices = "2,3" }\nauto_restart = true\n\n\n[[processors.tree_d]]\ncgroup.cpuset = "40-45"\nauto_restart = true\n\n\nthe above is an example processors.{stage_name} configuration for a 48c + 4gpu box. under this configuration, it will start:\n\n * 2 pc1 external processors, both using multicore_sdr mode, each with 8 cores, allowing 2 concurrent tasks, and memory allocation preferentially uses its configured numa partition\n * 2 pc2 external processors, each with 8 cores, each using one gpu\n * 1 c2 external processor, allocated 8 cores, using one gpu\n * 1 tree_d external processor with 6 cores allocated\n\n# special configuration example\n\n# 1. using user-defined closed source, algorithmically optimized c2 external processor\n\n[[processors.c2]]\nbin = "/usr/local/bin/damocles-worker-c2-optimized"\ncgroup.cpuset = "40-47"\nenvs = { cuda_visible_devices = "2,3" }\n\n\n# 2. c2 external processor using outsourced mode (remote computation)\n\n[[processors.c2]]\nbin = "/usr/local/bin/damocles-worker-c2-outsource"\nargs = ["--url", "/ip4/apis.filecoin.io/tcp/10086/https", "--timeout", "10s"]\nenvs = { licence_path = "/var/tmp/c2.licence.dev" }\n\n\n# 3. use cpu mode to make up for pc2 computing power when gpu is lacking\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-45"\n\n\n# 4. under the optimal planning, the total amount of pc1 is odd and cannot be divided equally\n\n[processors.limitation.concurrent]\npc1 = 29\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-41"\nconcurrent = 14\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-92"\nconcurrent = 15\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n\n# 5. to prioritize numa 0 area for pc1\n\n[processors.limit]\npc1 = 29\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-47"\nconcurrent = 16\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-86"\nconcurrent = 13\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n\n# cgroup.cpuset configuration notes\n\n * for pc1 external processors with multicore sdr enabled, cgroup.cpuset should use the entirety of the cpu cores under one l3 cache as a base unit for configuration as much as possible. if you need to configure portion of cpu cores under one l3 cache, you must make sure that the number of cpu cores under each l3 cache is the same.\n   \n   according to rust-fil-proofs, when multicore sdr is enabled, the number of cpu cores and number of corresponding cores on single die must be an integer multiple relationship. if the external processor is using rust-fil-proofs or rust-fil-proofs based variants, this rule must be followed, otherwise it can be ignored. the default processor of damocles-worker is based on rust-fil-proofs.\n\n * the cpu cores in cgroup.cpuset should not span numa nodes as much as possible. across numa nodes will make cpu access to memory slower. (damocles supports loading numa affinity hugepage memory files after v0.5.0, if this feature is enabled then you can allocate cpusets across numa nodes without concerns)\n   \n   if the configured cpu cores are on the same numa node, processors.{stage_name}.numa_preferred can be configured with the corresponding numa node id.\n\nuse damocles-worker-util to check cpu information\n\n./damocles-worker-util hwinfo\n\n\noutput:\n\ncpu topology:\nmachine (503.55 gib)\n├── package (251.57 gib) (*** *** *** 32-core processor)\n│ ├── numanode (#0 251.57 gib)\n│ ├── l3 (#0 16 mib)\n│ │ └── pu #0 + pu #1 + pu #2 + pu #3\n│ ├── l3 (#1 16 mib)\n│ │ └── pu #4 + pu #5 + pu #6 + pu #7\n│ ├── l3 (#2 16 mib)\n│ │ └── pu #8 + pu #9 + pu #10 + pu #11\n│ ├── l3 (#3 16 mib)\n│ │ └── pu #12 + pu #13 + pu #14 + pu #15\n│ ├── l3 (#4 16 mib)\n│ │ └── pu #16 + pu #17 + pu #18 + pu #19\n│ ├── l3 (#5 16 mib)\n│ │ └── pu #20 + pu #21 + pu #22 + pu #23\n│ ├── l3 (#6 16 mib)\n│ │ └── pu #24 + pu #25 + pu #26 + pu #27\n│ └── l3 (#7 16 mib)\n│ └── pu #28 + pu #29 + pu #30 + pu #31\n└── package (251.98 gib) (*** *** *** 32-core processor)\n    ├── numanode (#1 251.98 gib)\n    ├── l3 (#8 16 mib)\n    │ └── pu #32 + pu #33 + pu #34 + pu #35\n    ├── l3 (#9 16 mib)\n    │ └── pu #36 + pu #37 + pu #38 + pu #39\n    ├── l3 (#10 16 mib)\n    │ └── pu #40 + pu #41 + pu #42 + pu #43\n    ├── l3 (#11 16 mib)\n    │ └── pu #44 + pu #45 + pu #46 + pu #47\n    ├── l3 (#12 16 mib)\n    │ └── pu #48 + pu #49 + pu #50 + pu #51\n    ├── l3 (#13 16 mib)\n    │ └── pu #52 + pu #53 + pu #54 + pu #55\n    ├── l3 (#14 16 mib)\n    │ └── pu #56 + pu #57 + pu #58 + pu #59\n    └── l3 (#15 16 mib)\n        └── pu #60 + pu #61 + pu #62 + pu #63\n...\n\n\nfrom the output information, we can see that this machine has two numanodes, each numanode has 8 l3 caches, and each l3 cache has 4 cpu cores.\n\n * cpu cores on numanode #0: 0-31\n * cpu cores on numanode #1: 31-63\n\nusing this machine as an example, configuration like processors.pc1.cgroup.cpuset = "0-6" does not satisfy the rust-fil-proofs rule.\n\n 1. pu#0, pu#1, pu#2, pu#3, these 4 cpu cores belong to l3#0\n 2. pu#4, pu#5, pu#6, these 3 cpu cores belong to l3#1\n\nthe number of shared caches of is 2 (l3#0, l3#1), and the number of configured cpu cores is inconsistent, which does not meet the rust-fil-proofs rule and cannot be started. the correct configuration could be processors.pc1.cgroup.cpuset = "0-7".\n\n\n# a minimal working configuration file example\n\n[sector_manager]\nrpc_client.addr = "/ip4/{some_ip}/tcp/1789"\n\n# according to actual resource planning\n[[sealing_thread]]\nlocation = "{path to sealing store1}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store2}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store3}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store4}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store5}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store6}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store7}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store8}"\n\n\n[remote_store]\nname = "{remote store name}"\nlocation = "{path to remote store}"\n\n[processors.static_tree_d]\n32gib = "{path to static tree_d for 32gib}"\n64gib = "{path to static tree_d for 64gib}"\n\n# according to actual resource planning\n[processors.limit]\npc1 = 4\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-7"\nconcurrent = 2\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-19"\nconcurrent = 2\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-39"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "1" }\n\n\n[[processors.c2]]\ncgroup.cpuset = "28-35"\nenvs = { cuda_visible_devices = "2,3" }\n\n\n[[processors.tree_d]]\ncgroup.cpuset = "40-45"\n\n\nafter planning according to the actual situation and filling in the corresponding information, the above is a minimal working configuration file that...\n\n * does cc sector only\n * optimized tree_d generation for 32gib and 64gib sectors\n * integrated resource allocation\n\n# references to the venus community test cases\n\nreference example 1 takeaways: pc1 is precisely limited to cores, and c2 is completed by gpuproxy, which has strong scalability. the disadvantage is that the configuration is complex, and the number of tasks needs to be adjusted according to the actual hardware specs\n\nreference example 2 takeaways: pc2 and c2 share 1 gpu, may cause some c2 tasks be backlogged\n\nreference example 3 takeaways: 2 sets of pc2 share gpu resources with 2 sets of c2 respectively\n\nreference example 4 takeaways: suitable for lower end machines, creates 96g of swap space on nvme, but this may cause some tasks to be slower',charsets:{cjk:!0}},{title:"Import existing sector data",frontmatter:{},regularPath:"/operation/migrate-sectors.html",relativePath:"operation/migrate-sectors.md",key:"v-7f2f1742",path:"/operation/migrate-sectors.html",headers:[{level:2,title:"Import and verify",slug:"import-and-verify",normalizedTitle:"import and verify",charIndex:232},{level:3,title:"import",slug:"import",normalizedTitle:"import",charIndex:172},{level:3,title:"Validation",slug:"validation",normalizedTitle:"validation",charIndex:5691}],headersStr:"Import and verify import Validation",content:'# Import existing sector data\n\nWhen one would like to migrate the sector storage directory sealed by other sealer solution to damocles, it needs to use damocles-manager to import and update the corresponding configuration file.\n\n\n# Import and verify\n\nNote: Both import and validation need to be done without damocles-manager daemon being started.\n\n\n# import\n\ndamocles-manager provides an import tool called storage attach, which can be used as following:\n\ndamocles-manager util storage attach --verbose --name={storage name} <path>\n\n\n * name is an optional parameter;\n * <path> is the storage path, which is converted to an absolute path during import.\n\nFor details of name and <path>, please refer to [Common.PersistStores](./04.damocles-manager-config.md #commonpersiststores).\n\nFor example, by using...\n\ndamocles-manager util storage attach --verbose --name=a ./mock-tmp/remote\n\n\nlogs similar to the following would likely be produced:\n\n2022-03-11T16:03:52.492+0800 DEBUG policy policy/const.go:18 NETWORK SETUP {"name": "mainnet"}\n2022-03-11T16:03:52.493+0800 INFO cmd internal/util_storage.go:104 use match pattern "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/remote /sealed/*" {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.493+0800 INFO cmd internal/util_storage.go:121 path "s-t010000-16" matched=true {"name": "a", "strict": false, "read-only ": false}\n2022-03-11T16:03:52.494+0800 INFO cmd internal/util_storage.go:121 path "s-t010000-17" matched=true {"name": "a", "strict": false, "read-only ": false}\n2022-03-11T16:03:52.494+0800 INFO cmd internal/util_storage.go:121 path "s-t010000-18" matched=true {"name": "a", "strict": false, "read-only ": false}\n2022-03-11T16:03:52.508+0800 INFO cmd internal/util_storage.go:148 sector indexer updated for s-t010000-16 {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.509+0800 INFO cmd internal/util_storage.go:148 sector indexer updated for s-t010000-17 {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.509+0800 INFO cmd internal/util_storage.go:148 sector indexer updated for s-t010000-18 {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.509+0800 INFO cmd internal/util_storage.go:152 3 sectors out of 3 files have been updated {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.509+0800 WARN cmd internal/util_storage.go:153 add the section below into the config file: {"name": "a", "strict": false, "read-only": false}\n\n[[Common.PersistStores]]\nName = "a"\nPath = "/home/dtynn/proj/github.com/ipfs-force-community/damocles/mock-tmp/remote"\nStrict = false\nReadOnly = false\n\n\nAt this point, the directory import has been completed, and the location information of all sectors has also been recorded. We can then complete the import by copying configuration in the output into the configuration file of damocles-manager.\n\n# reimport\n\nIf we find that the information supplied during import is wrong, such as --name is misspelled, we can re-import with correct information to complete the import process. The location information of the sector will be overwritten and updated.\n\n# separation of sealed_file and cache_dir\n\nSome sealer components allow sealed_file and cache_dir to be on different storage instances, in which case regular import may fail to locate sector files. In this case, you can enable the split scan mode by adding the command line parameter --allow-splitted. In this mode, paths in the sealed folder and the cache folder that match the sector naming rules will be scanned separately.\n\nAt this point, the log will look similar to:\n\n2022-04-19T19:11:55.137+0800 DEBUG policy policy/const.go:18 NETWORK SETUP {"name": "mainnet"}\n2022-04-19T19:11:55.154+0800 INFO cmd internal/util_storage.go:120 scan for sectors(upgrade=false) {"name": "p3", "strict": false, "read-only": false , "splitted": true}\n2022-04-19T19:11:55.154+0800 INFO cmd internal/util_storage.go:211 0 sectors out of 0 files have been found {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.154+0800 INFO cmd internal/util_storage.go:145 scan for splitted cache dirs(upgrade=false) {"name": "p3", "strict": false, "read-only" : false, "splitted": true}\n2022-04-19T19:11:55.155+0800 INFO cmd internal/util_storage.go:211 3 sectors out of 3 files have been found {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.156+0800 INFO cmd internal/util_storage.go:120 scan for sectors(upgrade=true) {"name": "p3", "strict": false, "read-only": false , "splitted": true}\n2022-04-19T19:11:55.156+0800 INFO cmd internal/util_storage.go:211 0 sectors out of 0 files have been found {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.156+0800 INFO cmd internal/util_storage.go:145 scan for splitted cache dirs(upgrade=true) {"name": "p3", "strict": false, "read-only" : false, "splitted": true}\n2022-04-19T19:11:55.156+0800 INFO cmd internal/util_storage.go:211 0 sectors out of 0 files have been found {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.156+0800 WARN cmd internal/util_storage.go:166 add the section below into the config file: {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n\n[[Common.PersistStores]]\nName = "p3"\nPath = "/home/dtynn/proj/github.com/ipfs-force-community/damocles/\nmock-tmp/pstore3"\nStrict = false\nReadOnly = false\n\n\nNotes for using this mode:\n\n * There are no duplicate stored files due to storage exceptions in the target directory\n * Sectors with only cache_dir and no corresponding sealed_file won\'t be located\n\n\n# Validation\n\nThe storage find tool provided by damocles-manager can be used to check whether the result of sector import is correct. It can be used as following:\n\ndamocles-manager util storage find <miner actor id> <sector number>\n\n\nContinuing with an example of the validation command demonstrated above, we want to verify that the sector s-t010000-17 has been recorded correctly, we can use:\n\ndamocles-manager util storage find 10000 17\n\n\nUsually a log similar to the following would be produced:\n\n2022-04-19T19:13:15.235+0800 DEBUG policy policy/const.go:18 NETWORK SETUP {"name": "mainnet"}\n2022-04-19T19:13:15.249+0800 INFO cmd internal/util_storage.go:279 sector s-t010000-17 located, sealed file in "a", cache dir in "a"\n2022-04-19T19:13:15.249+0800 INFO cmd internal/util_storage.go:285 store instance exists {"instance": "a"}\n2022-04-19T19:13:15.249+0800 INFO cmd internal/util_storage.go:285 store instance exists {"instance": "a"}\n\n\nThis means that our import and configuration work is complete.\n\n# Validation exception: sector information was not recorded successfully\n\nIf you encounter something like the following during the verification process...\n\n2022-03-11T16:45:59.120+0800 WARN cmd internal/util_storage.go:214 s-t010000-17 not found\n\n\nSuch log indicates that the specified sector was not imported successfully, and we need to recheck the import process.\n\n# Validation exception: Storage configuration not updated\n\nIf you encounter something like the following during the verification process...\n\n2022-03-11T16:22:34.044+0800 DEBUG policy policy/const.go:18 NETWORK SETUP {"name": "mainnet"}\n2022-03-11T16:22:34.059+0800 INFO cmd internal/util_storage.go:218 found s-t010000-17 in "a"\n2022-03-11T16:22:34.059+0800 WARN cmd internal/util_storage.go:227 store instance not found, check your config file\n\n\nSuch a log indicates that the configuration file of damocles-manager has not been successfully updated, and we need to update the configuration according to the method mentioned above.',normalizedContent:'# import existing sector data\n\nwhen one would like to migrate the sector storage directory sealed by other sealer solution to damocles, it needs to use damocles-manager to import and update the corresponding configuration file.\n\n\n# import and verify\n\nnote: both import and validation need to be done without damocles-manager daemon being started.\n\n\n# import\n\ndamocles-manager provides an import tool called storage attach, which can be used as following:\n\ndamocles-manager util storage attach --verbose --name={storage name} <path>\n\n\n * name is an optional parameter;\n * <path> is the storage path, which is converted to an absolute path during import.\n\nfor details of name and <path>, please refer to [common.persiststores](./04.damocles-manager-config.md #commonpersiststores).\n\nfor example, by using...\n\ndamocles-manager util storage attach --verbose --name=a ./mock-tmp/remote\n\n\nlogs similar to the following would likely be produced:\n\n2022-03-11t16:03:52.492+0800 debug policy policy/const.go:18 network setup {"name": "mainnet"}\n2022-03-11t16:03:52.493+0800 info cmd internal/util_storage.go:104 use match pattern "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/remote /sealed/*" {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.493+0800 info cmd internal/util_storage.go:121 path "s-t010000-16" matched=true {"name": "a", "strict": false, "read-only ": false}\n2022-03-11t16:03:52.494+0800 info cmd internal/util_storage.go:121 path "s-t010000-17" matched=true {"name": "a", "strict": false, "read-only ": false}\n2022-03-11t16:03:52.494+0800 info cmd internal/util_storage.go:121 path "s-t010000-18" matched=true {"name": "a", "strict": false, "read-only ": false}\n2022-03-11t16:03:52.508+0800 info cmd internal/util_storage.go:148 sector indexer updated for s-t010000-16 {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.509+0800 info cmd internal/util_storage.go:148 sector indexer updated for s-t010000-17 {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.509+0800 info cmd internal/util_storage.go:148 sector indexer updated for s-t010000-18 {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.509+0800 info cmd internal/util_storage.go:152 3 sectors out of 3 files have been updated {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.509+0800 warn cmd internal/util_storage.go:153 add the section below into the config file: {"name": "a", "strict": false, "read-only": false}\n\n[[common.persiststores]]\nname = "a"\npath = "/home/dtynn/proj/github.com/ipfs-force-community/damocles/mock-tmp/remote"\nstrict = false\nreadonly = false\n\n\nat this point, the directory import has been completed, and the location information of all sectors has also been recorded. we can then complete the import by copying configuration in the output into the configuration file of damocles-manager.\n\n# reimport\n\nif we find that the information supplied during import is wrong, such as --name is misspelled, we can re-import with correct information to complete the import process. the location information of the sector will be overwritten and updated.\n\n# separation of sealed_file and cache_dir\n\nsome sealer components allow sealed_file and cache_dir to be on different storage instances, in which case regular import may fail to locate sector files. in this case, you can enable the split scan mode by adding the command line parameter --allow-splitted. in this mode, paths in the sealed folder and the cache folder that match the sector naming rules will be scanned separately.\n\nat this point, the log will look similar to:\n\n2022-04-19t19:11:55.137+0800 debug policy policy/const.go:18 network setup {"name": "mainnet"}\n2022-04-19t19:11:55.154+0800 info cmd internal/util_storage.go:120 scan for sectors(upgrade=false) {"name": "p3", "strict": false, "read-only": false , "splitted": true}\n2022-04-19t19:11:55.154+0800 info cmd internal/util_storage.go:211 0 sectors out of 0 files have been found {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.154+0800 info cmd internal/util_storage.go:145 scan for splitted cache dirs(upgrade=false) {"name": "p3", "strict": false, "read-only" : false, "splitted": true}\n2022-04-19t19:11:55.155+0800 info cmd internal/util_storage.go:211 3 sectors out of 3 files have been found {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.156+0800 info cmd internal/util_storage.go:120 scan for sectors(upgrade=true) {"name": "p3", "strict": false, "read-only": false , "splitted": true}\n2022-04-19t19:11:55.156+0800 info cmd internal/util_storage.go:211 0 sectors out of 0 files have been found {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.156+0800 info cmd internal/util_storage.go:145 scan for splitted cache dirs(upgrade=true) {"name": "p3", "strict": false, "read-only" : false, "splitted": true}\n2022-04-19t19:11:55.156+0800 info cmd internal/util_storage.go:211 0 sectors out of 0 files have been found {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.156+0800 warn cmd internal/util_storage.go:166 add the section below into the config file: {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n\n[[common.persiststores]]\nname = "p3"\npath = "/home/dtynn/proj/github.com/ipfs-force-community/damocles/\nmock-tmp/pstore3"\nstrict = false\nreadonly = false\n\n\nnotes for using this mode:\n\n * there are no duplicate stored files due to storage exceptions in the target directory\n * sectors with only cache_dir and no corresponding sealed_file won\'t be located\n\n\n# validation\n\nthe storage find tool provided by damocles-manager can be used to check whether the result of sector import is correct. it can be used as following:\n\ndamocles-manager util storage find <miner actor id> <sector number>\n\n\ncontinuing with an example of the validation command demonstrated above, we want to verify that the sector s-t010000-17 has been recorded correctly, we can use:\n\ndamocles-manager util storage find 10000 17\n\n\nusually a log similar to the following would be produced:\n\n2022-04-19t19:13:15.235+0800 debug policy policy/const.go:18 network setup {"name": "mainnet"}\n2022-04-19t19:13:15.249+0800 info cmd internal/util_storage.go:279 sector s-t010000-17 located, sealed file in "a", cache dir in "a"\n2022-04-19t19:13:15.249+0800 info cmd internal/util_storage.go:285 store instance exists {"instance": "a"}\n2022-04-19t19:13:15.249+0800 info cmd internal/util_storage.go:285 store instance exists {"instance": "a"}\n\n\nthis means that our import and configuration work is complete.\n\n# validation exception: sector information was not recorded successfully\n\nif you encounter something like the following during the verification process...\n\n2022-03-11t16:45:59.120+0800 warn cmd internal/util_storage.go:214 s-t010000-17 not found\n\n\nsuch log indicates that the specified sector was not imported successfully, and we need to recheck the import process.\n\n# validation exception: storage configuration not updated\n\nif you encounter something like the following during the verification process...\n\n2022-03-11t16:22:34.044+0800 debug policy policy/const.go:18 network setup {"name": "mainnet"}\n2022-03-11t16:22:34.059+0800 info cmd internal/util_storage.go:218 found s-t010000-17 in "a"\n2022-03-11t16:22:34.059+0800 warn cmd internal/util_storage.go:227 store instance not found, check your config file\n\n\nsuch a log indicates that the configuration file of damocles-manager has not been successfully updated, and we need to update the configuration according to the method mentioned above.',charsets:{}},{title:"Standalone PoSter node",frontmatter:{},regularPath:"/operation/poster.html",relativePath:"operation/poster.md",key:"v-113a25c1",path:"/operation/poster.html",headers:[{level:2,title:"worker-prover mode",slug:"worker-prover-mode",normalizedTitle:"worker-prover mode",charIndex:982},{level:3,title:"Fundamental",slug:"fundamental",normalizedTitle:"fundamental",charIndex:1263},{level:3,title:"Damocles-manager Configuration and Startup",slug:"damocles-manager-configuration-and-startup",normalizedTitle:"damocles-manager configuration and startup",charIndex:3211},{level:3,title:"damocles-worker configuration",slug:"damocles-worker-configuration",normalizedTitle:"damocles-worker configuration",charIndex:4376},{level:3,title:"Manage window post tasks",slug:"manage-window-post-tasks",normalizedTitle:"manage window post tasks",charIndex:8970},{level:3,title:"Disable damocles-manager to use GPU",slug:"disable-damocles-manager-to-use-gpu",normalizedTitle:"disable damocles-manager to use gpu",charIndex:10669},{level:2,title:"Proxy node mode",slug:"proxy-node-mode",normalizedTitle:"proxy node mode",charIndex:10996},{level:3,title:"How to use the proxy node",slug:"how-to-use-the-proxy-node",normalizedTitle:"how to use the proxy node",charIndex:11545},{level:3,title:"The agent node uses the existing configuration file",slug:"the-agent-node-uses-the-existing-configuration-file",normalizedTitle:"the agent node uses the existing configuration file",charIndex:12966},{level:2,title:"ext-prover executor",slug:"ext-prover-executor",normalizedTitle:"ext-prover executor",charIndex:14180},{level:2,title:"Deployment Practice",slug:"deployment-practice",normalizedTitle:"deployment practice",charIndex:16517},{level:2,title:"Limitations",slug:"limitations",normalizedTitle:"limitations",charIndex:18362}],headersStr:"worker-prover mode Fundamental Damocles-manager Configuration and Startup damocles-worker configuration Manage window post tasks Disable damocles-manager to use GPU Proxy node mode How to use the proxy node The agent node uses the existing configuration file ext-prover executor Deployment Practice Limitations",content:'# Standalone PoSter node\n\nIn earlier versions, although damocles-manager supports using --poster and --miner parameters of the daemon run command to enable the corresponding module, the post proof process is still of strong correlation with sector location information which makes it more limited and difficult to expand.\n\nFrom v0.2.0 onwards, we have provided a series of function combinations that make easy-to-use, scalable standalone PoSter nodes an option for SP of large-scale operations and multiple miner ids.\n\nBelow, we will introduce these new features and provide a practice to complete the deployment of standalone PoSter nodes using these features. Subsequent documents use the node with --poster enabled as an example, and the standalone --miner node operates in a similar manner, which will not be described separately.\n\n----------------------------------------\n\nFrom version v0.8.0 and onwards, damocles supports three ways to run PoSter nodes independently, namely worker-prover mode, proxy node mode, and ext-prover mode (external executor mode).\n\n\n# worker-prover mode\n\nThe worker-prover mode is a new feature of v0.8.0. It is characterized by simplicity and can support multi-machine wdpost (with coordination and redundancy) very easily.\n\n\n# Fundamental\n\nThe worker-prover mode uses damocles-worker to compute the window post proof, obtains the window post task from damocles-manager through RPC and returns the computation result.\n\ndamocles-worker adds wdpost planner for executing window post tasks.\n\n# Architecture\n\n                +-----------------------------------+\n                |     damocles-manager daemon       |\n                |     with --worker-prover flag     |\n                |                                   |\n                |        +-----------------+        |\n                |        |damocles-manager |        |\n                |        |  poster module  |        |\n                |        +-------+-^-------+        |\n                |           send | |recv            |\n                |                | |                |\n                |        +-------v-+-------+        |\n                |        |  worker-prover  |        |\n       +--------+--------\x3e      module     <--------+--------+\n       |        |        +--------^--------+        |        |\n       |        |                 |                 |        |\n       |        +-----------------+-----------------+        |\n       |                          |                          |\n-------+--------------------------+--------------------------+------------\n       |                          |                          |\n       | pull job                 | pull job                 | pull job\n       | push res                 | push res                 | push res\n       | by rpc                   | by rpc                   | by rpc\n       |                          |                          |\n+------+--------+         +-------+-------+           +------+--------+\n|damocles-worker|         |damocles-worker|           |damocles-worker|\n|wdpost planner |         |wdpost planner |  ...      |wdpost planner |\n+---------------+         +---------------+           +---------------+\n\n\n\n# Damocles-manager Configuration and Startup\n\nNew configuration:\n\n# ~/.damocles-manager/sector-manager.cfg\n\n#...\n\n[Common.Proving.WorkerProver]\n# The maximum number of attempts of the WindowPoSt task, optional, number type\n# Default value is 2\n# The WindowPoSt task whose number of attempts exceeds JobMaxTry can only be re-executed by manually resetting\nJobMaxTry = 2\n# WindowPoSt task heartbeat timeout, optional, time string type\n# Default value is 15s\n# Tasks that have not sent a heartbeat for more than this amount of time will be set as failed and retried\nHeartbeatTimeout = "15s"\n# WindowPoSt task timeout, optional, time string type\n# Default value is 25h\n# WindowPoSt tasks whose creation time exceeds this time will be deleted\nJobLifetime = "25h0m0s"\n\n#...\n\n\nStart the damocles-manager process:\n\n# --miner flag is optional to add, which means to start the miner module to execute WinningPoSt and produce blocks\n# --poster flag must be added, which means to start the WindowPoSt module\n# --worker-prover must be added, indicating that the WorkerProver module is used to execute WindowPoSt\n./damocles-manager daemon run --miner --poster --worker-prover\n\n\n\n# damocles-worker configuration\n\nConfiguration walkthrough:\n\n[[sealing_thread]]\n# Configure to use wdpost plan\nplan = "wdpost"\n# The configuration limits the execution of the task to the specified miner id; if it is left empty, it means no limit\n# sealing.allowed_miners = [6666, 7777]\n# Configure tasks that only allow sectors of the specified size to run\n# allowed_sizes = ["32GiB", "64GiB"]\n\n[[attached]]\n# Configure the permanent storage that this worker will use during the execution of the window post task\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n\n# Control window_post task concurrency (optional), no limit if not configured\n[processors.limitation.concurrent]\nwindow_post = 2\n\n[[processors. window_post]]\n# Use a custom wdpost proof (optional), if you do not configure bin, the built-in proof will be used by default\nbin="~/my_algorithm"\nargs = ["window_post"]\n# Limit the cpuset to this sub processes\n# cgroup.cpuset = "10-19"\n# Configure environment variables for custom proof (optional)\nenvs = { BELLMAN_GPU_INDEXS="0", CUDA_VISIBLE_DEVICES="0", TMPDIR = "/tmp/worker-prover1/", ... }\n# Configure the maximum concurrent number of this process (optional), no limit if not configured\nconcurrent = 1\n\n\n# A simple example configuration to start just one wdpost sealing_thread is as follows:\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-USA-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\n# The time interval for trying to claim tasks, the default is 60s,\n# For wdpost plan, we can reduce this value to get new wdpost tasks faster\nsealing.recover_interval = "15s"\n# sealing.allowed_miners = [6666]\n# sealing. allowed_sizes = ["32GiB"]\n#...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n\n# Example configuration for two wdpost job with one GPU\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-USA-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [6666]\n# ...\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [7777]\n# ...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n[[attached]]\nname = "miner-7777-store"\nlocation = "/mnt/miner-7777-store"\n\n# -------------------------\n\n[[processors.window_post]]\nbin="~/my_algorithm"\n# args = ["window_post", ...]\ncgroup.cpuset = "10-19"\nenvs = { CUDA_VISIBLE_DEVICES="0", TMPDIR = "/tmp/worker-prover1/", ... }\nconcurrent = 1\n\n[[processors.window_post]]\nbin="~/my_algorithm"\n# args = ["window_post", ...]\ncgroup.cpuset = "20-29"\nenvs = { CUDA_VISIBLE_DEVICES="0", TMPDIR = "/tmp/worker-prover1/", ... }\nconcurrent = 1\n\n\nDoing two wdPost jobs with one GPU can greatly improve utilization. As each wdPost job is composed of vanilla_proofs (CPU computation) and snark_proof (GPU computation), improved GPU utilization can be achieved through allowing vanilla_proofs to be computed in parallel and having snark_proof to be computed serially using GPU lock file in TMPDIR.\n\n# An example of a wdpost machine equipped with two graphics cards\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-USA-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [6666]\n#...\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [7777]\n#...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n[[attached]]\nname = "miner-7777-store"\nlocation = "/mnt/miner-7777-store"\n\n# -------------------------\n\n[[processors. window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post", ...]\n# cgroup.cpuset = "10-19"\nenvs = { ... }\nconcurrent = 2\n\n# ----------- or ---------\n\n#[[processors. window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post", ...]\n# cgroup.cpuset = "10-19"\n# envs = { CUDA_VISIBLE_DEVICES="0", TMPDIR = "/tmp/worker-prover1/",, ... }\n# concurrent = 1\n\n# [[processors. window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post"]\n# cgroup.cpuset = "20-29"\n# envs = { CUDA_VISIBLE_DEVICES="1", TMPDIR = "/tmp/worker-prover2/", ... }\n# concurrent = 1\n\n\nWhen damocles-worker is running wdpost plan, it is not necessary to use the damocles-worker store sealing-init -l command to initialize the local storage directory of the data.\n\n\n# Manage window post tasks\n\n * # Show window post task list\n\n# By default, unfinished tasks and failed tasks are displayed, where the DDL field represents the deadline Index of the task, and the Try field is the number of attempts of the task\n./damocles-manager util worker wdpost list\n\nJobID MinerID DDL Partitions Worker State Try CreateAt Elapsed Heartbeat Error\n3FgfEnvrub1 1037 3 1,2 10.122.63.30 ReadyToRun 1 07-27 16:37:31 - -\ngbCVH4TUgEf 1037 2 1,2 ReadyToRun 0 07-27 16:35:56 - -\nCrotWCLaXLa 1037 1 1,2 10.122.63.30 Succeed 1 07-27 17:19:04 6m38s(done) -\n\n# show all tasks\n./damocles-manager util worker wdpost list --all\n#...\n\n# show window post task details\n./damocles-manager util worker wdpost list --detail\n#...\n\n\n * # reset task\n\nWhen the execution of the window post task fails and the number of automatic retries reaches the limit, the task status can be manually reset so that it can continue to be picked up and executed by damocles-worker.\n\n./damocles-manager util worker wdpost reset gbCVH4TUgEf 3FgfEnvrub1\n\n\n * # delete task\n\nDeleting a task is similar to resetting a task. When the command to delete a task is executed, the retry mechanism of damocles-manager will detect whether the window post task of the current deadline exists in the database, if not, it will resend the task and record it in the database.\n\nIn addition, worker-prover will automatically delete tasks that have been created for more than a certain period of time (the default is 25 hours, and the time is configurable).\n\n# Delete the specific task\n./damocles-manager util worker wdpost remove gbCVH4TUgEf 3FgfEnvrub1\n\n# delete all tasks\n./damocles-manager util worker wdpost remove-all --really-do-it\n\n\n\n# Disable damocles-manager to use GPU\n\nAfter enabling the worker-prover feature, winning_post is executed by damocles-manager. If you do not want winning_post to use the GPU, you can compile damocles-manager with the following command to disable it:\n\nmake dist-clean\nFFI_BUILD_FROM_SOURCE=1 FFI_USE_GPU=0 make build-manager\n\n\n\n# Proxy node mode\n\nWe know that for PoSter nodes, the most important capability is to obtain real-time and accurate sector location information. In the current damocles-manager version, we only provide metadata management based on the local embedded kv database (more to be supported).\n\nThis only allows data to be managed by one process, and direct data sharing across processes is not possible.\n\nTherefore, we designed the proxy node mode to provide some metadata to other consumer nodes through network interfaces, thus realizing data sharing.\n\n\n# How to use the proxy node\n\nWe have added the --proxy parameter to the daemon run command. Its format is like {ip}:{port}. When the startup command contains a valid --proxy parameter, {ip}:{port} will be used as data source for the current damocles-manager node and the necessary metadata (read-only) management module will be constructed.\n\nIn addition to --proxy, we also provide switches that control whether proxy mode is enabled for specific data management modules.\n\nWe just provide --proxy-sector-indexer-off switch for the time being. When --proxy-sector-indexer-off is enabled, nodes use the SectorIndexer database in their own data directory.\n\nFor example, if started with the damocles-manager daemon run --miner command, it will launch a damocles-manager instance listening on port 1789 using ~/.damocles-manager as the data directory with mining module enabled.\n\nAt this time, we can use the following command to initialize and start a proxy node with the above instance as the data source on the same machine. This proxy node will use ~/.damocles-manager2 as the data directory and listen to 2789 port.\n\ndamocles-manager --home ~/.damocles-manager2 daemon init\n// maintain configuration files\ndamocles-manager --home ~/.damocles-manager2 daemon run --proxy="127.0.0.1:1789" --listen=":2789" --poster\n\n\nThe proxy node can provide the exact same and real-time sector location information as the source node.\n\n\n# The agent node uses the existing configuration file\n\nAccording to the method described in the previous section, we can already start an proxy node, but there is still a problem with this startup method: the configuration file of the proxy node needs to be written again, or copied from the data directory of the source node. This introduces additional maintenance work, especially when configuration files may change frequently.\n\nFor this, we also provide a --conf-dir parameter, which is in the form of a directory path. When the startup command includes a valid --conf-dir parameter, the node will use the configuration file that already exists in the specified directory as its own configuration file.\n\nThis saves the work of writing and maintaining configuration files for different source and agent nodes on the same machine and serving the same set of clusters.\n\nBased on this function, the agent node startup method mentioned in the previous section can become:\n\ndamocles-manager --home ~/.damocles-manager2 daemon run --proxy="127.0.0.1:1789" --listen=":2789" --conf-dir="~/.damocles-manager" --poster\n\n\nAt this point, the source node and the proxy node will use the same batch of configuration files.\n\n\n# ext-prover executor\n\nIn addition to sharing sector information, another challenge faced by standalone PoSter nodes is the utilization of hardware resources.\n\nLimited by the underlying algorithm library, granularity of computing nodes utilizing GPUs is by process. This makes it difficult for PoSter nodes to effectively utilize the computing power of multiple GPUs, and it is also difficult to safely avoid proof timeouts when multiple SPs have conflicting WindostPoSt proof windows.\n\nFor this, we provide an ext-prover mechanism similar to the ext processor in damocles-worker.\n\nThe ext-prover mechanism consists of two components:\n\n 1. The --ext-prover parameter of the daemon run command\n 2. The ext-prover.cfg configuration file in the node data directory\n\nA default ext-prover.cfg file looks like:\n\n# Default config:\n#[[WdPost]]\n#Bin = "/path/to/custom/bin"\n#Args = ["args1", "args2", "args3"]\n#Concurrent = 1\n#Weight = 1\n#ReadyTimeoutSecs = 5\n#[WdPost.Envs]\n#ENV_KEY = "ENV_VAL"\n#\n#[[WinPost]]\n#Bin = "/path/to/custom/bin"\n#Args = ["args1", "args2", "args3"]\n#Concurrent = 1\n#Weight = 1\n#ReadyTimeoutSecs = 5\n#[WinPost.Envs]\n#ENV_KEY = "ENV_VAL"\n#\n\n\nIn recent versions, daemon init initializes the ext-prover.cfg file.\n\nUsers can write their own, or copy the corresponding files from a data directory initialized by the latest version to an existing data directory.\n\nThe functions of the configuration items in ext-prover.cfg are very similar to the configuration blocks in damocles-worker, and users can refer to the corresponding documents for reference.\n\nWhen the --ext-prover parameter is included in the start command of damocles-manager, the node will use the ext-prover.cfg configuration file in the configuration directory as the basis for starting child processes. For this configuration file, setting the --conf-dir parameter will also have an effect.\n\nIf user sees logs like the following, then it means that ext-prover is ready.\n\n2022-04-27T19:15:00.441+0800 INFO porver-ext ext/prover.go:122 response loop start {"pid": 24764, "ppid": 24732, "loop": "resp"}\n2022-04-27T19:15:00.441+0800 INFO porver-ext ext/prover.go:155 request loop start {"pid": 24764, "ppid": 24732, "loop": "req"}\n2022-04-27T19:15:00.468+0800 INFO processor-cmd processor/processor.go:35 ready {"pid": 24764, "ppid": 24732, "proc": "wdpost"}\n\n\n\n# Deployment Practice\n\nSuppose we have a node machine with 8 GPUs, then we can provide stronger PoSt processing capabilities through the following configuration.\n\n 1. Configure and start the source node\n    \n    damocles-manager daemon run --miner\n    \n    \n    At this time, the source node only provides functions and capabilities related to sealing;\n\n 2. Configure the ext-prover.cfg file:\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "0"\n    TMPDIR = "/tmp/ext-prover0/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "1"\n    TMPDIR = "/tmp/ext-prover1/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "2"\n    TMPDIR = "/tmp/ext-prover2/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "3"\n    TMPDIR = "/tmp/ext-prover3/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "4"\n    TMPDIR = "/tmp/ext-prover4/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "5"\n    TMPDIR = "/tmp/ext-prover5/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "6"\n    TMPDIR = "/tmp/ext-prover6/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "7"\n    TMPDIR = "/tmp/ext-prover7/"\n    \n    \n\n 3. Initialize and start a standalone PoSter node\n    \n    damocles-manager --home=~/.damocles-individual-poster daemon init\n    damocles-manager --home=~/.damocles-individual-poster daemon run --proxy="127.0.0.1:1789" --poster --listen=":2789" --conf-dir="~/.damocles-manager" --ext-prover\n    \n\nBy this way of deployment,\n\n * The source node provides both sealing and mining support\n * Proxy nodes provide WindowPoSt support\n   * The proxy node enables ext-prover, and each child process independently uses a GPU and a computing lock directory\n\nThere is no conflict between winningPost and windowPost due to device usage\n\n\n# Limitations\n\nSo far, we have described the functions, principles and simple usage examples that stand-alone PoSter nodes rely on.\n\nHowever, this mode still has some limitations for very large SP clusters, which may manifest in:\n\n * The scheduling of the PoSt and the serious conflict in the PoSt window period still relies on the operation and maintenance to a certain extent;',normalizedContent:'# standalone poster node\n\nin earlier versions, although damocles-manager supports using --poster and --miner parameters of the daemon run command to enable the corresponding module, the post proof process is still of strong correlation with sector location information which makes it more limited and difficult to expand.\n\nfrom v0.2.0 onwards, we have provided a series of function combinations that make easy-to-use, scalable standalone poster nodes an option for sp of large-scale operations and multiple miner ids.\n\nbelow, we will introduce these new features and provide a practice to complete the deployment of standalone poster nodes using these features. subsequent documents use the node with --poster enabled as an example, and the standalone --miner node operates in a similar manner, which will not be described separately.\n\n----------------------------------------\n\nfrom version v0.8.0 and onwards, damocles supports three ways to run poster nodes independently, namely worker-prover mode, proxy node mode, and ext-prover mode (external executor mode).\n\n\n# worker-prover mode\n\nthe worker-prover mode is a new feature of v0.8.0. it is characterized by simplicity and can support multi-machine wdpost (with coordination and redundancy) very easily.\n\n\n# fundamental\n\nthe worker-prover mode uses damocles-worker to compute the window post proof, obtains the window post task from damocles-manager through rpc and returns the computation result.\n\ndamocles-worker adds wdpost planner for executing window post tasks.\n\n# architecture\n\n                +-----------------------------------+\n                |     damocles-manager daemon       |\n                |     with --worker-prover flag     |\n                |                                   |\n                |        +-----------------+        |\n                |        |damocles-manager |        |\n                |        |  poster module  |        |\n                |        +-------+-^-------+        |\n                |           send | |recv            |\n                |                | |                |\n                |        +-------v-+-------+        |\n                |        |  worker-prover  |        |\n       +--------+--------\x3e      module     <--------+--------+\n       |        |        +--------^--------+        |        |\n       |        |                 |                 |        |\n       |        +-----------------+-----------------+        |\n       |                          |                          |\n-------+--------------------------+--------------------------+------------\n       |                          |                          |\n       | pull job                 | pull job                 | pull job\n       | push res                 | push res                 | push res\n       | by rpc                   | by rpc                   | by rpc\n       |                          |                          |\n+------+--------+         +-------+-------+           +------+--------+\n|damocles-worker|         |damocles-worker|           |damocles-worker|\n|wdpost planner |         |wdpost planner |  ...      |wdpost planner |\n+---------------+         +---------------+           +---------------+\n\n\n\n# damocles-manager configuration and startup\n\nnew configuration:\n\n# ~/.damocles-manager/sector-manager.cfg\n\n#...\n\n[common.proving.workerprover]\n# the maximum number of attempts of the windowpost task, optional, number type\n# default value is 2\n# the windowpost task whose number of attempts exceeds jobmaxtry can only be re-executed by manually resetting\njobmaxtry = 2\n# windowpost task heartbeat timeout, optional, time string type\n# default value is 15s\n# tasks that have not sent a heartbeat for more than this amount of time will be set as failed and retried\nheartbeattimeout = "15s"\n# windowpost task timeout, optional, time string type\n# default value is 25h\n# windowpost tasks whose creation time exceeds this time will be deleted\njoblifetime = "25h0m0s"\n\n#...\n\n\nstart the damocles-manager process:\n\n# --miner flag is optional to add, which means to start the miner module to execute winningpost and produce blocks\n# --poster flag must be added, which means to start the windowpost module\n# --worker-prover must be added, indicating that the workerprover module is used to execute windowpost\n./damocles-manager daemon run --miner --poster --worker-prover\n\n\n\n# damocles-worker configuration\n\nconfiguration walkthrough:\n\n[[sealing_thread]]\n# configure to use wdpost plan\nplan = "wdpost"\n# the configuration limits the execution of the task to the specified miner id; if it is left empty, it means no limit\n# sealing.allowed_miners = [6666, 7777]\n# configure tasks that only allow sectors of the specified size to run\n# allowed_sizes = ["32gib", "64gib"]\n\n[[attached]]\n# configure the permanent storage that this worker will use during the execution of the window post task\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n\n# control window_post task concurrency (optional), no limit if not configured\n[processors.limitation.concurrent]\nwindow_post = 2\n\n[[processors. window_post]]\n# use a custom wdpost proof (optional), if you do not configure bin, the built-in proof will be used by default\nbin="~/my_algorithm"\nargs = ["window_post"]\n# limit the cpuset to this sub processes\n# cgroup.cpuset = "10-19"\n# configure environment variables for custom proof (optional)\nenvs = { bellman_gpu_indexs="0", cuda_visible_devices="0", tmpdir = "/tmp/worker-prover1/", ... }\n# configure the maximum concurrent number of this process (optional), no limit if not configured\nconcurrent = 1\n\n\n# a simple example configuration to start just one wdpost sealing_thread is as follows:\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-usa-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\n# the time interval for trying to claim tasks, the default is 60s,\n# for wdpost plan, we can reduce this value to get new wdpost tasks faster\nsealing.recover_interval = "15s"\n# sealing.allowed_miners = [6666]\n# sealing. allowed_sizes = ["32gib"]\n#...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n\n# example configuration for two wdpost job with one gpu\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-usa-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [6666]\n# ...\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [7777]\n# ...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n[[attached]]\nname = "miner-7777-store"\nlocation = "/mnt/miner-7777-store"\n\n# -------------------------\n\n[[processors.window_post]]\nbin="~/my_algorithm"\n# args = ["window_post", ...]\ncgroup.cpuset = "10-19"\nenvs = { cuda_visible_devices="0", tmpdir = "/tmp/worker-prover1/", ... }\nconcurrent = 1\n\n[[processors.window_post]]\nbin="~/my_algorithm"\n# args = ["window_post", ...]\ncgroup.cpuset = "20-29"\nenvs = { cuda_visible_devices="0", tmpdir = "/tmp/worker-prover1/", ... }\nconcurrent = 1\n\n\ndoing two wdpost jobs with one gpu can greatly improve utilization. as each wdpost job is composed of vanilla_proofs (cpu computation) and snark_proof (gpu computation), improved gpu utilization can be achieved through allowing vanilla_proofs to be computed in parallel and having snark_proof to be computed serially using gpu lock file in tmpdir.\n\n# an example of a wdpost machine equipped with two graphics cards\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-usa-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [6666]\n#...\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [7777]\n#...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n[[attached]]\nname = "miner-7777-store"\nlocation = "/mnt/miner-7777-store"\n\n# -------------------------\n\n[[processors. window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post", ...]\n# cgroup.cpuset = "10-19"\nenvs = { ... }\nconcurrent = 2\n\n# ----------- or ---------\n\n#[[processors. window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post", ...]\n# cgroup.cpuset = "10-19"\n# envs = { cuda_visible_devices="0", tmpdir = "/tmp/worker-prover1/",, ... }\n# concurrent = 1\n\n# [[processors. window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post"]\n# cgroup.cpuset = "20-29"\n# envs = { cuda_visible_devices="1", tmpdir = "/tmp/worker-prover2/", ... }\n# concurrent = 1\n\n\nwhen damocles-worker is running wdpost plan, it is not necessary to use the damocles-worker store sealing-init -l command to initialize the local storage directory of the data.\n\n\n# manage window post tasks\n\n * # show window post task list\n\n# by default, unfinished tasks and failed tasks are displayed, where the ddl field represents the deadline index of the task, and the try field is the number of attempts of the task\n./damocles-manager util worker wdpost list\n\njobid minerid ddl partitions worker state try createat elapsed heartbeat error\n3fgfenvrub1 1037 3 1,2 10.122.63.30 readytorun 1 07-27 16:37:31 - -\ngbcvh4tugef 1037 2 1,2 readytorun 0 07-27 16:35:56 - -\ncrotwclaxla 1037 1 1,2 10.122.63.30 succeed 1 07-27 17:19:04 6m38s(done) -\n\n# show all tasks\n./damocles-manager util worker wdpost list --all\n#...\n\n# show window post task details\n./damocles-manager util worker wdpost list --detail\n#...\n\n\n * # reset task\n\nwhen the execution of the window post task fails and the number of automatic retries reaches the limit, the task status can be manually reset so that it can continue to be picked up and executed by damocles-worker.\n\n./damocles-manager util worker wdpost reset gbcvh4tugef 3fgfenvrub1\n\n\n * # delete task\n\ndeleting a task is similar to resetting a task. when the command to delete a task is executed, the retry mechanism of damocles-manager will detect whether the window post task of the current deadline exists in the database, if not, it will resend the task and record it in the database.\n\nin addition, worker-prover will automatically delete tasks that have been created for more than a certain period of time (the default is 25 hours, and the time is configurable).\n\n# delete the specific task\n./damocles-manager util worker wdpost remove gbcvh4tugef 3fgfenvrub1\n\n# delete all tasks\n./damocles-manager util worker wdpost remove-all --really-do-it\n\n\n\n# disable damocles-manager to use gpu\n\nafter enabling the worker-prover feature, winning_post is executed by damocles-manager. if you do not want winning_post to use the gpu, you can compile damocles-manager with the following command to disable it:\n\nmake dist-clean\nffi_build_from_source=1 ffi_use_gpu=0 make build-manager\n\n\n\n# proxy node mode\n\nwe know that for poster nodes, the most important capability is to obtain real-time and accurate sector location information. in the current damocles-manager version, we only provide metadata management based on the local embedded kv database (more to be supported).\n\nthis only allows data to be managed by one process, and direct data sharing across processes is not possible.\n\ntherefore, we designed the proxy node mode to provide some metadata to other consumer nodes through network interfaces, thus realizing data sharing.\n\n\n# how to use the proxy node\n\nwe have added the --proxy parameter to the daemon run command. its format is like {ip}:{port}. when the startup command contains a valid --proxy parameter, {ip}:{port} will be used as data source for the current damocles-manager node and the necessary metadata (read-only) management module will be constructed.\n\nin addition to --proxy, we also provide switches that control whether proxy mode is enabled for specific data management modules.\n\nwe just provide --proxy-sector-indexer-off switch for the time being. when --proxy-sector-indexer-off is enabled, nodes use the sectorindexer database in their own data directory.\n\nfor example, if started with the damocles-manager daemon run --miner command, it will launch a damocles-manager instance listening on port 1789 using ~/.damocles-manager as the data directory with mining module enabled.\n\nat this time, we can use the following command to initialize and start a proxy node with the above instance as the data source on the same machine. this proxy node will use ~/.damocles-manager2 as the data directory and listen to 2789 port.\n\ndamocles-manager --home ~/.damocles-manager2 daemon init\n// maintain configuration files\ndamocles-manager --home ~/.damocles-manager2 daemon run --proxy="127.0.0.1:1789" --listen=":2789" --poster\n\n\nthe proxy node can provide the exact same and real-time sector location information as the source node.\n\n\n# the agent node uses the existing configuration file\n\naccording to the method described in the previous section, we can already start an proxy node, but there is still a problem with this startup method: the configuration file of the proxy node needs to be written again, or copied from the data directory of the source node. this introduces additional maintenance work, especially when configuration files may change frequently.\n\nfor this, we also provide a --conf-dir parameter, which is in the form of a directory path. when the startup command includes a valid --conf-dir parameter, the node will use the configuration file that already exists in the specified directory as its own configuration file.\n\nthis saves the work of writing and maintaining configuration files for different source and agent nodes on the same machine and serving the same set of clusters.\n\nbased on this function, the agent node startup method mentioned in the previous section can become:\n\ndamocles-manager --home ~/.damocles-manager2 daemon run --proxy="127.0.0.1:1789" --listen=":2789" --conf-dir="~/.damocles-manager" --poster\n\n\nat this point, the source node and the proxy node will use the same batch of configuration files.\n\n\n# ext-prover executor\n\nin addition to sharing sector information, another challenge faced by standalone poster nodes is the utilization of hardware resources.\n\nlimited by the underlying algorithm library, granularity of computing nodes utilizing gpus is by process. this makes it difficult for poster nodes to effectively utilize the computing power of multiple gpus, and it is also difficult to safely avoid proof timeouts when multiple sps have conflicting windostpost proof windows.\n\nfor this, we provide an ext-prover mechanism similar to the ext processor in damocles-worker.\n\nthe ext-prover mechanism consists of two components:\n\n 1. the --ext-prover parameter of the daemon run command\n 2. the ext-prover.cfg configuration file in the node data directory\n\na default ext-prover.cfg file looks like:\n\n# default config:\n#[[wdpost]]\n#bin = "/path/to/custom/bin"\n#args = ["args1", "args2", "args3"]\n#concurrent = 1\n#weight = 1\n#readytimeoutsecs = 5\n#[wdpost.envs]\n#env_key = "env_val"\n#\n#[[winpost]]\n#bin = "/path/to/custom/bin"\n#args = ["args1", "args2", "args3"]\n#concurrent = 1\n#weight = 1\n#readytimeoutsecs = 5\n#[winpost.envs]\n#env_key = "env_val"\n#\n\n\nin recent versions, daemon init initializes the ext-prover.cfg file.\n\nusers can write their own, or copy the corresponding files from a data directory initialized by the latest version to an existing data directory.\n\nthe functions of the configuration items in ext-prover.cfg are very similar to the configuration blocks in damocles-worker, and users can refer to the corresponding documents for reference.\n\nwhen the --ext-prover parameter is included in the start command of damocles-manager, the node will use the ext-prover.cfg configuration file in the configuration directory as the basis for starting child processes. for this configuration file, setting the --conf-dir parameter will also have an effect.\n\nif user sees logs like the following, then it means that ext-prover is ready.\n\n2022-04-27t19:15:00.441+0800 info porver-ext ext/prover.go:122 response loop start {"pid": 24764, "ppid": 24732, "loop": "resp"}\n2022-04-27t19:15:00.441+0800 info porver-ext ext/prover.go:155 request loop start {"pid": 24764, "ppid": 24732, "loop": "req"}\n2022-04-27t19:15:00.468+0800 info processor-cmd processor/processor.go:35 ready {"pid": 24764, "ppid": 24732, "proc": "wdpost"}\n\n\n\n# deployment practice\n\nsuppose we have a node machine with 8 gpus, then we can provide stronger post processing capabilities through the following configuration.\n\n 1. configure and start the source node\n    \n    damocles-manager daemon run --miner\n    \n    \n    at this time, the source node only provides functions and capabilities related to sealing;\n\n 2. configure the ext-prover.cfg file:\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "0"\n    tmpdir = "/tmp/ext-prover0/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "1"\n    tmpdir = "/tmp/ext-prover1/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "2"\n    tmpdir = "/tmp/ext-prover2/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "3"\n    tmpdir = "/tmp/ext-prover3/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "4"\n    tmpdir = "/tmp/ext-prover4/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "5"\n    tmpdir = "/tmp/ext-prover5/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "6"\n    tmpdir = "/tmp/ext-prover6/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "7"\n    tmpdir = "/tmp/ext-prover7/"\n    \n    \n\n 3. initialize and start a standalone poster node\n    \n    damocles-manager --home=~/.damocles-individual-poster daemon init\n    damocles-manager --home=~/.damocles-individual-poster daemon run --proxy="127.0.0.1:1789" --poster --listen=":2789" --conf-dir="~/.damocles-manager" --ext-prover\n    \n\nby this way of deployment,\n\n * the source node provides both sealing and mining support\n * proxy nodes provide windowpost support\n   * the proxy node enables ext-prover, and each child process independently uses a gpu and a computing lock directory\n\nthere is no conflict between winningpost and windowpost due to device usage\n\n\n# limitations\n\nso far, we have described the functions, principles and simple usage examples that stand-alone poster nodes rely on.\n\nhowever, this mode still has some limitations for very large sp clusters, which may manifest in:\n\n * the scheduling of the post and the serious conflict in the post window period still relies on the operation and maintenance to a certain extent;',charsets:{cjk:!0}},{title:"SnapDeal Support",frontmatter:{},regularPath:"/operation/snapup.html",relativePath:"operation/snapup.md",key:"v-3ece3291",path:"/operation/snapup.html",headers:[{level:2,title:"SnapDeal Overview",slug:"snapdeal-overview",normalizedTitle:"snapdeal overview",charIndex:23},{level:2,title:"Damocles SnapDeal Support",slug:"damocles-snapdeal-support",normalizedTitle:"damocles snapdeal support",charIndex:592},{level:2,title:"Example",slug:"example",normalizedTitle:"example",charIndex:1286},{level:3,title:"Importing Candidate Sectors",slug:"importing-candidate-sectors",normalizedTitle:"importing candidate sectors",charIndex:1431},{level:3,title:"Monitoring Remaining Candidate Sector",slug:"monitoring-remaining-candidate-sector",normalizedTitle:"monitoring remaining candidate sector",charIndex:2047},{level:3,title:"Configuring damocles-worker",slug:"configuring-damocles-worker",normalizedTitle:"configuring damocles-worker",charIndex:2402},{level:3,title:"Configuring damocles-manager",slug:"configuring-damocles-manager",normalizedTitle:"configuring damocles-manager",charIndex:3830},{level:3,title:"Tips",slug:"tips",normalizedTitle:"tips",charIndex:4441},{level:2,title:"Continuous Improvement",slug:"continuous-improvement",normalizedTitle:"continuous improvement",charIndex:5293}],headersStr:"SnapDeal Overview Damocles SnapDeal Support Example Importing Candidate Sectors Monitoring Remaining Candidate Sector Configuring damocles-worker Configuring damocles-manager Tips Continuous Improvement",content:'# SnapDeal Support\n\n\n# SnapDeal Overview\n\nSnapDeal is a sector upgrade proposal introduced in FIP-19 and merged in nv15. Compared to previous upgrade proposals that required a complete re-seal process, SnapDeal is relatively lightweight. Its cost is approximately:\n\n * Completing an add piece task\n * Completing a tree d task\n * Completing a snap_encode task, which costs about the same as a pc2 task\n * Completing a snap_prove task, which costs about the same as a c1 + c2 task\n\nTherefore, SnapDeal is attractive for both first-time data onboarding and upgrading of existing CC sectors.\n\n\n# Damocles SnapDeal Support\n\nDamocles is designed to provide a way to mass produce storage power. For this purpose, we have introduced a SnapDeal production scheme that requires minimal human intervention, which we call SnapUp. The steps involved are as follows:\n\n 1. Import existing CC sectors in batches as candidate sectors.\n 2. Configure damocles-manager to enable SnapUp mode for specified SPs.\n 3. Configure sealing_thread of damocles-worker to SnapUp production plans or add new sealing_thread for SnapUp.\n\nThroughout this process, users only need to focus on importing new candidate sectors and monitoring the remaining of candidate sectors, as all other processes will be automated.\n\n\n# Example\n\nLet\'s use an example of a production cluster on the butterfly network to demonstrate how to configure the SnapUp production scheme.\n\n\n# Importing Candidate Sectors\n\nUsing the newly added util sealer snap fetch tool, you can import CC sectors that meet the SnapUp condition (remaining lifetime greater than 180 days and satisfying the minimum lifetime requirement for storage deals) as candidate sectors per their deadlines.\n\n./dist/bin/damocles-manager util sealer snap fetch 1153 3\n2022-04-15T04:28:03.380Z        DEBUG   policy  policy/const.go:18      NETWORK SETUP   {"name": "butterfly"}\n2022-04-15T04:28:03.401Z        INFO    cmd     internal/util_sealer_snap.go:53 candidate sectors fetched        {"available-in-deadline": 2, "added": 2}\n\n\n\n# Monitoring Remaining Candidate Sector\n\n./dist/bin/damocles-manager util sealer snap candidates 1153\n2022-04-15T04:28:13.955Z        DEBUG   policy  policy/const.go:18      NETWORK SETUP   {"name": "butterfly"}\ndeadline  count\n3         2\n\n\nIn the above example, there are currently 2 CC sectors available in the #3 deadline as candidates for upgrade.\n\n\n# Configuring damocles-worker\n\nIn damocles-worker, the main configurations required related to SnapUp job are the resource allocation for snap_encode and snap_prove tasks in sealing_thread.\n\nHere is an example configuration:\n\n[[sealing_thread]]\nlocation = "/data/local-snap-1"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-2"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-3"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-4"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-5"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-6"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-6"\nplan = "snapup"\n\n[processors.limitation.concurrent]\n# ...\ntree_d = 1\nsnap_encode = 5\nsnap_prove = 2\n\n[[processors.snap_encode]]\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0", TMPDIR="/var/tmp/worker0" }\n\n[[processors.snap_prove]]\nenvs = { CUDA_VISIBLE_DEVICES = "0" , TMPDIR="/var/tmp/worker0" }\n\n[[processors.snap_encode]]\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0", TMPDIR="/var/tmp/worker1" }\n\n[[processors.snap_prove]]\nenvs = { CUDA_VISIBLE_DEVICES = "1", TMPDIR="/var/tmp/worker1" }\n\n\nThe resource allocation for snap_encode can be similar to pc2, and for snap_prove, it can be similar to c2.\n\n\n# Configuring damocles-manager\n\nIn damocles-manager, the required configuration mainly involves enabling SnapUp for specific SPs. Here is an example:\n\n[[Miners]]\nActor = 1153\n[Miners.Sector]\nInitNumber = 0\nMaxNumber = 10000\nEnabled = true\nEnableDeals = false\n\n[Miners.SnapUp]\nEnabled = true\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n\nIn this configuration:\n\n * The settings in [Miners.Sector] do not affect SnapUp production.\n * Under this example configuration, you would get...\n   * continuous production of CC Sector\n   * SnapUp continues production while there are available candidate sectors\n\n\n# Tips\n\n * Considering the computing resources required by snap_encode and snap_prove, if you enable regular sector sealing and SnapUp at the same time in the same damocles-worker instance, you may encounter race condition of hardware resource. You can refer to 07.damocles-worker external executor configuration example (En doc to be updated).\n\n * Considering the deployment of sector persistent storage, the damocles-worker used for SnapUp needs to be able to have both read and write access to all persistent storage spaces (persist stores), and ensure that their names are consistent with those defined in damocles-manager.\n\n * Based on the above two points, we recommend using a separate box specifically for SnapUp production to avoid the configuration, operation and maintenance complexity caused by mixing regular sectors and SnapUp sealing.\n\n\n# Continuous Improvement\n\nThe improvement and optimization of the SnapUp solution are still in progress. Currently we are mainly focusing on:\n\n * Convert semi-automatic candidate sector import to automatic mode, or provide equivalent operation and maintenance tools\n * More candidate sector import rules, such as import by storage configuration\n * Aggregation of on-chain messages to reduce costs\n * Other optimizations and tooling that can simplify operation & maintenance, reduce costs, and improve efficiency',normalizedContent:'# snapdeal support\n\n\n# snapdeal overview\n\nsnapdeal is a sector upgrade proposal introduced in fip-19 and merged in nv15. compared to previous upgrade proposals that required a complete re-seal process, snapdeal is relatively lightweight. its cost is approximately:\n\n * completing an add piece task\n * completing a tree d task\n * completing a snap_encode task, which costs about the same as a pc2 task\n * completing a snap_prove task, which costs about the same as a c1 + c2 task\n\ntherefore, snapdeal is attractive for both first-time data onboarding and upgrading of existing cc sectors.\n\n\n# damocles snapdeal support\n\ndamocles is designed to provide a way to mass produce storage power. for this purpose, we have introduced a snapdeal production scheme that requires minimal human intervention, which we call snapup. the steps involved are as follows:\n\n 1. import existing cc sectors in batches as candidate sectors.\n 2. configure damocles-manager to enable snapup mode for specified sps.\n 3. configure sealing_thread of damocles-worker to snapup production plans or add new sealing_thread for snapup.\n\nthroughout this process, users only need to focus on importing new candidate sectors and monitoring the remaining of candidate sectors, as all other processes will be automated.\n\n\n# example\n\nlet\'s use an example of a production cluster on the butterfly network to demonstrate how to configure the snapup production scheme.\n\n\n# importing candidate sectors\n\nusing the newly added util sealer snap fetch tool, you can import cc sectors that meet the snapup condition (remaining lifetime greater than 180 days and satisfying the minimum lifetime requirement for storage deals) as candidate sectors per their deadlines.\n\n./dist/bin/damocles-manager util sealer snap fetch 1153 3\n2022-04-15t04:28:03.380z        debug   policy  policy/const.go:18      network setup   {"name": "butterfly"}\n2022-04-15t04:28:03.401z        info    cmd     internal/util_sealer_snap.go:53 candidate sectors fetched        {"available-in-deadline": 2, "added": 2}\n\n\n\n# monitoring remaining candidate sector\n\n./dist/bin/damocles-manager util sealer snap candidates 1153\n2022-04-15t04:28:13.955z        debug   policy  policy/const.go:18      network setup   {"name": "butterfly"}\ndeadline  count\n3         2\n\n\nin the above example, there are currently 2 cc sectors available in the #3 deadline as candidates for upgrade.\n\n\n# configuring damocles-worker\n\nin damocles-worker, the main configurations required related to snapup job are the resource allocation for snap_encode and snap_prove tasks in sealing_thread.\n\nhere is an example configuration:\n\n[[sealing_thread]]\nlocation = "/data/local-snap-1"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-2"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-3"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-4"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-5"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-6"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-6"\nplan = "snapup"\n\n[processors.limitation.concurrent]\n# ...\ntree_d = 1\nsnap_encode = 5\nsnap_prove = 2\n\n[[processors.snap_encode]]\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0", tmpdir="/var/tmp/worker0" }\n\n[[processors.snap_prove]]\nenvs = { cuda_visible_devices = "0" , tmpdir="/var/tmp/worker0" }\n\n[[processors.snap_encode]]\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0", tmpdir="/var/tmp/worker1" }\n\n[[processors.snap_prove]]\nenvs = { cuda_visible_devices = "1", tmpdir="/var/tmp/worker1" }\n\n\nthe resource allocation for snap_encode can be similar to pc2, and for snap_prove, it can be similar to c2.\n\n\n# configuring damocles-manager\n\nin damocles-manager, the required configuration mainly involves enabling snapup for specific sps. here is an example:\n\n[[miners]]\nactor = 1153\n[miners.sector]\ninitnumber = 0\nmaxnumber = 10000\nenabled = true\nenabledeals = false\n\n[miners.snapup]\nenabled = true\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n\nin this configuration:\n\n * the settings in [miners.sector] do not affect snapup production.\n * under this example configuration, you would get...\n   * continuous production of cc sector\n   * snapup continues production while there are available candidate sectors\n\n\n# tips\n\n * considering the computing resources required by snap_encode and snap_prove, if you enable regular sector sealing and snapup at the same time in the same damocles-worker instance, you may encounter race condition of hardware resource. you can refer to 07.damocles-worker external executor configuration example (en doc to be updated).\n\n * considering the deployment of sector persistent storage, the damocles-worker used for snapup needs to be able to have both read and write access to all persistent storage spaces (persist stores), and ensure that their names are consistent with those defined in damocles-manager.\n\n * based on the above two points, we recommend using a separate box specifically for snapup production to avoid the configuration, operation and maintenance complexity caused by mixing regular sectors and snapup sealing.\n\n\n# continuous improvement\n\nthe improvement and optimization of the snapup solution are still in progress. currently we are mainly focusing on:\n\n * convert semi-automatic candidate sector import to automatic mode, or provide equivalent operation and maintenance tools\n * more candidate sector import rules, such as import by storage configuration\n * aggregation of on-chain messages to reduce costs\n * other optimizations and tooling that can simplify operation & maintenance, reduce costs, and improve efficiency',charsets:{}},{title:"Task status flow",frontmatter:{},regularPath:"/operation/task-flow.html",relativePath:"operation/task-flow.md",key:"v-ef0bd702",path:"/operation/task-flow.html",headers:[{level:2,title:"The state flow of sealer tasks",slug:"the-state-flow-of-sealer-tasks",normalizedTitle:"the state flow of sealer tasks",charIndex:1527},{level:2,title:"Upgrade (snapup) task status flow",slug:"upgrade-snapup-task-status-flow",normalizedTitle:"upgrade (snapup) task status flow",charIndex:3839},{level:2,title:"The state flow of rebuild tasks",slug:"the-state-flow-of-rebuild-tasks",normalizedTitle:"the state flow of rebuild tasks",charIndex:4869},{level:2,title:"Example usage in combination with worker management tools",slug:"example-usage-in-combination-with-worker-management-tools",normalizedTitle:"example usage in combination with worker management tools",charIndex:6628}],headersStr:"The state flow of sealer tasks Upgrade (snapup) task status flow The state flow of rebuild tasks Example usage in combination with worker management tools",content:"# Task status flow\n\nUnderstanding the status flow of tasks will help users understand the status of damocles-worker and perform targeted recovery of paused sector tasks.\n\nThe status flow of the task is related to the type of the task, that is, the plan option in sealing_thread. We will elaborate them separately .\n\nIn the description below, entries prefixed with State:: are states, and entries prefixed with Event:: are events or procedures.\n\nState::A => {\n\tEvent::B => State::C,\n\tEvent::D => State::E,\n}\n\n\nmeans when the task is in A state\n\n 1. If B event occurs, then go to C state\n 2. If D event occurs, then go to E state\n\nIn addition…\n\n 1. For each state, transitions to one or more other states may occur, that is, one or more lines may appear in {}\n\n 2. In addition to the specific states listed, there are also some special states, such as:\n\n * State::Aborted, indicating that the sector did not complete normally\n   \n   1. In any logic evaluations, an exception belonging to the abort category will cause the task to change to the Aborted state. The task will be terminated, and the current sealing_thread will move onto the next task\n   \n   2. When the user sends the resume command to a paused task with the Aborted state is attached, the flow described above will still be taken into effect\n      \n      Therefore, users can use this mechanism to handle exceptions that are difficult to recover, but have not yet been defined as abort category\n\n * State::Finished indicates that the sector completed normally\n\n\n# The state flow of sealer tasks\n\n// Empty state, that is, the sector has not been allocated yet\nState::Empty => {\n\t// apply for a new sector\n\tEvent::Allocate(_) => State::Allocated,\n},\n\n// new sector is allocated\nState::Allocated => {\n\t// request a deal\n\tEvent::AcquireDeals(_) => State::DealsAcquired,\n},\n\n// deal has been applied\nState::DealsAcquired => {\n\t// fill piece data\n\tEvent::AddPiece(_) => State::PieceAdded,\n},\n\n// data is filled\nState::PieceAdded => {\n\t// construct TreeD\n\tEvent::BuildTreeD => State::TreeDBuilt,\n},\n\n// TreeD has been generated\nState::TreeDBuilt => {\n\t// Request on-chain random seed required by pc1\n\tEvent::AssignTicket(_) => State::TicketAssigned,\n},\n\n// The on-chain random seed required by pc1 has been obtained\nState::TicketAssigned => {\n\t// execute pc1\n\tEvent::PC1(_, _) => State::PC1Done,\n},\n\n// pc1 is done\nState::PC1Done => {\n\t// execute pc2\n\tEvent::PC2(_) => State::PC2Done,\n},\n\n// pc2 is done\nState::PC2Done => {\n\t// Submit PreCommit on-chain information\n\tEvent::SubmitPC => State::PCSubmitted,\n},\n\n// PreCommit on-chain information has been submitted\nState::PCSubmitted => {\n\t// Failed to submit, need to resubmit\n\tEvent::ReSubmitPC => State::PC2Done,\n\t// Successfully submitted message\n\tEvent::CheckPC => State::PCLanded,\n},\n\n// PreCommit information has been submitted \nState::PCLanded => {\n\t// Perform persistence of sector files\n\tEvent::Persist(_) => State::Persisted,\n},\n\n// sector files are persisted\nState::Persisted => {\n\t// Pass persistent files check\n\tEvent::SubmitPersistance => State::PersistanceSubmitted,\n},\n\n// Persistence files are confirmed\nState::PersistanceSubmitted => {\n\t// Request on-chain random seed required for c1\n\tEvent::AssignSeed(_) => State::SeedAssigned,\n},\n\n// The on-chain random seed required by c1 has been obtained\nState::SeedAssigned => {\n\t// execute c1\n\tEvent::C1(_) => State::C1Done,\n},\n\n// c1 is done\nState::C1Done => {\n\t// execute c2\n\tEvent::C2(_) => State::C2Done,\n},\n\n// C2 is done\nState::C2Done => {\n\t// Submit CommitProof information\n\tEvent::SubmitProof => State::ProofSubmitted,\n},\n\n// CommitProof information has been submitted\nState::ProofSubmitted => {\n\t// Failed to submit, need to resubmit\n\tEvent::ReSubmitProof => State::C2Done,\n\t// Successful on-chain or skip on-chain check\n\tEvent::Finish => State::Finished,\n},\n\n\n\n# Upgrade (snapup) task status flow\n\n// Empty state, that is, the sector has not been allocated yet\nState::Empty => {\n\t// Allocate sector and deals for upgrade\n\tEvent::AllocatedSnapUpSector(_, _, _) => State::Allocated,\n},\n\n// Sector and deal for upgrade are allocated\nState::Allocated => {\n\t// fill piece data\n\tEvent::AddPiece(_) => State::PieceAdded,\n},\n\n// data is filled\nState::PieceAdded => {\n\t// construct TreeD\n\tEvent::BuildTreeD => State::TreeDBuilt,\n},\n\n// TreeD has been generated\nState::TreeDBuilt => {\n\t// perform Snap encoding\n\tEvent::SnapEncode(_) => State::SnapEncoded,\n},\n\n// Snap encoding is done\nState::SnapEncoded => {\n\t// Compute Snap proof\n\tEvent::SnapProve(_) => State::SnapProved,\n},\n\n// Snap proof has been generated\nState::SnapProved => {\n\t// Perform persistence of sector files\n\tEvent::Persist(_) => State::Persisted,\n},\n\n// sector files are persisted\nState::Persisted => {\n\t// files fails check\n\tEvent::RePersist => State::SnapProved,\n\t// files passed the check\n\tEvent::Finish => State::Finished,\n},\n\n\n\n# The state flow of rebuild tasks\n\n// Empty state, that is, the sector has not been allocated yet\nState::Empty => {\n\t// Allocate sector info for rebuild\n\tEvent::AllocatedRebuildSector(_) => State::Allocated,\n},\n\n// Sector info for rebuild are allocated\nState::Allocated => {\n\t// fill piece data\n\tEvent::AddPiece(_) => State::PieceAdded,\n},\n\n// Data is filled\nState::PieceAdded => {\n\t// Construct TreeD\n\tEvent::BuildTreeD => State::TreeDBuilt,\n},\n\n// TreeD has been generated\nState::TreeDBuilt => {\n\t// Execute pc1\n\tEvent::PC1(_, _) => State::PC1Done,\n},\n\n// PC1 is done\nState::PC1Done => {\n\t// Execute pc2\n\tEvent::PC2(_) => State::PC2Done,\n},\n\n// PC2 is done\nState::PC2Done => {\n\t// Check sealed file (execute c1)\n\tEvent::CheckSealed => State::SealedChecked,\n},\n\n// Sealed file is checked (c1 is done)\nState::SealedChecked => {\n\t// Non upgrade sector, skip snapup steps\n\tEvent::SkipSnap => State::SnapDone,\n\t// Upgrade sector, fill piece data\n\tEvent::AddPiece(_) => State::SnapPieceAdded,\n},\n\n// Data is filled (only upgrade sector)\nState::SnapPieceAdded => {\n\t// Construct TreeD (only upgrade sector)\n\tEvent::BuildTreeD => State::SnapTreeDBuilt,\n},\n\n// TreeD has been generated (only upgrade sector)\nState::SnapTreeDBuilt => {\n\t// Perform Snap encoding (only upgrade sector)\n\tEvent::SnapEncode(_) => State::SnapEncoded,\n},\n\n// Snap encoding is done (only upgrade sector)\nState::SnapEncoded => {\n\t// Compute Snap proof (only upgrade sector)\n\tEvent::SnapProve(_) => State::SnapDone,\n},\n\n// snapup is done or not needed\nState::SnapDone => {\n\t// Perform persistence of sector files\n\tEvent::Persist(_) => State::Persisted,\n},\n\n// Sector files are persisted\nState::Persisted => {\n\t// Pass persistent files check\n\tEvent::SubmitPersistance => State::Finished,\n},\n\n\n\n# Example usage in combination with worker management tools\n\n# 1. For a sector sealing task that has been paused due to an error and cannot be resumed, such as the ticket has expired, you can use…\n\ndamocles-worker worker resume --state Aborted --index <index>\n\n\nor\n\ndamocles-manager util worker resume <worker instance name or address> <thread index> Aborted\n\n\nto terminate the current task.\n\n# 2. For a sector sealing task that has been paused with an error, but it is considered that the task can be reattempted from a previous state. For example, if the ticket has expired, you can use…\n\ndamocles-worker worker resume --state TreeDBuilt --index <index>\n\n\nor\n\ndamocles-manager util worker resume <worker instance name or address> <thread index> TreeDBuilt\n\n\nto let it fall back to the previous state and retry.",normalizedContent:"# task status flow\n\nunderstanding the status flow of tasks will help users understand the status of damocles-worker and perform targeted recovery of paused sector tasks.\n\nthe status flow of the task is related to the type of the task, that is, the plan option in sealing_thread. we will elaborate them separately .\n\nin the description below, entries prefixed with state:: are states, and entries prefixed with event:: are events or procedures.\n\nstate::a => {\n\tevent::b => state::c,\n\tevent::d => state::e,\n}\n\n\nmeans when the task is in a state\n\n 1. if b event occurs, then go to c state\n 2. if d event occurs, then go to e state\n\nin addition…\n\n 1. for each state, transitions to one or more other states may occur, that is, one or more lines may appear in {}\n\n 2. in addition to the specific states listed, there are also some special states, such as:\n\n * state::aborted, indicating that the sector did not complete normally\n   \n   1. in any logic evaluations, an exception belonging to the abort category will cause the task to change to the aborted state. the task will be terminated, and the current sealing_thread will move onto the next task\n   \n   2. when the user sends the resume command to a paused task with the aborted state is attached, the flow described above will still be taken into effect\n      \n      therefore, users can use this mechanism to handle exceptions that are difficult to recover, but have not yet been defined as abort category\n\n * state::finished indicates that the sector completed normally\n\n\n# the state flow of sealer tasks\n\n// empty state, that is, the sector has not been allocated yet\nstate::empty => {\n\t// apply for a new sector\n\tevent::allocate(_) => state::allocated,\n},\n\n// new sector is allocated\nstate::allocated => {\n\t// request a deal\n\tevent::acquiredeals(_) => state::dealsacquired,\n},\n\n// deal has been applied\nstate::dealsacquired => {\n\t// fill piece data\n\tevent::addpiece(_) => state::pieceadded,\n},\n\n// data is filled\nstate::pieceadded => {\n\t// construct treed\n\tevent::buildtreed => state::treedbuilt,\n},\n\n// treed has been generated\nstate::treedbuilt => {\n\t// request on-chain random seed required by pc1\n\tevent::assignticket(_) => state::ticketassigned,\n},\n\n// the on-chain random seed required by pc1 has been obtained\nstate::ticketassigned => {\n\t// execute pc1\n\tevent::pc1(_, _) => state::pc1done,\n},\n\n// pc1 is done\nstate::pc1done => {\n\t// execute pc2\n\tevent::pc2(_) => state::pc2done,\n},\n\n// pc2 is done\nstate::pc2done => {\n\t// submit precommit on-chain information\n\tevent::submitpc => state::pcsubmitted,\n},\n\n// precommit on-chain information has been submitted\nstate::pcsubmitted => {\n\t// failed to submit, need to resubmit\n\tevent::resubmitpc => state::pc2done,\n\t// successfully submitted message\n\tevent::checkpc => state::pclanded,\n},\n\n// precommit information has been submitted \nstate::pclanded => {\n\t// perform persistence of sector files\n\tevent::persist(_) => state::persisted,\n},\n\n// sector files are persisted\nstate::persisted => {\n\t// pass persistent files check\n\tevent::submitpersistance => state::persistancesubmitted,\n},\n\n// persistence files are confirmed\nstate::persistancesubmitted => {\n\t// request on-chain random seed required for c1\n\tevent::assignseed(_) => state::seedassigned,\n},\n\n// the on-chain random seed required by c1 has been obtained\nstate::seedassigned => {\n\t// execute c1\n\tevent::c1(_) => state::c1done,\n},\n\n// c1 is done\nstate::c1done => {\n\t// execute c2\n\tevent::c2(_) => state::c2done,\n},\n\n// c2 is done\nstate::c2done => {\n\t// submit commitproof information\n\tevent::submitproof => state::proofsubmitted,\n},\n\n// commitproof information has been submitted\nstate::proofsubmitted => {\n\t// failed to submit, need to resubmit\n\tevent::resubmitproof => state::c2done,\n\t// successful on-chain or skip on-chain check\n\tevent::finish => state::finished,\n},\n\n\n\n# upgrade (snapup) task status flow\n\n// empty state, that is, the sector has not been allocated yet\nstate::empty => {\n\t// allocate sector and deals for upgrade\n\tevent::allocatedsnapupsector(_, _, _) => state::allocated,\n},\n\n// sector and deal for upgrade are allocated\nstate::allocated => {\n\t// fill piece data\n\tevent::addpiece(_) => state::pieceadded,\n},\n\n// data is filled\nstate::pieceadded => {\n\t// construct treed\n\tevent::buildtreed => state::treedbuilt,\n},\n\n// treed has been generated\nstate::treedbuilt => {\n\t// perform snap encoding\n\tevent::snapencode(_) => state::snapencoded,\n},\n\n// snap encoding is done\nstate::snapencoded => {\n\t// compute snap proof\n\tevent::snapprove(_) => state::snapproved,\n},\n\n// snap proof has been generated\nstate::snapproved => {\n\t// perform persistence of sector files\n\tevent::persist(_) => state::persisted,\n},\n\n// sector files are persisted\nstate::persisted => {\n\t// files fails check\n\tevent::repersist => state::snapproved,\n\t// files passed the check\n\tevent::finish => state::finished,\n},\n\n\n\n# the state flow of rebuild tasks\n\n// empty state, that is, the sector has not been allocated yet\nstate::empty => {\n\t// allocate sector info for rebuild\n\tevent::allocatedrebuildsector(_) => state::allocated,\n},\n\n// sector info for rebuild are allocated\nstate::allocated => {\n\t// fill piece data\n\tevent::addpiece(_) => state::pieceadded,\n},\n\n// data is filled\nstate::pieceadded => {\n\t// construct treed\n\tevent::buildtreed => state::treedbuilt,\n},\n\n// treed has been generated\nstate::treedbuilt => {\n\t// execute pc1\n\tevent::pc1(_, _) => state::pc1done,\n},\n\n// pc1 is done\nstate::pc1done => {\n\t// execute pc2\n\tevent::pc2(_) => state::pc2done,\n},\n\n// pc2 is done\nstate::pc2done => {\n\t// check sealed file (execute c1)\n\tevent::checksealed => state::sealedchecked,\n},\n\n// sealed file is checked (c1 is done)\nstate::sealedchecked => {\n\t// non upgrade sector, skip snapup steps\n\tevent::skipsnap => state::snapdone,\n\t// upgrade sector, fill piece data\n\tevent::addpiece(_) => state::snappieceadded,\n},\n\n// data is filled (only upgrade sector)\nstate::snappieceadded => {\n\t// construct treed (only upgrade sector)\n\tevent::buildtreed => state::snaptreedbuilt,\n},\n\n// treed has been generated (only upgrade sector)\nstate::snaptreedbuilt => {\n\t// perform snap encoding (only upgrade sector)\n\tevent::snapencode(_) => state::snapencoded,\n},\n\n// snap encoding is done (only upgrade sector)\nstate::snapencoded => {\n\t// compute snap proof (only upgrade sector)\n\tevent::snapprove(_) => state::snapdone,\n},\n\n// snapup is done or not needed\nstate::snapdone => {\n\t// perform persistence of sector files\n\tevent::persist(_) => state::persisted,\n},\n\n// sector files are persisted\nstate::persisted => {\n\t// pass persistent files check\n\tevent::submitpersistance => state::finished,\n},\n\n\n\n# example usage in combination with worker management tools\n\n# 1. for a sector sealing task that has been paused due to an error and cannot be resumed, such as the ticket has expired, you can use…\n\ndamocles-worker worker resume --state aborted --index <index>\n\n\nor\n\ndamocles-manager util worker resume <worker instance name or address> <thread index> aborted\n\n\nto terminate the current task.\n\n# 2. for a sector sealing task that has been paused with an error, but it is considered that the task can be reattempted from a previous state. for example, if the ticket has expired, you can use…\n\ndamocles-worker worker resume --state treedbuilt --index <index>\n\n\nor\n\ndamocles-manager util worker resume <worker instance name or address> <thread index> treedbuilt\n\n\nto let it fall back to the previous state and retry.",charsets:{}},{title:"damocles-worker task management",frontmatter:{},regularPath:"/operation/task-management.html",relativePath:"operation/task-management.md",key:"v-92b0af42",path:"/operation/task-management.html",headers:[{level:2,title:"damocles-worker local management",slug:"damocles-worker-local-management",normalizedTitle:"damocles-worker local management",charIndex:709},{level:3,title:"list",slug:"list",normalizedTitle:"list",charIndex:934},{level:3,title:"pause",slug:"pause",normalizedTitle:"pause",charIndex:942},{level:3,title:"resume",slug:"resume",normalizedTitle:"resume",charIndex:951},{level:2,title:"damocles-manager remote management of damocles-worker",slug:"damocles-manager-remote-management-of-damocles-worker",normalizedTitle:"damocles-manager remote management of damocles-worker",charIndex:4011},{level:3,title:"list",slug:"list-2",normalizedTitle:"list",charIndex:934},{level:3,title:"info / pause / resume",slug:"info-pause-resume",normalizedTitle:"info / pause / resume",charIndex:5304}],headersStr:"damocles-worker local management list pause resume damocles-manager remote management of damocles-worker list info / pause / resume",content:'# damocles-worker task management\n\nIn the previous document, we mentioned that in the damocles architecture, the process management of a sector is by worker.\n\nTherefore, the management of sector tasks, especially exception handling, is also performed by the worker instance where the sector is being processed.\n\nHowever, it must be very inconvenient if all status checking and exception handling require remote access to the corresponding worker machine to operate.\n\nTherefore, in v0.2.0 and later versions, workers report status to sector-manager, and sector-manager manages workers remotely.\n\nBelow, we will explain both how worker can be managed locally and how sector-manager manage workers remotely.\n\n\n# damocles-worker local management\n\nThe local management of damocles-worker is mainly through a set of tools provided to call the management interface to operate…\n\n./dist/bin/damocles-worker worker\n\n\nWith subcommands like…\n\n * list\n * pause\n * resume\n\n\n# list\n\nlist is used to list the current state of all sealing_threads in the currently running damocles-worker instance.\n\ndamocles-worker worker list\n\n\nLet\'s take the mock configuration in the codebase as an example:\n\n$ ./dist/bin/damocles-worker worker list\n\n#0: "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store1"; sector_id=Some(s-t010000-2), paused=true, paused_elapsed=Some (17s), state=C1Done, last_err=Some("permanent: No cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793/cointmpail/var finding [f3793/cointmpail/var] -proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]")\n#1: "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store2"; sector_id=Some(s-t010000-3), paused=true, paused_elapsed=Some (17s), state=C1Done, last_err=Some("permanent: No cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793/cointmpail/var finding [f3793/cointmpail/var] -proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]")\n#2: "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store3"; sector_id=Some(s-t010000-1), paused=true, paused_elapsed=Some (17s), state=C1Done, last_err=Some("permanent: No cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793/cointmpail/var finding [f3793/cointmpail/var] -proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]")\n\n\nAs you can see, for each sealing_thread , it will list\n\n * Index number\n * Local storage location information\n * Sector ID (if there is a sector task being processed)\n * If the task is paused\n * Paused time (if there are paused sector tasks)\n * current status\n * Last exception information (if there is a sector task suspended due to exception)\n\n\n# pause\n\npause is used to pause the sealing_thread with the specified index number.\n\n$ damocles-worker worker pause --index <index>\n\n\nNote that:\n\n * index needs to match the index number from list command.\n\n\n# resume\n\nresume is used to resume a suspended sealing_thread.\n\ndamocles-worker worker resume [--state <state>] --index <index>\n\n\nNote that:\n\n * index needs to match the index number from list command.\n * state is optional.\n\nIf state is not supplied, the sector will try to restart with the current state; if the correct state value is supplied, it will restart with the specified state\n\nFor different sealing_thread task types, the optional status values can be found in 11. Task Status Flow\n\n\n# damocles-manager remote management of damocles-worker\n\nThe management of damocles-worker by damocles-manager is mainly in two aspects:\n\n 1. Receive periodic report information of the worker instance\n 2. Call the management interface on the specified damocles-worker instance\n\nRemote management is done through a set of tools provided to call the management interface of damocles-manager, or a proxy call to the management interface of the specified damocles-worker.\n\n./dist/bin/damocles-manager util worker\n\n\nThe subcommands included are:\n\n * list\n * info\n * pause\n * resume\n\n\n# list\n\nThe list here is used to list the worker profiles that have reported information to this damocles-manager instance, for example:\n\n$ ./dist/bin/damocles-manager util worker list\nName Dest Threads Empty Paused Errors LastPing(with ! if expired)\n127.0.0.1 127.0.0.1:17890 3 0 3 3 2.756922465s\n\n\nAs you can see, for each instance, it will list:\n\n * instance name (if no instance name is specified, it will be the ip used to connect to damocles-manager)\n * instance connection information\n * sealing_thread number\n * The number of empty sealing_thread\n * The number of suspended sealing_thread\n * The number of sealing_thread that have reported errors\n * The interval from the last report to the current time\n\n\n# info / pause / resume\n\nThis set of commands is executed against the specified damocles-worker instance.\n\nTheir effects are equivalent to damocles-worker’s own list / pause / resume, which are used in the following ways.\n\n * damocles-manager util worker info <worker instance name or address>\n * damocles-manager util worker pause <worker instance name or address> <thread index>\n * damocles-manager util worker resume <worker instance name or address> <thread index> [<next state>]\n\nSpecific information can be viewed through help, and the definition and effect of parameters are consistent with the damocles-worker management tool.\n\nfor example:\n\n$ ./dist/bin/damocles-manager util worker info 127.0.0.1\n\nIndex Loc SectorID Paused PausedElapsed State LastErr\n0 /home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store1 s-t010000-2 true 13m42s C1Done permanent: No cached parameters found for stacked-proof-of-replication- merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f [failure finding /var/tmp/filecoin-proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]\n1      /home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store2  s-t010000-3  true    13m42s         C1Done  permanent: No cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f [failure finding /var/tmp/filecoin-proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]\n2      /home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store3  s-t010000-1  true    13m42s         C1Done  permanent: No cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f [failure finding /var/tmp/filecoin-proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]\n',normalizedContent:'# damocles-worker task management\n\nin the previous document, we mentioned that in the damocles architecture, the process management of a sector is by worker.\n\ntherefore, the management of sector tasks, especially exception handling, is also performed by the worker instance where the sector is being processed.\n\nhowever, it must be very inconvenient if all status checking and exception handling require remote access to the corresponding worker machine to operate.\n\ntherefore, in v0.2.0 and later versions, workers report status to sector-manager, and sector-manager manages workers remotely.\n\nbelow, we will explain both how worker can be managed locally and how sector-manager manage workers remotely.\n\n\n# damocles-worker local management\n\nthe local management of damocles-worker is mainly through a set of tools provided to call the management interface to operate…\n\n./dist/bin/damocles-worker worker\n\n\nwith subcommands like…\n\n * list\n * pause\n * resume\n\n\n# list\n\nlist is used to list the current state of all sealing_threads in the currently running damocles-worker instance.\n\ndamocles-worker worker list\n\n\nlet\'s take the mock configuration in the codebase as an example:\n\n$ ./dist/bin/damocles-worker worker list\n\n#0: "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store1"; sector_id=some(s-t010000-2), paused=true, paused_elapsed=some (17s), state=c1done, last_err=some("permanent: no cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793/cointmpail/var finding [f3793/cointmpail/var] -proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]")\n#1: "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store2"; sector_id=some(s-t010000-3), paused=true, paused_elapsed=some (17s), state=c1done, last_err=some("permanent: no cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793/cointmpail/var finding [f3793/cointmpail/var] -proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]")\n#2: "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store3"; sector_id=some(s-t010000-1), paused=true, paused_elapsed=some (17s), state=c1done, last_err=some("permanent: no cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793/cointmpail/var finding [f3793/cointmpail/var] -proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]")\n\n\nas you can see, for each sealing_thread , it will list\n\n * index number\n * local storage location information\n * sector id (if there is a sector task being processed)\n * if the task is paused\n * paused time (if there are paused sector tasks)\n * current status\n * last exception information (if there is a sector task suspended due to exception)\n\n\n# pause\n\npause is used to pause the sealing_thread with the specified index number.\n\n$ damocles-worker worker pause --index <index>\n\n\nnote that:\n\n * index needs to match the index number from list command.\n\n\n# resume\n\nresume is used to resume a suspended sealing_thread.\n\ndamocles-worker worker resume [--state <state>] --index <index>\n\n\nnote that:\n\n * index needs to match the index number from list command.\n * state is optional.\n\nif state is not supplied, the sector will try to restart with the current state; if the correct state value is supplied, it will restart with the specified state\n\nfor different sealing_thread task types, the optional status values can be found in 11. task status flow\n\n\n# damocles-manager remote management of damocles-worker\n\nthe management of damocles-worker by damocles-manager is mainly in two aspects:\n\n 1. receive periodic report information of the worker instance\n 2. call the management interface on the specified damocles-worker instance\n\nremote management is done through a set of tools provided to call the management interface of damocles-manager, or a proxy call to the management interface of the specified damocles-worker.\n\n./dist/bin/damocles-manager util worker\n\n\nthe subcommands included are:\n\n * list\n * info\n * pause\n * resume\n\n\n# list\n\nthe list here is used to list the worker profiles that have reported information to this damocles-manager instance, for example:\n\n$ ./dist/bin/damocles-manager util worker list\nname dest threads empty paused errors lastping(with ! if expired)\n127.0.0.1 127.0.0.1:17890 3 0 3 3 2.756922465s\n\n\nas you can see, for each instance, it will list:\n\n * instance name (if no instance name is specified, it will be the ip used to connect to damocles-manager)\n * instance connection information\n * sealing_thread number\n * the number of empty sealing_thread\n * the number of suspended sealing_thread\n * the number of sealing_thread that have reported errors\n * the interval from the last report to the current time\n\n\n# info / pause / resume\n\nthis set of commands is executed against the specified damocles-worker instance.\n\ntheir effects are equivalent to damocles-worker’s own list / pause / resume, which are used in the following ways.\n\n * damocles-manager util worker info <worker instance name or address>\n * damocles-manager util worker pause <worker instance name or address> <thread index>\n * damocles-manager util worker resume <worker instance name or address> <thread index> [<next state>]\n\nspecific information can be viewed through help, and the definition and effect of parameters are consistent with the damocles-worker management tool.\n\nfor example:\n\n$ ./dist/bin/damocles-manager util worker info 127.0.0.1\n\nindex loc sectorid paused pausedelapsed state lasterr\n0 /home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store1 s-t010000-2 true 13m42s c1done permanent: no cached parameters found for stacked-proof-of-replication- merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f [failure finding /var/tmp/filecoin-proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]\n1      /home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store2  s-t010000-3  true    13m42s         c1done  permanent: no cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f [failure finding /var/tmp/filecoin-proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]\n2      /home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/store3  s-t010000-1  true    13m42s         c1done  permanent: no cached parameters found for stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f [failure finding /var/tmp/filecoin-proof-parameters/v28-stacked-proof-of-replication-merkletree-poseidon_hasher-8-0-0-sha256_hasher-032d3138d22506ec0082ed72b2dcba18df18477904e35bafee82b3793b06832f.params]\n',charsets:{}},{title:"damocles-worker configuration guide",frontmatter:{},regularPath:"/operation/worker-config-guide.html",relativePath:"operation/worker-config-guide.md",key:"v-b7b7c982",path:"/operation/worker-config-guide.html",headers:[{level:2,title:"Resource consumption at each stage of sealing job  (32 GiB sector)",slug:"resource-consumption-at-each-stage-of-sealing-job-32-gib-sector",normalizedTitle:"resource consumption at each stage of sealing job  (32 gib sector)",charIndex:null},{level:2,title:"damocles-worker-util documentation",slug:"damocles-worker-util-documentation",normalizedTitle:"damocles-worker-util documentation",charIndex:2642},{level:3,title:"hwinfo",slug:"hwinfo",normalizedTitle:"hwinfo",charIndex:2771},{level:3,title:"sealcalc",slug:"sealcalc",normalizedTitle:"sealcalc",charIndex:2804}],headersStr:"Resource consumption at each stage of sealing job  (32 GiB sector) damocles-worker-util documentation hwinfo sealcalc",content:'# damocles-worker configuration guide\n\n\n# Resource consumption at each stage of sealing job (32 GiB sector)\n\nExmaple computer specs:\n\n * CPU: AMD EPYC 7642 (max MHz: 2300)\n * GPU: RTX 3080\n * Memory: DIMM DDR4 Synchronous Registered (Buffered) 2933 MHz (0.3 ns)\n\nSTAGE            CONCURRENCY   DURATION     RAM       CPU                      GPU   DISKIO READ   DISKIO WIRTE   REMARK\nWindowPoSt       1             ~4-10mins    ~120GiB   RAYON_NUM_THREADS*100%   ✓     TODO          -              \nWinningPoSt      1             ~1-10s       -         -                        -     -             -              \nAddPieces        1             ~3mins       ~210MiB   RAYON_NUM_THREADS*100%   x     <=32GiB       32GiB          \nTreeD            1             ~1min        ~47GiB    RAYON_NUM_THREADS*100%   x     32GiB         64GiB          64 GiB sector ~60GiB RAM\nPC1              1             ~177mins     <=64GiB   150%                     x     -             352GiB         \nPC2              1             ~10-13mins   ~64GiB    RAYON_NUM_THREADS*100%   ✓     384GiB        ~37GiB         \nSupraPC2         1             ~2-5mins     ~40GiB    ~400%                    ✓     384GiB        ~37GiB         \nWaitSeed         -             75mins       -         -                        x     -             -              \nC1               1             ~1s          -         -                        x     -             -              \nC2               1             ~13-16mins   TODO      RAYON_NUM_THREADS*100%   ✓     -             -              \nSupraC2          1             ~3-5mins     128 GiB   RAYON_NUM_THREADS*100%   ✓     -             -              \nSnapEncode       1             ~3-5mins     TODO      RAYON_NUM_THREADS*100%   ✓     ~32GiB(NFS)   ~32GiB(NFS)    It is recommended to run multiple SnapEncode tasks\n                                                                                                                  concurrently on each GPU to enable concurrent I/O and reduce\n                                                                                                                  GPU idle time\nSnapProve        1             ~3-5mins     TODO      RAYON_NUM_THREADS*100%   ✓     -             -              \nSupraSnapProve   1             ~3-5mins     TODO      RAYON_NUM_THREADS*100%   ✓     ~32GiB(NFS)   ~32GiB(NFS)    \nUnseal           1             TODO         TODO      TODO                     x     TODO          TODO           \n\nNote: The RAYON_NUM_THREADS environment variable is used to configure the number of threads used by tasks, and it defaults to the number of CPU cores.\n\n\n# damocles-worker-util documentation\n\ndamocles-worker-util contains a set of damocles-worker related utilities. These include:\n\n * hwinfo (Hardware information)\n * sealcalc (Sealing calculator)\n\n\n# hwinfo\n\nhwinfo displays hardware information so that we can configure damocles-worker accordingly given the output of hardware information and use them more effectively.\n\nThe information currently available from hwinfo is as follows:\n\n * CPU topology (including the number of CPU cores, NUMA Memory Node, CPU Cache, etc.)\n * Disk information\n * GPU information\n * Memory information\n\nParameter Description:\n\ndamocles-worker-util-hwinfo\n// Display hardware information\n\nUSAGE:\n    damocles-worker-util hwinfo [OPTIONS]\n\nOPTIONS:\n        --full  // display full CPU topology information\n    -h, --help \t// print help information\n\n\n# hwinfo dependency installation\n\n * hwloc 2.x is used to get CPU topology information\n * OpenCL is used to get GPU information\n\n# hwloc 2.x installation\n\n# On Ubuntu 20.04 or later, it can be installed directly using apt\n\n apt install hwloc=2.\\*\n\n\n# Source installation:\n\n# Install necessary tools.\napt install -y wget make gcc\n# Download hwloc-2.7.1.tar.gz\nwget https://download.open-mpi.org/release/hwloc/v2.7/hwloc-2.7.1.tar.gz\n\ntar -zxpf hwloc-2.7.1.tar.gz\ncd hwloc-2.7.1\n./configure --prefix=/usr/local\nmake -j$(nproc)\nsudo make install\nldconfig /usr/local/lib\n\n\n# OpenCL installation\n\napt install ocl-icd-opencl-dev\n\n\n# hwinfo example\n\nExample run on a machine with 2 32-core CPUs:\n\ndamocles-worker-util hwinfo\n\n\noutput:\n\nCPU topology:\nMachine (503.55 GiB)\n├── Package (251.57 GiB) (*** *** *** 32-Core Processor)\n│ ├── NUMANode (#0 251.57 GiB)\n│ ├── L3 (#0 16 MiB)\n│ │ └── PU #0 + PU #1 + PU #2 + PU #3\n│ ├── L3 (#1 16 MiB)\n│ │ └── PU #4 + PU #5 + PU #6 + PU #7\n│ ├── L3 (#2 16 MiB)\n│ │ └── PU #8 + PU #9 + PU #10 + PU #11\n│ ├── L3 (#3 16 MiB)\n│ │ └── PU #12 + PU #13 + PU #14 + PU #15\n│ ├── L3 (#4 16 MiB)\n│ │ └── PU #16 + PU #17 + PU #18 + PU #19\n│ ├── L3 (#5 16 MiB)\n│ │ └── PU #20 + PU #21 + PU #22 + PU #23\n│ ├── L3 (#6 16 MiB)\n│ │ └── PU #24 + PU #25 + PU #26 + PU #27\n│ └── L3 (#7 16 MiB)\n│ └── PU #28 + PU #29 + PU #30 + PU #31\n└── Package (251.98 GiB) (*** *** *** 32-Core Processor)\n    ├── NUMANode (#1 251.98 GiB)\n    ├── L3 (#8 16 MiB)\n    │ └── PU #32 + PU #33 + PU #34 + PU #35\n    ├── L3 (#9 16 MiB)\n    │ └── PU #36 + PU #37 + PU #38 + PU #39\n    ├── L3 (#10 16 MiB)\n    │ └── PU #40 + PU #41 + PU #42 + PU #43\n    ├── L3 (#11 16 MiB)\n    │ └── PU #44 + PU #45 + PU #46 + PU #47\n    ├── L3 (#12 16 MiB)\n    │ └── PU #48 + PU #49 + PU #50 + PU #51\n    ├── L3 (#13 16 MiB)\n    │ └── PU #52 + PU #53 + PU #54 + PU #55\n    ├── L3 (#14 16 MiB)\n    │ └── PU #56 + PU #57 + PU #58 + PU #59\n    └── L3 (#15 16 MiB)\n        └── PU #60 + PU #61 + PU #62 + PU #63\n\nDisks:\n╭───────────┬─────────────┬─────────────┬────────────┬───────────────────────────────────────╮\n│ Disk type │ Device name │ Mount point │ Filesystem │                 Space                 │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ SSD       │ /dev/sda3   │ /           │ ext4       │ 346.87 GiB / 434.68 GiB (79.80% used) │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ SSD       │ /dev/sda2   │ /boot       │ ext4       │ 675.00 MiB / 3.87 GiB (17.01% used)   │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ SSD       │ /dev/md127  │ /mnt/mount  │ ext4       │ 4.83 TiB / 13.86 TiB (34.86% used)    │\n╰───────────┴─────────────┴─────────────┴────────────┴───────────────────────────────────────╯\n\nGPU:\n╭─────────────────────────┬────────┬───────────╮\n│           Name          │ Vendor │   Memory  │\n├─────────────────────────┼────────┼───────────┤\n│ NVIDIA GeForce RTX 3080 │ NVIDIA │ 9.78 GiB  │\n├─────────────────────────┼────────┼───────────┤\n│ NVIDIA GeForce RTX 3080 │ NVIDIA │ 9.78 GiB  │\n├─────────────────────────┼────────┼───────────┤\n│ NVIDIA GeForce RTX 3080 │ NVIDIA │ 9.78 GiB  │\n╰─────────────────────────┴────────┴───────────╯\n\n\n\nMemory:\n╭──────────────┬───────────────────┬────────────┬─────────────╮\n│ Total memory │    Used memory    │ Total swap │  Used swap  │\n├──────────────┼───────────────────┼────────────┼─────────────┤\n│ 515.63 GiB   │ 33.51 GiB (6.50%) │ 0 B        │ 0 B (0.00%) │\n╰──────────────┴───────────────────┴────────────┴─────────────╯\n\n\n\nFrom the output CPU topology information, this machine has two NUMANodes:\n\n 1. CPU set of NUMANode #0: 0-31\n 2. CPU set of NUMANode #1: 32-63\n\nWe can modify the external processor configuration group in the damocles-worker configuration file ([[processors.{stage_name}]]).\n\nThrough cgroup.cpuset + numa_preferred configuration items, the external processor is restricted to only use the CPU in the specified NUMANode, and memory will also be allocated from the said NUMANode first, thereby improving the CPU efficiency (from v0.5.0 or later, damocles supports loading NUMA-affinity hugepage memory files; if this feature is enabled, cpuset can be allocated across NUMA nodes without performance impact).\n\nexample:\n\n# damocles-worker.toml\n\n[[processors.{stage_name}]]\nnuma_preferred = 0\ncgroup.cpuset = "0-3"\n# ...\n\n[[processors.{stage_name}]]\nnuma_preferred = 1\ncgroup.cpuset = "32-35"\n# ...\n\n\n----------------------------------------\n\n\n# sealcalc\n\nGiven fixed parameters, sealcalc computes the running status of tasks in each stage during respective time period to maximize the sealing efficiency by adjusting the maximum concurrent number of tasks and sealing_threads.\n\nParameter Description:\n\nUSAGE:\n    damocles-worker-util sealcalc [OPTIONS] --tree_d_mins <tree_d_mins> --tree_d_concurrent <tree_d_concurrent> --pc1_mins <pc1_mins> --pc1_concurrent <pc1_concurrent> --pc2_mins <pc2_mins> --pc2_concurrent <pc2_concurrent> --c2_mins <c2_mins > --c2_concurrent <c2_concurrent> --sealing_threads <sealing_threads>\n\nOPTIONS:\n        --c2_concurrent <c2_concurrent>                Specifies the maximum number of concurrent c2 tasks\n        --c2_mins <c2_mins>                            Specifies the time it takes to execute one c2 task, unit: minutes\n        --calculate_days <calculate_days>              Calculate the total duration, unit: days [default: 30]\n        --calculate_step_mins <calculate_step_mins>    Output step duration, unit: minutes [default: 60], if this value is 60, each line of results will be separated by 1 hour\n        --csv                                          Output results in csv format\n -h, --help                                            Print help information\n        --pc1_concurrent <pc1_concurrent>              Specifies the maximum number of concurrent pc1 tasks\n        --pc1_mins <pc1_mins>                          Specifies the time it takes to execute one pc1 task, unit: minutes\n        --pc2_concurrent <pc2_concurrent>              Specifies the maximum number of concurrent pc2 tasks\n        --pc2_mins <pc2_mins>                          Specifies the time it takes to execute one pc2 task, unit: minutes\n        --sealing_threads <sealing_threads>            Specify the number of sealing_threads\n        --seed_mins <seed_mins>                        Specifies the time to wait for the seed, unit: minutes [default: 80]\n        --tree_d_concurrent <tree_d_concurrent>        Specifies the maximum number of concurrent tree_d tasks\n        --tree_d_mins <tree_d_mins>                    Specify the time it takes to execute one tree_d task, unit: minutes\n\n\n# sealcalc example:\n\n# Fixed parameters:\n\n * Time required for tree_d task execution: 10 minutes\n * Time required for pc1 task execution: 320 minutes\n * Time required for pc2 task execution: 25 minutes\n * Time required for c2 task execution: 18 minutes\n\n# Adjustable parameters:\n\n * sealing_threads number: 18\n * tree_d maximum concurrency: 2\n * pc1 maximum concurrency: 10\n * pc2 maximum concurrency: 5\n * c2 maximum concurrency: 2\n\ndamocles-worker-util sealcalc --tree_d_mins=10 --pc1_mins=320 --pc2_mins=1 --c2_mins=2 --tree_d_concurrent=2 --pc1_concurrent=10 --pc2_concurrent=5 --c2_concurrent=2 --sealing_threads= 18\n\n\nThe output is as follows:\n\n┌sealing calculator─────────────────────────────────────────────────────┐\n│time    sealing    tree_d      pc1      pc2     wait    c2     finished│\n│(mins)  threads    (...)      (...)     (...)   seed   (...)   sectors │\n│                                                                       │\n│0       2/18        2/2       0/10       0/5      0     0/2      0     │\n│60      14/18       2/2       10/10      0/5      0     0/2      0     │\n│120     18/18       0/2       10/10      0/5      0     0/2      0     │\n│180     18/18       0/2       10/10      0/5      0     0/2      0     │\n│240     18/18       0/2       10/10      0/5      0     0/2      0     │\n│300     18/18       0/2       10/10      0/5      0     0/2      0     │\n│360     18/18       0/2       10/10      2/5      6     0/2      0     │\n│420     18/18       2/2       8/10       0/5      8     0/2      2     │\n│480     18/18       0/2       10/10      0/5      0     0/2      10    │\n│540     18/18       0/2       10/10      0/5      0     0/2      10    │\n│600     18/18       0/2       10/10      0/5      0     0/2      10    │\n│660     18/18       0/2       10/10      2/5      2     0/2      10    │\n│720     18/18       0/2       10/10      0/5      8     0/2      10    │\n│780     18/18       0/2       10/10      0/5      2     0/2      18    │\n│840     18/18       0/2       10/10      0/5      0     0/2      20    │\n│900     18/18       0/2       10/10      0/5      0     0/2      20    │\n│960     18/18       0/2       10/10      0/5      0     0/2      20    │\n│1020    18/18       0/2       10/10      0/5      8     0/2      20    │\n│1080    18/18       2/2       10/10      0/5      4     0/2      26    │\n│1140    18/18       0/2       10/10      0/5      2     0/2      28    │\n│1200    18/18       0/2       10/10      0/5      0     0/2      30    │\n│1260    18/18       0/2       10/10      0/5      0     0/2      30    │\n│1320    18/18       0/2       10/10      2/5      6     0/2      30    │\n│1380    18/18       2/2       10/10      0/5      6     0/2      32    │\n│1440    18/18       0/2       10/10      0/5      2     0/2      38    │\n│1500    18/18       0/2       10/10      0/5      0     0/2      40    │\n│1560    18/18       0/2       10/10      0/5      0     0/2      40    │\n│1620    18/18       0/2       10/10      2/5      2     0/2      40    │\n│1680    18/18       0/2       10/10      0/5      8     0/2      40    │\n│1740    18/18       0/2       10/10      0/5      2     0/2      48    │\n└───────────────────────────────────────────────────────────────────────┘\n\n\n\nArrow keys to turn pages\n\nDescription of each column of the output result:\n\n * time (mins): time, unit: minutes. Each item of data output is the result of running within one step\n * sealing thread (running/total): sealing thread status (running thread/total thread)\n * tree_d (running/total): task status of tree_d stage (number of running tasks/total number of tasks)\n * pc1 (running/total): task status of pc1 stage (number of running tasks/total number of tasks)\n * pc2 (running/total): task status of pc2 stage (number of running tasks/total number of tasks)\n * wait seed: the number of tasks waiting for the seed\n * c2 (running/total): task status of c2 stage (number of running tasks/total number of tasks)\n * finish sector: the sector completed up to this step\n\nWe can maximize the sealing efficiency by continuously adjusting the above-mentioned adjustable parameters. These parameters can be used as a reference for the configuration of damocles-worker.',normalizedContent:'# damocles-worker configuration guide\n\n\n# resource consumption at each stage of sealing job (32 gib sector)\n\nexmaple computer specs:\n\n * cpu: amd epyc 7642 (max mhz: 2300)\n * gpu: rtx 3080\n * memory: dimm ddr4 synchronous registered (buffered) 2933 mhz (0.3 ns)\n\nstage            concurrency   duration     ram       cpu                      gpu   diskio read   diskio wirte   remark\nwindowpost       1             ~4-10mins    ~120gib   rayon_num_threads*100%   ✓     todo          -              \nwinningpost      1             ~1-10s       -         -                        -     -             -              \naddpieces        1             ~3mins       ~210mib   rayon_num_threads*100%   x     <=32gib       32gib          \ntreed            1             ~1min        ~47gib    rayon_num_threads*100%   x     32gib         64gib          64 gib sector ~60gib ram\npc1              1             ~177mins     <=64gib   150%                     x     -             352gib         \npc2              1             ~10-13mins   ~64gib    rayon_num_threads*100%   ✓     384gib        ~37gib         \nsuprapc2         1             ~2-5mins     ~40gib    ~400%                    ✓     384gib        ~37gib         \nwaitseed         -             75mins       -         -                        x     -             -              \nc1               1             ~1s          -         -                        x     -             -              \nc2               1             ~13-16mins   todo      rayon_num_threads*100%   ✓     -             -              \nsuprac2          1             ~3-5mins     128 gib   rayon_num_threads*100%   ✓     -             -              \nsnapencode       1             ~3-5mins     todo      rayon_num_threads*100%   ✓     ~32gib(nfs)   ~32gib(nfs)    it is recommended to run multiple snapencode tasks\n                                                                                                                  concurrently on each gpu to enable concurrent i/o and reduce\n                                                                                                                  gpu idle time\nsnapprove        1             ~3-5mins     todo      rayon_num_threads*100%   ✓     -             -              \nsuprasnapprove   1             ~3-5mins     todo      rayon_num_threads*100%   ✓     ~32gib(nfs)   ~32gib(nfs)    \nunseal           1             todo         todo      todo                     x     todo          todo           \n\nnote: the rayon_num_threads environment variable is used to configure the number of threads used by tasks, and it defaults to the number of cpu cores.\n\n\n# damocles-worker-util documentation\n\ndamocles-worker-util contains a set of damocles-worker related utilities. these include:\n\n * hwinfo (hardware information)\n * sealcalc (sealing calculator)\n\n\n# hwinfo\n\nhwinfo displays hardware information so that we can configure damocles-worker accordingly given the output of hardware information and use them more effectively.\n\nthe information currently available from hwinfo is as follows:\n\n * cpu topology (including the number of cpu cores, numa memory node, cpu cache, etc.)\n * disk information\n * gpu information\n * memory information\n\nparameter description:\n\ndamocles-worker-util-hwinfo\n// display hardware information\n\nusage:\n    damocles-worker-util hwinfo [options]\n\noptions:\n        --full  // display full cpu topology information\n    -h, --help \t// print help information\n\n\n# hwinfo dependency installation\n\n * hwloc 2.x is used to get cpu topology information\n * opencl is used to get gpu information\n\n# hwloc 2.x installation\n\n# on ubuntu 20.04 or later, it can be installed directly using apt\n\n apt install hwloc=2.\\*\n\n\n# source installation:\n\n# install necessary tools.\napt install -y wget make gcc\n# download hwloc-2.7.1.tar.gz\nwget https://download.open-mpi.org/release/hwloc/v2.7/hwloc-2.7.1.tar.gz\n\ntar -zxpf hwloc-2.7.1.tar.gz\ncd hwloc-2.7.1\n./configure --prefix=/usr/local\nmake -j$(nproc)\nsudo make install\nldconfig /usr/local/lib\n\n\n# opencl installation\n\napt install ocl-icd-opencl-dev\n\n\n# hwinfo example\n\nexample run on a machine with 2 32-core cpus:\n\ndamocles-worker-util hwinfo\n\n\noutput:\n\ncpu topology:\nmachine (503.55 gib)\n├── package (251.57 gib) (*** *** *** 32-core processor)\n│ ├── numanode (#0 251.57 gib)\n│ ├── l3 (#0 16 mib)\n│ │ └── pu #0 + pu #1 + pu #2 + pu #3\n│ ├── l3 (#1 16 mib)\n│ │ └── pu #4 + pu #5 + pu #6 + pu #7\n│ ├── l3 (#2 16 mib)\n│ │ └── pu #8 + pu #9 + pu #10 + pu #11\n│ ├── l3 (#3 16 mib)\n│ │ └── pu #12 + pu #13 + pu #14 + pu #15\n│ ├── l3 (#4 16 mib)\n│ │ └── pu #16 + pu #17 + pu #18 + pu #19\n│ ├── l3 (#5 16 mib)\n│ │ └── pu #20 + pu #21 + pu #22 + pu #23\n│ ├── l3 (#6 16 mib)\n│ │ └── pu #24 + pu #25 + pu #26 + pu #27\n│ └── l3 (#7 16 mib)\n│ └── pu #28 + pu #29 + pu #30 + pu #31\n└── package (251.98 gib) (*** *** *** 32-core processor)\n    ├── numanode (#1 251.98 gib)\n    ├── l3 (#8 16 mib)\n    │ └── pu #32 + pu #33 + pu #34 + pu #35\n    ├── l3 (#9 16 mib)\n    │ └── pu #36 + pu #37 + pu #38 + pu #39\n    ├── l3 (#10 16 mib)\n    │ └── pu #40 + pu #41 + pu #42 + pu #43\n    ├── l3 (#11 16 mib)\n    │ └── pu #44 + pu #45 + pu #46 + pu #47\n    ├── l3 (#12 16 mib)\n    │ └── pu #48 + pu #49 + pu #50 + pu #51\n    ├── l3 (#13 16 mib)\n    │ └── pu #52 + pu #53 + pu #54 + pu #55\n    ├── l3 (#14 16 mib)\n    │ └── pu #56 + pu #57 + pu #58 + pu #59\n    └── l3 (#15 16 mib)\n        └── pu #60 + pu #61 + pu #62 + pu #63\n\ndisks:\n╭───────────┬─────────────┬─────────────┬────────────┬───────────────────────────────────────╮\n│ disk type │ device name │ mount point │ filesystem │                 space                 │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ ssd       │ /dev/sda3   │ /           │ ext4       │ 346.87 gib / 434.68 gib (79.80% used) │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ ssd       │ /dev/sda2   │ /boot       │ ext4       │ 675.00 mib / 3.87 gib (17.01% used)   │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ ssd       │ /dev/md127  │ /mnt/mount  │ ext4       │ 4.83 tib / 13.86 tib (34.86% used)    │\n╰───────────┴─────────────┴─────────────┴────────────┴───────────────────────────────────────╯\n\ngpu:\n╭─────────────────────────┬────────┬───────────╮\n│           name          │ vendor │   memory  │\n├─────────────────────────┼────────┼───────────┤\n│ nvidia geforce rtx 3080 │ nvidia │ 9.78 gib  │\n├─────────────────────────┼────────┼───────────┤\n│ nvidia geforce rtx 3080 │ nvidia │ 9.78 gib  │\n├─────────────────────────┼────────┼───────────┤\n│ nvidia geforce rtx 3080 │ nvidia │ 9.78 gib  │\n╰─────────────────────────┴────────┴───────────╯\n\n\n\nmemory:\n╭──────────────┬───────────────────┬────────────┬─────────────╮\n│ total memory │    used memory    │ total swap │  used swap  │\n├──────────────┼───────────────────┼────────────┼─────────────┤\n│ 515.63 gib   │ 33.51 gib (6.50%) │ 0 b        │ 0 b (0.00%) │\n╰──────────────┴───────────────────┴────────────┴─────────────╯\n\n\n\nfrom the output cpu topology information, this machine has two numanodes:\n\n 1. cpu set of numanode #0: 0-31\n 2. cpu set of numanode #1: 32-63\n\nwe can modify the external processor configuration group in the damocles-worker configuration file ([[processors.{stage_name}]]).\n\nthrough cgroup.cpuset + numa_preferred configuration items, the external processor is restricted to only use the cpu in the specified numanode, and memory will also be allocated from the said numanode first, thereby improving the cpu efficiency (from v0.5.0 or later, damocles supports loading numa-affinity hugepage memory files; if this feature is enabled, cpuset can be allocated across numa nodes without performance impact).\n\nexample:\n\n# damocles-worker.toml\n\n[[processors.{stage_name}]]\nnuma_preferred = 0\ncgroup.cpuset = "0-3"\n# ...\n\n[[processors.{stage_name}]]\nnuma_preferred = 1\ncgroup.cpuset = "32-35"\n# ...\n\n\n----------------------------------------\n\n\n# sealcalc\n\ngiven fixed parameters, sealcalc computes the running status of tasks in each stage during respective time period to maximize the sealing efficiency by adjusting the maximum concurrent number of tasks and sealing_threads.\n\nparameter description:\n\nusage:\n    damocles-worker-util sealcalc [options] --tree_d_mins <tree_d_mins> --tree_d_concurrent <tree_d_concurrent> --pc1_mins <pc1_mins> --pc1_concurrent <pc1_concurrent> --pc2_mins <pc2_mins> --pc2_concurrent <pc2_concurrent> --c2_mins <c2_mins > --c2_concurrent <c2_concurrent> --sealing_threads <sealing_threads>\n\noptions:\n        --c2_concurrent <c2_concurrent>                specifies the maximum number of concurrent c2 tasks\n        --c2_mins <c2_mins>                            specifies the time it takes to execute one c2 task, unit: minutes\n        --calculate_days <calculate_days>              calculate the total duration, unit: days [default: 30]\n        --calculate_step_mins <calculate_step_mins>    output step duration, unit: minutes [default: 60], if this value is 60, each line of results will be separated by 1 hour\n        --csv                                          output results in csv format\n -h, --help                                            print help information\n        --pc1_concurrent <pc1_concurrent>              specifies the maximum number of concurrent pc1 tasks\n        --pc1_mins <pc1_mins>                          specifies the time it takes to execute one pc1 task, unit: minutes\n        --pc2_concurrent <pc2_concurrent>              specifies the maximum number of concurrent pc2 tasks\n        --pc2_mins <pc2_mins>                          specifies the time it takes to execute one pc2 task, unit: minutes\n        --sealing_threads <sealing_threads>            specify the number of sealing_threads\n        --seed_mins <seed_mins>                        specifies the time to wait for the seed, unit: minutes [default: 80]\n        --tree_d_concurrent <tree_d_concurrent>        specifies the maximum number of concurrent tree_d tasks\n        --tree_d_mins <tree_d_mins>                    specify the time it takes to execute one tree_d task, unit: minutes\n\n\n# sealcalc example:\n\n# fixed parameters:\n\n * time required for tree_d task execution: 10 minutes\n * time required for pc1 task execution: 320 minutes\n * time required for pc2 task execution: 25 minutes\n * time required for c2 task execution: 18 minutes\n\n# adjustable parameters:\n\n * sealing_threads number: 18\n * tree_d maximum concurrency: 2\n * pc1 maximum concurrency: 10\n * pc2 maximum concurrency: 5\n * c2 maximum concurrency: 2\n\ndamocles-worker-util sealcalc --tree_d_mins=10 --pc1_mins=320 --pc2_mins=1 --c2_mins=2 --tree_d_concurrent=2 --pc1_concurrent=10 --pc2_concurrent=5 --c2_concurrent=2 --sealing_threads= 18\n\n\nthe output is as follows:\n\n┌sealing calculator─────────────────────────────────────────────────────┐\n│time    sealing    tree_d      pc1      pc2     wait    c2     finished│\n│(mins)  threads    (...)      (...)     (...)   seed   (...)   sectors │\n│                                                                       │\n│0       2/18        2/2       0/10       0/5      0     0/2      0     │\n│60      14/18       2/2       10/10      0/5      0     0/2      0     │\n│120     18/18       0/2       10/10      0/5      0     0/2      0     │\n│180     18/18       0/2       10/10      0/5      0     0/2      0     │\n│240     18/18       0/2       10/10      0/5      0     0/2      0     │\n│300     18/18       0/2       10/10      0/5      0     0/2      0     │\n│360     18/18       0/2       10/10      2/5      6     0/2      0     │\n│420     18/18       2/2       8/10       0/5      8     0/2      2     │\n│480     18/18       0/2       10/10      0/5      0     0/2      10    │\n│540     18/18       0/2       10/10      0/5      0     0/2      10    │\n│600     18/18       0/2       10/10      0/5      0     0/2      10    │\n│660     18/18       0/2       10/10      2/5      2     0/2      10    │\n│720     18/18       0/2       10/10      0/5      8     0/2      10    │\n│780     18/18       0/2       10/10      0/5      2     0/2      18    │\n│840     18/18       0/2       10/10      0/5      0     0/2      20    │\n│900     18/18       0/2       10/10      0/5      0     0/2      20    │\n│960     18/18       0/2       10/10      0/5      0     0/2      20    │\n│1020    18/18       0/2       10/10      0/5      8     0/2      20    │\n│1080    18/18       2/2       10/10      0/5      4     0/2      26    │\n│1140    18/18       0/2       10/10      0/5      2     0/2      28    │\n│1200    18/18       0/2       10/10      0/5      0     0/2      30    │\n│1260    18/18       0/2       10/10      0/5      0     0/2      30    │\n│1320    18/18       0/2       10/10      2/5      6     0/2      30    │\n│1380    18/18       2/2       10/10      0/5      6     0/2      32    │\n│1440    18/18       0/2       10/10      0/5      2     0/2      38    │\n│1500    18/18       0/2       10/10      0/5      0     0/2      40    │\n│1560    18/18       0/2       10/10      0/5      0     0/2      40    │\n│1620    18/18       0/2       10/10      2/5      2     0/2      40    │\n│1680    18/18       0/2       10/10      0/5      8     0/2      40    │\n│1740    18/18       0/2       10/10      0/5      2     0/2      48    │\n└───────────────────────────────────────────────────────────────────────┘\n\n\n\narrow keys to turn pages\n\ndescription of each column of the output result:\n\n * time (mins): time, unit: minutes. each item of data output is the result of running within one step\n * sealing thread (running/total): sealing thread status (running thread/total thread)\n * tree_d (running/total): task status of tree_d stage (number of running tasks/total number of tasks)\n * pc1 (running/total): task status of pc1 stage (number of running tasks/total number of tasks)\n * pc2 (running/total): task status of pc2 stage (number of running tasks/total number of tasks)\n * wait seed: the number of tasks waiting for the seed\n * c2 (running/total): task status of c2 stage (number of running tasks/total number of tasks)\n * finish sector: the sector completed up to this step\n\nwe can maximize the sealing efficiency by continuously adjusting the above-mentioned adjustable parameters. these parameters can be used as a reference for the configuration of damocles-worker.',charsets:{}},{title:"Have a question?",frontmatter:{editLink:!1},regularPath:"/questions.html",relativePath:"questions.md",key:"v-6971be1f",path:"/questions.html",headers:[{level:3,title:"Have a question?",slug:"have-a-question",normalizedTitle:"have a question?",charIndex:2}],lastUpdated:"10/23/2023, 7:59:15 AM",headersStr:"Have a question?",content:"# Have a question?\n\nFilecoin.io - general information about the Filecoin network\n\nCommunity Channels - links to Filecoin community chat and forum",normalizedContent:"# have a question?\n\nfilecoin.io - general information about the filecoin network\n\ncommunity channels - links to filecoin community chat and forum",charsets:{}},{title:"Home",frontmatter:{home:!0,heroImage:"/assets/damocles-hero.jpg",actionText:"快速上手 →",actionLink:"/zh/intro/",footer:"MIT Apache dual Licensed"},regularPath:"/zh/",relativePath:"zh/README.md",key:"v-5f486ce8",path:"/zh/",lastUpdated:"2023/10/23 07:59:15",headersStr:null,content:"任务调度重塑\n\n重塑任务调度，miner 被动倾听 worker 的封装进度，而非主动调度；最终提高封装管道效率 30%\n\n\n私有 SaaS\n\n通过 Damocles 独创的架构和广泛的配置选择，SP 可以快速将 Worker 池组织成私有 SaaS 服务（Sealing-as-a-Service），可用于同时为多个 miner_ids 提供封装\n\n\n优化的 Snapdeal 流程\n\nSP 可以批量导入已有的 CC 扇区作为候选扇区。随着存储订单的到来，Worker 将自己去抓取候选扇区，并在这些候选扇区上 Snapdeal，从而最大限度地减少人工干预",normalizedContent:"任务调度重塑\n\n重塑任务调度，miner 被动倾听 worker 的封装进度，而非主动调度；最终提高封装管道效率 30%\n\n\n私有 saas\n\n通过 damocles 独创的架构和广泛的配置选择，sp 可以快速将 worker 池组织成私有 saas 服务（sealing-as-a-service），可用于同时为多个 miner_ids 提供封装\n\n\n优化的 snapdeal 流程\n\nsp 可以批量导入已有的 cc 扇区作为候选扇区。随着存储订单的到来，worker 将自己去抓取候选扇区，并在这些候选扇区上 snapdeal，从而最大限度地减少人工干预",charsets:{cjk:!0}},{title:"原则",frontmatter:{},regularPath:"/zh/TODO.html",relativePath:"zh/TODO.md",key:"v-3266a61f",path:"/zh/TODO.html",headers:[{level:2,title:"原则",slug:"原则",normalizedTitle:"原则",charIndex:2},{level:2,title:"TODO list",slug:"todo-list",normalizedTitle:"todo list",charIndex:340}],lastUpdated:"2023/10/23 07:59:15",headersStr:"原则 TODO list",content:"# 原则\n\n  基于目前的需要，先从重构 guide 和 modules 模块，让两块联动，形成比较易读的 venus 集群部署介绍。\n\n * modules 模块存储各组件的简介，将不相干的文档移到对应位置\n\n * guide 和 modules 对于组件介绍存在很多重复的，只保留一份\n\n * modules 模块组件介绍老旧，超链接到项目的介绍文档，避免项目内更新后 doc 中不及时及重复劳动\n\n * guide 将组件介绍，编译构建，部署文档独立\n\n * guide 模块增加 QA 模块，将 venus discussion 的问题以超链接方式统一到 QA 模块\n\n * 部署模块尽量少一些设计的介绍，简明扼要，每个指定的操作尽量不涉及不相干的，以免造成误解\n\n\n# TODO list\n\n * [ ] modules 模块\n   \n   * [ ] 引导页内容更新\n   * [ ] 不相关文档移走\n   * [ ] 文档命名一致\n   * [ ] 组件介绍匹配项目进度\n   * [x] 编译构建独立到一个文档\n   * [ ] 组件文档移除从项目中搬过来的内容，用超链接\n   * [ ] market 和 cluster 模块的技术文档移到 modules 模块，\n\n * [ ] guide 模块\n   \n   * [ ] 将部署用到的基础知识单独形成文档\n   * [x] 编译构建超链接到对应文档\n   * [x] 增加部分组件的验证：怎么判断部署成功\n   * [ ] 目录结构调整\n     * [ ] market 和 cluster 归属到链服务部署和独立组件部署文档\n     * [ ] .fil_withdraw_and_send.md.swp 文件删除\n     * [ ] sealer 文档放到单独的目录\n   * [ ] Troubleshooting-&-FAQ.md 用超链接方式管理 venus discussion\n\n * [ ] 移除没有价值或过时的文档\n\n * [ ] 在很多模块中重复的内容整合\n\n * [ ] 每个项目搬过来的内容使用超链接",normalizedContent:"# 原则\n\n  基于目前的需要，先从重构 guide 和 modules 模块，让两块联动，形成比较易读的 venus 集群部署介绍。\n\n * modules 模块存储各组件的简介，将不相干的文档移到对应位置\n\n * guide 和 modules 对于组件介绍存在很多重复的，只保留一份\n\n * modules 模块组件介绍老旧，超链接到项目的介绍文档，避免项目内更新后 doc 中不及时及重复劳动\n\n * guide 将组件介绍，编译构建，部署文档独立\n\n * guide 模块增加 qa 模块，将 venus discussion 的问题以超链接方式统一到 qa 模块\n\n * 部署模块尽量少一些设计的介绍，简明扼要，每个指定的操作尽量不涉及不相干的，以免造成误解\n\n\n# todo list\n\n * [ ] modules 模块\n   \n   * [ ] 引导页内容更新\n   * [ ] 不相关文档移走\n   * [ ] 文档命名一致\n   * [ ] 组件介绍匹配项目进度\n   * [x] 编译构建独立到一个文档\n   * [ ] 组件文档移除从项目中搬过来的内容，用超链接\n   * [ ] market 和 cluster 模块的技术文档移到 modules 模块，\n\n * [ ] guide 模块\n   \n   * [ ] 将部署用到的基础知识单独形成文档\n   * [x] 编译构建超链接到对应文档\n   * [x] 增加部分组件的验证：怎么判断部署成功\n   * [ ] 目录结构调整\n     * [ ] market 和 cluster 归属到链服务部署和独立组件部署文档\n     * [ ] .fil_withdraw_and_send.md.swp 文件删除\n     * [ ] sealer 文档放到单独的目录\n   * [ ] troubleshooting-&-faq.md 用超链接方式管理 venus discussion\n\n * [ ] 移除没有价值或过时的文档\n\n * [ ] 在很多模块中重复的内容整合\n\n * [ ] 每个项目搬过来的内容使用超链接",charsets:{cjk:!0}},{frontmatter:{},regularPath:"/zh/about/",relativePath:"zh/about/README.md",key:"v-6cf8170c",path:"/zh/about/",headers:[{level:2,title:"使命，愿景，价值观",slug:"使命-愿景-价值观",normalizedTitle:"使命，愿景，价值观",charIndex:2},{level:2,title:"联系方式",slug:"联系方式",normalizedTitle:"联系方式",charIndex:100},{level:2,title:"其他资源",slug:"其他资源",normalizedTitle:"其他资源",charIndex:172}],lastUpdated:"2023/10/23 07:59:15",headersStr:"使命，愿景，价值观 联系方式 其他资源",content:"# 使命，愿景，价值观\n\n * ❗️使命：建设 Filecoin 基础设施和工具，支持生态繁荣发展\n * 🌏 愿景：人人都能参与 Filecoin 生态\n * ☯️ 价值观：开放并且去信任\n\n\n# 联系方式\n\n * Slack:#fil-venus, Slack:#fil-venus-cn, venus@ipfsforce.com\n\n\n# 其他资源\n\n * venus-docs: venus.filecoin.io\n * VenusHub: venushub.io\n * Github: damocles, venus, droplet\n * Social Media: Twitter, Wechat\n * 安全审计报告：link",normalizedContent:"# 使命，愿景，价值观\n\n * ❗️使命：建设 filecoin 基础设施和工具，支持生态繁荣发展\n * 🌏 愿景：人人都能参与 filecoin 生态\n * ☯️ 价值观：开放并且去信任\n\n\n# 联系方式\n\n * slack:#fil-venus, slack:#fil-venus-cn, venus@ipfsforce.com\n\n\n# 其他资源\n\n * venus-docs: venus.filecoin.io\n * venushub: venushub.io\n * github: damocles, venus, droplet\n * social media: twitter, wechat\n * 安全审计报告：link",charsets:{cjk:!0}},{title:"简述",frontmatter:{},regularPath:"/zh/developer/",relativePath:"zh/developer/README.md",key:"v-87056da8",path:"/zh/developer/",headers:[{level:2,title:"设计思路",slug:"设计思路",normalizedTitle:"设计思路",charIndex:182},{level:3,title:"lotus 封装组件的模式",slug:"lotus-封装组件的模式",normalizedTitle:"lotus 封装组件的模式",charIndex:585},{level:3,title:"venus-cluster 的设计思路",slug:"venus-cluster-的设计思路",normalizedTitle:"venus-cluster 的设计思路",charIndex:1718},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:66}],lastUpdated:"2023/10/23 07:59:15",headersStr:"设计思路 lotus 封装组件的模式 venus-cluster 的设计思路 总结",content:"# 简述\n\n我们的不少合作伙伴曾大量、深度使用过 lotus 的封装组件，也不乏自行定制的经历。\n\n使用过程中，他们遇到不少痛点，也总结出许多经验。我们将这些思考汇总起来，尝试提供一套与 lotus 封装组件思路不完全相同、以简洁、效率和灵活为最优先考虑，但同样具备竞争力的集群方案，供广大的 SP （Storage Provider，存储供应商）选择。\n\n\n# 设计思路\n\nvenus-cluster 主要包含 venus-sector-manager 和 venus-worker 两个组件，其功能大致与 lotus-miner 和 lotus-seal-worker 的组合相当。\n\nvenus-cluster 着眼于实现一套满足：\n\n * 面向大量同质化计算设备，降低硬件需求类型、实例数量和部署复杂度\n * 尽可能针对不同环节、不同类型的异常，安全地完成“自愈”，减少人工介入、提供生产效率\n * 依托云服务、外包服务及其他可共享的基础设施，降低门槛，削减不必要的成本\n\n等目标的高效封装集群方案。\n\n当然，明确且针对性强的设计意图可能导致这套方案并不普适于所有用户。例如，如果使用者已经拥有大量针对封装的不同阶段单独配置、形成搭配的设备，那么 venus-cluster 方案可能并不能有效地提升。具体情形，可以在阅读完后续文档后，由用户自行判断。\n\n\n# lotus 封装组件的模式\n\n在介绍 venus-cluster 的具体设计思路之前，我们有必要回顾一下 louts 相关组件的工作模式。\n\nlotus 的封装组件是一套标准的中心化调度集群：\n\n * lotus-miner 是中心调度服务，对任务进行分配、管理；\n * lotus-seal-worker 是封装执行体，负责完成分配到其之上的具体任务，并反馈给 lotus-miner；\n * lotus-miner 作为发起方，与 lotus-seal-worker 之间通过 RPC 交互。\n\n这套设计优点十分明显：\n\n * lotus-seal-worker 是几乎无状态的工作进程，理论上很方便进行横向扩容；\n * lotus-miner 可以灵活地将任务在 lotus-worker 之间进行调度。\n\n但是这套机制也存在另一面：\n\n 1. lotus-miner 作为调度核心：\n    \n    * 承担着繁重的、不同类型的逻辑，如任务调度和管理，与链进行交互等\n    * 保留着需要同时兼容本地模式和远程模式的历史负担\n    \n    导致内部逻辑（状态机）复杂、异常处理难以细化。\n\n 2. lotus-seal-worker 被完全设计为无状态的模式。\n\n继而产生了一些问题，例如：\n\n 1. 由于各个阶段之间对硬件资源的要求迥异，为了保证资源的合理分配，通常会将 lotus-seal-worker 设置成每个实例支持一个阶段任务的模式，在同一机器上部署多个不同的实例，通过操作系统级别的资源控制（cgroup、docker、numa 等）进行隔离。\n    \n    这导致在大规模集群中， lotus-seal-worker 数量众多，编排和管理都较为复杂。\n\n 2. 任何中心化任务调度机制都很有可能需要考虑计算和存储资源的匹配、亲和性等。\n    \n    我们说 lotus-seal-worker 几乎无状态，但是事实上，扇区封装过程中存在着 临时文件 这样一种特殊的“上下文”。对于这类“上下文”，lotus-miner 长期以来简单地对其进行整体搬移。这导致了巨大的网络带宽开销，以及一系列随之产生的网络、存储异常的可能性。\n    \n    直到 PR 7453 提出“存储分组”概念之后，亲和性问题才初步解决，但这一方案给 worker 的编排又增加了额外的复杂度。\n\n 3. 大规模集群中存在着大量可能导致任务异常的因素，包括但不限于：计算错误、存储失效、网络抖动。而 lotus-miner 内部缺乏有效感知异常根因的手段，使得其难以根据具体原因选择不同的异常恢复机制，只能统一粗粒度地处理或等待人工介入。\n\n\n# venus-cluster 的设计思路\n\n基于我们的目标和过去的经验，venus-cluster 在设计之初就确立了几点思路：\n\n * 简化逻辑\n * 优化流程\n * 隔离异常\n * 降低部署复杂度\n * 灵活替换功能组件\n\n下面我们会具体进行阐述。\n\n# 简化逻辑\n\n我们重新规划了上下游组件之间的责任划分：\n\n * venus-sector-manager 负责：\n   * 扇区封装过程中与链进行交互的部分，如 Ticket 和 Seed 的获取，消息的独立或聚合提交等\n   * 扇区封装任务的状态记录，如当前阶段、异常及原因等\n   * 已完成扇区的管理和维持，如响应 WindowPoST 等\n * venus-worker 负责：\n   * 管理各个阶段计算资源的分配\n   * 使用预先规划好的本地存储空间完成扇区封装的完整过程\n\n通过这样的划分，使得 venus-sector-manager 只需被动地接收必须传递的信息，不必实现一套复杂的感知和调度系统；同时 venus-worker 直接管理各类本地异常，便于执行不同的处理策略。\n\n这样，两者的业务逻辑都不会频繁出现分支，相对便于理解及修改。\n\n# 优化流程\n\n在 lotus 的封装流程中，最大的问题就是上文提到的，对封装过程中的临时文件的处理。此外还存在一些不尽如人意的点，例如：\n\n * 订单的等待逻辑可能会使计算资源闲置\n * 存储资源和计算资源的强制一对一匹配\n * 部分阶段，内部多个环节对资源的需求度不一致\n * 将与链的交互和扇区封装计算捆绑，带来的存储空间释放不及时等\n\n等。这些都会到封装的整体效率产生影响。\n\n# 隔离异常\n\n一方面，主要的异常处理被移动到了 venus-worker 一层，且在与 venus-sector-manager 交互的接口设计中，就尽早考虑了区分网络异常和逻辑异常，从而使得 venus-worker 比较容易感知到异常的具体原因。\n\n另一方面，我们将封装过程中可能产生的错误类型归为四个级别，并设定了不同的处理方式：\n\n * temp(orary)，临时： 这个级别的错误通常明确属于临时性的，或者我们知道重试不会带来负面影响的。 对于这类错误，worker 会自动尝试重试（以 recover_interval 为间隔，最多 max_retries 次）。 当重试次数超过设定的上限时，会自动升级到 perm 级别。 RPC 错误是比较典型的临时错误类型。\n\n * perm(anent)，持续： 这个级别的错误通常出现在 sealing 的过程中，无法简单判断是否可以安全重试，或一旦修复会有较大的收益（如不必重新完成 pre commit phase1）。 这类错误会阻塞 worker 线程，直到人工介入完成处理。\n\n * crit(tical)，严重： 严重级别的错误在各个方面都与持续级别的错误比较相似。 比较显著的区别是，严重级别的错误通常明确来自运行的环境而非 sealing 的过程。 如可甄别的本地文件系统异常、本地持久化数据库异常等都归入此类。 严重级别的错误同样也会阻塞 worker 线程直到人工介入。\n\n * abort，终止： 遇到这个级别的错误，worker 会直接放弃当前的 sealing 进度，尝试重新开始一个新的流程。\n\n这使得 venus-worker 能够在更多不需要人工介入的异常场景下自行恢复，同时也给予将来用户定制异常处理策略提供了空间。\n\n# 降低部署复杂度\n\nvenus-worker 降低部署复杂度的努力主要体现在三个方面：\n\n * 降低实例数\n * 减少差异化的配置\n * 降低硬件失效，尤其是存储失效影响的范围\n\nvenus-worker 是以一台设备一个实例的部署方式为目标设计的，同时内部融入了精简但足够的资源控制和隔离方式。使用者可以根据实际情况决定资源的分配策略以期达到最高的使用效率。同时，扇区粒度的任务隔离也可以做到确保部分硬件的失效不会蔓延影响到其他扇区任务。\n\nvenus-sector-manager 被设计为一个实例可以为多个不同的 SP 服务，同时得益于 venus 系列链服务组件的设计，可以达成一些不太常见的运行模式：\n\n * SP 联合体\n * 使用云厂商设备的多算力集群协同\n * 多 SP 共享算力设备下的动态调配\n\n# 灵活替换功能组件\n\nvenus-cluster 内的大量功能组件都是按照先抽象接口，再具体实现的路径设计的。这样做可以确保我们仅对必要的部分进行约束，保留不同实现并存的可能性，从而能够更容易地提供对诸如：\n\n * 定制化或闭源的封装算法执行器\n\n * 共享的订单数据存储\n\n * 部分阶段外包服务\n\n等功能的支持。\n\n\n# 总结\n\n我们并不将 venus-cluster 定义为一套全能的封装集群方案，我们希望它是在特定的场景下，满足对简洁、高效、灵活有需求的使用者的一种选择。\n\n同时我们期待有更多的参与者为其引入更丰富的功能组件实现。\n\n我们认为扩大“特定场景”的覆盖范围有助于扩大 venus-cluster 的受众，但也不会贸然允诺对于 venus-cluster 的全部功能需求，尤其是那些对其自身特质有影响的部分。",normalizedContent:"# 简述\n\n我们的不少合作伙伴曾大量、深度使用过 lotus 的封装组件，也不乏自行定制的经历。\n\n使用过程中，他们遇到不少痛点，也总结出许多经验。我们将这些思考汇总起来，尝试提供一套与 lotus 封装组件思路不完全相同、以简洁、效率和灵活为最优先考虑，但同样具备竞争力的集群方案，供广大的 sp （storage provider，存储供应商）选择。\n\n\n# 设计思路\n\nvenus-cluster 主要包含 venus-sector-manager 和 venus-worker 两个组件，其功能大致与 lotus-miner 和 lotus-seal-worker 的组合相当。\n\nvenus-cluster 着眼于实现一套满足：\n\n * 面向大量同质化计算设备，降低硬件需求类型、实例数量和部署复杂度\n * 尽可能针对不同环节、不同类型的异常，安全地完成“自愈”，减少人工介入、提供生产效率\n * 依托云服务、外包服务及其他可共享的基础设施，降低门槛，削减不必要的成本\n\n等目标的高效封装集群方案。\n\n当然，明确且针对性强的设计意图可能导致这套方案并不普适于所有用户。例如，如果使用者已经拥有大量针对封装的不同阶段单独配置、形成搭配的设备，那么 venus-cluster 方案可能并不能有效地提升。具体情形，可以在阅读完后续文档后，由用户自行判断。\n\n\n# lotus 封装组件的模式\n\n在介绍 venus-cluster 的具体设计思路之前，我们有必要回顾一下 louts 相关组件的工作模式。\n\nlotus 的封装组件是一套标准的中心化调度集群：\n\n * lotus-miner 是中心调度服务，对任务进行分配、管理；\n * lotus-seal-worker 是封装执行体，负责完成分配到其之上的具体任务，并反馈给 lotus-miner；\n * lotus-miner 作为发起方，与 lotus-seal-worker 之间通过 rpc 交互。\n\n这套设计优点十分明显：\n\n * lotus-seal-worker 是几乎无状态的工作进程，理论上很方便进行横向扩容；\n * lotus-miner 可以灵活地将任务在 lotus-worker 之间进行调度。\n\n但是这套机制也存在另一面：\n\n 1. lotus-miner 作为调度核心：\n    \n    * 承担着繁重的、不同类型的逻辑，如任务调度和管理，与链进行交互等\n    * 保留着需要同时兼容本地模式和远程模式的历史负担\n    \n    导致内部逻辑（状态机）复杂、异常处理难以细化。\n\n 2. lotus-seal-worker 被完全设计为无状态的模式。\n\n继而产生了一些问题，例如：\n\n 1. 由于各个阶段之间对硬件资源的要求迥异，为了保证资源的合理分配，通常会将 lotus-seal-worker 设置成每个实例支持一个阶段任务的模式，在同一机器上部署多个不同的实例，通过操作系统级别的资源控制（cgroup、docker、numa 等）进行隔离。\n    \n    这导致在大规模集群中， lotus-seal-worker 数量众多，编排和管理都较为复杂。\n\n 2. 任何中心化任务调度机制都很有可能需要考虑计算和存储资源的匹配、亲和性等。\n    \n    我们说 lotus-seal-worker 几乎无状态，但是事实上，扇区封装过程中存在着 临时文件 这样一种特殊的“上下文”。对于这类“上下文”，lotus-miner 长期以来简单地对其进行整体搬移。这导致了巨大的网络带宽开销，以及一系列随之产生的网络、存储异常的可能性。\n    \n    直到 pr 7453 提出“存储分组”概念之后，亲和性问题才初步解决，但这一方案给 worker 的编排又增加了额外的复杂度。\n\n 3. 大规模集群中存在着大量可能导致任务异常的因素，包括但不限于：计算错误、存储失效、网络抖动。而 lotus-miner 内部缺乏有效感知异常根因的手段，使得其难以根据具体原因选择不同的异常恢复机制，只能统一粗粒度地处理或等待人工介入。\n\n\n# venus-cluster 的设计思路\n\n基于我们的目标和过去的经验，venus-cluster 在设计之初就确立了几点思路：\n\n * 简化逻辑\n * 优化流程\n * 隔离异常\n * 降低部署复杂度\n * 灵活替换功能组件\n\n下面我们会具体进行阐述。\n\n# 简化逻辑\n\n我们重新规划了上下游组件之间的责任划分：\n\n * venus-sector-manager 负责：\n   * 扇区封装过程中与链进行交互的部分，如 ticket 和 seed 的获取，消息的独立或聚合提交等\n   * 扇区封装任务的状态记录，如当前阶段、异常及原因等\n   * 已完成扇区的管理和维持，如响应 windowpost 等\n * venus-worker 负责：\n   * 管理各个阶段计算资源的分配\n   * 使用预先规划好的本地存储空间完成扇区封装的完整过程\n\n通过这样的划分，使得 venus-sector-manager 只需被动地接收必须传递的信息，不必实现一套复杂的感知和调度系统；同时 venus-worker 直接管理各类本地异常，便于执行不同的处理策略。\n\n这样，两者的业务逻辑都不会频繁出现分支，相对便于理解及修改。\n\n# 优化流程\n\n在 lotus 的封装流程中，最大的问题就是上文提到的，对封装过程中的临时文件的处理。此外还存在一些不尽如人意的点，例如：\n\n * 订单的等待逻辑可能会使计算资源闲置\n * 存储资源和计算资源的强制一对一匹配\n * 部分阶段，内部多个环节对资源的需求度不一致\n * 将与链的交互和扇区封装计算捆绑，带来的存储空间释放不及时等\n\n等。这些都会到封装的整体效率产生影响。\n\n# 隔离异常\n\n一方面，主要的异常处理被移动到了 venus-worker 一层，且在与 venus-sector-manager 交互的接口设计中，就尽早考虑了区分网络异常和逻辑异常，从而使得 venus-worker 比较容易感知到异常的具体原因。\n\n另一方面，我们将封装过程中可能产生的错误类型归为四个级别，并设定了不同的处理方式：\n\n * temp(orary)，临时： 这个级别的错误通常明确属于临时性的，或者我们知道重试不会带来负面影响的。 对于这类错误，worker 会自动尝试重试（以 recover_interval 为间隔，最多 max_retries 次）。 当重试次数超过设定的上限时，会自动升级到 perm 级别。 rpc 错误是比较典型的临时错误类型。\n\n * perm(anent)，持续： 这个级别的错误通常出现在 sealing 的过程中，无法简单判断是否可以安全重试，或一旦修复会有较大的收益（如不必重新完成 pre commit phase1）。 这类错误会阻塞 worker 线程，直到人工介入完成处理。\n\n * crit(tical)，严重： 严重级别的错误在各个方面都与持续级别的错误比较相似。 比较显著的区别是，严重级别的错误通常明确来自运行的环境而非 sealing 的过程。 如可甄别的本地文件系统异常、本地持久化数据库异常等都归入此类。 严重级别的错误同样也会阻塞 worker 线程直到人工介入。\n\n * abort，终止： 遇到这个级别的错误，worker 会直接放弃当前的 sealing 进度，尝试重新开始一个新的流程。\n\n这使得 venus-worker 能够在更多不需要人工介入的异常场景下自行恢复，同时也给予将来用户定制异常处理策略提供了空间。\n\n# 降低部署复杂度\n\nvenus-worker 降低部署复杂度的努力主要体现在三个方面：\n\n * 降低实例数\n * 减少差异化的配置\n * 降低硬件失效，尤其是存储失效影响的范围\n\nvenus-worker 是以一台设备一个实例的部署方式为目标设计的，同时内部融入了精简但足够的资源控制和隔离方式。使用者可以根据实际情况决定资源的分配策略以期达到最高的使用效率。同时，扇区粒度的任务隔离也可以做到确保部分硬件的失效不会蔓延影响到其他扇区任务。\n\nvenus-sector-manager 被设计为一个实例可以为多个不同的 sp 服务，同时得益于 venus 系列链服务组件的设计，可以达成一些不太常见的运行模式：\n\n * sp 联合体\n * 使用云厂商设备的多算力集群协同\n * 多 sp 共享算力设备下的动态调配\n\n# 灵活替换功能组件\n\nvenus-cluster 内的大量功能组件都是按照先抽象接口，再具体实现的路径设计的。这样做可以确保我们仅对必要的部分进行约束，保留不同实现并存的可能性，从而能够更容易地提供对诸如：\n\n * 定制化或闭源的封装算法执行器\n\n * 共享的订单数据存储\n\n * 部分阶段外包服务\n\n等功能的支持。\n\n\n# 总结\n\n我们并不将 venus-cluster 定义为一套全能的封装集群方案，我们希望它是在特定的场景下，满足对简洁、高效、灵活有需求的使用者的一种选择。\n\n同时我们期待有更多的参与者为其引入更丰富的功能组件实现。\n\n我们认为扩大“特定场景”的覆盖范围有助于扩大 venus-cluster 的受众，但也不会贸然允诺对于 venus-cluster 的全部功能需求，尤其是那些对其自身特质有影响的部分。",charsets:{cjk:!0}},{title:"概念和基础设施",frontmatter:{},regularPath:"/zh/developer/concept.html",relativePath:"zh/developer/concept.md",key:"v-e7947532",path:"/zh/developer/concept.html",headers:[{level:2,title:"公共部分",slug:"公共部分",normalizedTitle:"公共部分",charIndex:72},{level:3,title:"ActorID",slug:"actorid",normalizedTitle:"actorid",charIndex:81},{level:3,title:"objstore",slug:"objstore",normalizedTitle:"objstore",charIndex:350},{level:3,title:"piece store",slug:"piece-store",normalizedTitle:"piece store",charIndex:776},{level:2,title:"damocles-worker 部分",slug:"damocles-worker-部分",normalizedTitle:"damocles-worker 部分",charIndex:952},{level:3,title:"sealing store",slug:"sealing-store",normalizedTitle:"sealing store",charIndex:975},{level:3,title:"remote store",slug:"remote-store",normalizedTitle:"remote store",charIndex:1364},{level:3,title:"processor",slug:"processor",normalizedTitle:"processor",charIndex:1628},{level:2,title:"damocles-manager 部分",slug:"damocles-manager-部分",normalizedTitle:"damocles-manager 部分",charIndex:3012},{level:3,title:"基础服务 API",slug:"基础服务-api",normalizedTitle:"基础服务 api",charIndex:3036},{level:3,title:"RandomnessAPI",slug:"randomnessapi",normalizedTitle:"randomnessapi",charIndex:3381},{level:3,title:"MinerInfoAPI",slug:"minerinfoapi",normalizedTitle:"minerinfoapi",charIndex:3531},{level:3,title:"SectorManager",slug:"sectormanager",normalizedTitle:"sectormanager",charIndex:3584},{level:3,title:"DealManager",slug:"dealmanager",normalizedTitle:"dealmanager",charIndex:3677},{level:3,title:"CommitmentManager",slug:"commitmentmanager",normalizedTitle:"commitmentmanager",charIndex:3747},{level:3,title:"SectorStateManager",slug:"sectorstatemanager",normalizedTitle:"sectorstatemanager",charIndex:3846},{level:3,title:"SectorIndexer",slug:"sectorindexer",normalizedTitle:"sectorindexer",charIndex:3923}],headersStr:"公共部分 ActorID objstore piece store damocles-worker 部分 sealing store remote store processor damocles-manager 部分 基础服务 API RandomnessAPI MinerInfoAPI SectorManager DealManager CommitmentManager SectorStateManager SectorIndexer",content:"# 概念和基础设施\n\ndamocles 包含了一系列的抽象、功能组件、基础设施。了解这些内容有助于我们理解 damocles 的运转。\n\n\n# 公共部分\n\n\n# ActorID\n\nActorID 是在 damocles 组件中使用的、SP 的标识格式，为数字类型。其与 SP 的地址是一一对应的关系。\n\n例如：\n\n * 在测试网络中，t01234 这样一个 SP 地址与 1234 这样一个 ActorID 一一对应\n * 在主网中，f02468 这样一个 SP 地址与 2468 这样一个 ActorID 一一对应\n\n之所以统一使用 ActorID 这样的标识格式，是为了：\n\n * 方便辨识和书写\n\n * 避免地址中的网络标识 (t，f) 和类型标识 (t0 中的 0) 可能带来的混淆\n\n\n# objstore\n\nobjstore 是基于对象存储模式的存储基础设施抽象。\n\n我们知道，在 Filecoin 中，与数据交互的各个环节广泛使用了基于文件系统的存储设施抽象。但在实践过程中，我们发现，除了一些相对基础的数据访问模式外，文件系统提供的大量特性并未被使用。\n\n经过分析，我们认为：\n\n 1. 在许多场景下，基本的对象存储抽象已经能够满足 Filecoin 的需求\n 2. 无论是搭建本地的大规模高可用分布式存储集群，还是使用已有的商业化存储方案，对象存储都有大量选项\n 3. 存量的、基于文件系统的存储可以经由简单的代理层完成向对象存储接口的转换\n\n当然，由于在算法层面，一些关键的环节还不支持基于对象存储的抽象（例如 MerkleStore），我们目前仅仅是将文件系统转换为对象存储的形式。希望在将来，通过社区推动Filecoin 算法层面对于对象存储的原生支持落地，令使用者能够按照自己的需求使用适合的存储方案，甚至混合方案。\n\n\n# piece store\n\npiece store 是在存储订单封装场景中使用的，用于访问订单 piece 数据的存储抽象。\n\n这里的 piece 数据，是指用户发出的、未经过填充 (padding) 和 FR32 转换的原始数据内容。\n\n由于 damocles-cluster 并不会涉及 piece 数据的写入，因此只提供了数据读取的接口。\n\n\n# damocles-worker 部分\n\n\n# sealing store\n\nsealing store 是位于 damocles-worker 所在宿主机的存储设施，用于临时存放扇区封装过程中产生的数据文件。通常由高速本地存储设备（如 nvme）构成。\n\nsealing store的使用遵循：\n\n * 一个扇区唯一对应一个 sealing store\n * 一个 sealing store 在同一时间只会存在一个扇区\n\n每个 sealing store 中都会包含 meta 和 data 两个子目录，分别存放正在进行中的扇区的状态和封装数据。这样，在sealing store 之间，状态数据不会相互影响。即使部分sealing store的底层存储设备损坏，影响面也会局限在它所容纳的 sealing store 范围内。\n\n通常来说，sealing store 会根据当前宿主机上存储资源数量来进行规划。\n\n\n# remote store\n\nremote store 是扇区数据的永久存储设施，通常是一个存储集群。\n\n目前在 damocles-worker 中，remote store 实现为一套在文件系统之上的对象存储接口封装。\n\n完成封装、等待上链的扇区数据会从 sealing store 移动到 remote store。\n\n实际上， remote store 更多相对 sealing store 的本地而言。对于 damocles-manager 和 damocles-worker 来说，都需要能够访问这一存储设施。\n\n\n# processor\n\nprocessor 是扇区封装步骤的执行器。通常来说，每一个独立的步骤会对应一类 processor，如 pc1 processor, c2 processor 等。使用者可以针对每一类 processor 设置其并行数量。\n\n通常来说，processor 会根据当前宿主机上的计算资源数量来进行规划。\n\n# external processor\n\nexternal processor 是一类特殊的 processor，它们以子进程的形式存在，通过确定的协议，与主进程完成任务上下文的交互，并具体执行某一特定的封装步骤。\n\nexternal processor 这一设定的存在，使得damocles 可以依托操作系统提供的接口，针对子进程进行计算资源的配置和隔离，例如\n\n * 通过 cgroup选项为 pc1 processor 指定 cpu sets，以避免 pc1 阶段的扇区相互抢占 cpu资源\n * 通过 numa 选项为 pc1 processor 指定内存分配的倾向性，以降低 pc1 阶段内存的访问延迟，提高 multicore_sdr 模式下的缓存预取效率\n * 通过英伟达默认的 GPU 相关环境变量，为多个 c2 prcessor 各自绑定唯一的可用 GPU，并将锁文件的位置分隔开\n\n等极其灵活的搭配方式。\n\n除了方便计算资源的配置和隔离之外，external processor 还使得 非通用或非公开的执行器实现 变得可能。任何满足上下文交互协议要求的可执行程序都可以作为 external processor的具体实现被集成进封装流程，这使得诸如：\n\n * 高度定制化的 c2 processor\n * 基于 GPU 外包的 pc2 processor 或 c2 processor\n\n等选项变得易于集成。使用者可以任意选择多种定制化的 external processor 进行组合，且这种选择不会受限于具体的 external processor 的开发和维护者是谁。\n\n# sealing store、processor 和 sector 的关系\n\n在之前的段落中我们提到，sealing store 和 processor 分别根据存储资源和计算资源进行规划，那么是否意味着他们之间并不需要 维持 1:1 的配比？\n\n答案是肯定的，原因如下：\n\n在任何一个特定阶段，一个 sector 所占用的资源是其所占用的计算资源（processor）和存储资源（sealing store）的组合。\n\n在 damocles-worker 的封装流程中，sector 从一个阶段进入下一个阶段时，会释放之前占用的计算资源，并尝试申请下一阶段所需要的计算资源。被释放的计算资源完全可以被分配给其他等待中的sector。\n\n举例来说：\n\n通常，基于成本和硬件规格考虑，可规划的 sealing store 数量往往多于可规划的 p1 processor 数量。在 damocles-worker 的封装中，如果一个 sector 完成了 p1 阶段的计算，那么释放出来的 p1 processor 就可以被用于等待中的其他 sealing store 上的 sector。\n\n这使得硬件资源的高密度利用变得可行。\n\n\n# damocles-manager 部分\n\n\n# 基础服务 API\n\n这是 damocles 依托的服务及其接口定义。\n\n这些服务和接口为 damocles 提供基础的、与链和其他参与者交互的能力。\n\n# chain.API\n\n这是一组定义在 venus/venus-shared 中的，链相关的接口定义。主要是向 damocles 提供基础的链服务接口。\n\n# messager.API\n\n这是一组定义在 venus/venus-shared 中的，消息相关的接口定义。主要是向 damocles 提供发送消息、确认上链和执行结果方面的基础能力。\n\n# market.API\n\n这是一组定义在 venus/venus-shared 中的，订单相关的接口定义。主要是向 damocles 提供订单分配、订单数据获取方面的基础能力。\n\n\n# RandomnessAPI\n\n这是对 chain.API 的一层简单封装，主要用于提供扇区封装、维持过程中所需的随机数信息。\n\n这一层封装使得 damocles-worker 及其他模块仅需要根据用途获取相应的结果，而不必在意具体的请求对象（Drand or Filecoin）或请求格式。\n\n\n# MinerInfoAPI\n\n这是对 chain.API 的一层封装，主要提供 SP 粒度的信息。\n\n\n# SectorManager\n\n这是管理扇区分配的模块。主要负责根据 damocles-worker 提交的参进行扇区编号的分配。\n\n可以为多个 SP 、不同的扇区大小提供支持。\n\n\n# DealManager\n\n这是管理订单的模块，主要负责为空白的扇区分配可容纳的订单 piece，以及对失败的扇区中的订单进行释放。\n\n\n# CommitmentManager\n\n这是管理扇区消息上链的模块。主要负责按照预先指定的策略对 PreCommit 和 ProveCommit 消息进行单条或聚合式的提交，并观察上链结果。\n\n\n# SectorStateManager\n\n这是管理进行中的扇区的状态的模块。主要负责接收来自 damocles-worker的状态和异常信息上报。\n\n\n# SectorIndexer\n\n这是管理已完成的扇区位置的模块。主要负责定位指定的扇区，常用在 PoSt 的计算过程中。",normalizedContent:"# 概念和基础设施\n\ndamocles 包含了一系列的抽象、功能组件、基础设施。了解这些内容有助于我们理解 damocles 的运转。\n\n\n# 公共部分\n\n\n# actorid\n\nactorid 是在 damocles 组件中使用的、sp 的标识格式，为数字类型。其与 sp 的地址是一一对应的关系。\n\n例如：\n\n * 在测试网络中，t01234 这样一个 sp 地址与 1234 这样一个 actorid 一一对应\n * 在主网中，f02468 这样一个 sp 地址与 2468 这样一个 actorid 一一对应\n\n之所以统一使用 actorid 这样的标识格式，是为了：\n\n * 方便辨识和书写\n\n * 避免地址中的网络标识 (t，f) 和类型标识 (t0 中的 0) 可能带来的混淆\n\n\n# objstore\n\nobjstore 是基于对象存储模式的存储基础设施抽象。\n\n我们知道，在 filecoin 中，与数据交互的各个环节广泛使用了基于文件系统的存储设施抽象。但在实践过程中，我们发现，除了一些相对基础的数据访问模式外，文件系统提供的大量特性并未被使用。\n\n经过分析，我们认为：\n\n 1. 在许多场景下，基本的对象存储抽象已经能够满足 filecoin 的需求\n 2. 无论是搭建本地的大规模高可用分布式存储集群，还是使用已有的商业化存储方案，对象存储都有大量选项\n 3. 存量的、基于文件系统的存储可以经由简单的代理层完成向对象存储接口的转换\n\n当然，由于在算法层面，一些关键的环节还不支持基于对象存储的抽象（例如 merklestore），我们目前仅仅是将文件系统转换为对象存储的形式。希望在将来，通过社区推动filecoin 算法层面对于对象存储的原生支持落地，令使用者能够按照自己的需求使用适合的存储方案，甚至混合方案。\n\n\n# piece store\n\npiece store 是在存储订单封装场景中使用的，用于访问订单 piece 数据的存储抽象。\n\n这里的 piece 数据，是指用户发出的、未经过填充 (padding) 和 fr32 转换的原始数据内容。\n\n由于 damocles-cluster 并不会涉及 piece 数据的写入，因此只提供了数据读取的接口。\n\n\n# damocles-worker 部分\n\n\n# sealing store\n\nsealing store 是位于 damocles-worker 所在宿主机的存储设施，用于临时存放扇区封装过程中产生的数据文件。通常由高速本地存储设备（如 nvme）构成。\n\nsealing store的使用遵循：\n\n * 一个扇区唯一对应一个 sealing store\n * 一个 sealing store 在同一时间只会存在一个扇区\n\n每个 sealing store 中都会包含 meta 和 data 两个子目录，分别存放正在进行中的扇区的状态和封装数据。这样，在sealing store 之间，状态数据不会相互影响。即使部分sealing store的底层存储设备损坏，影响面也会局限在它所容纳的 sealing store 范围内。\n\n通常来说，sealing store 会根据当前宿主机上存储资源数量来进行规划。\n\n\n# remote store\n\nremote store 是扇区数据的永久存储设施，通常是一个存储集群。\n\n目前在 damocles-worker 中，remote store 实现为一套在文件系统之上的对象存储接口封装。\n\n完成封装、等待上链的扇区数据会从 sealing store 移动到 remote store。\n\n实际上， remote store 更多相对 sealing store 的本地而言。对于 damocles-manager 和 damocles-worker 来说，都需要能够访问这一存储设施。\n\n\n# processor\n\nprocessor 是扇区封装步骤的执行器。通常来说，每一个独立的步骤会对应一类 processor，如 pc1 processor, c2 processor 等。使用者可以针对每一类 processor 设置其并行数量。\n\n通常来说，processor 会根据当前宿主机上的计算资源数量来进行规划。\n\n# external processor\n\nexternal processor 是一类特殊的 processor，它们以子进程的形式存在，通过确定的协议，与主进程完成任务上下文的交互，并具体执行某一特定的封装步骤。\n\nexternal processor 这一设定的存在，使得damocles 可以依托操作系统提供的接口，针对子进程进行计算资源的配置和隔离，例如\n\n * 通过 cgroup选项为 pc1 processor 指定 cpu sets，以避免 pc1 阶段的扇区相互抢占 cpu资源\n * 通过 numa 选项为 pc1 processor 指定内存分配的倾向性，以降低 pc1 阶段内存的访问延迟，提高 multicore_sdr 模式下的缓存预取效率\n * 通过英伟达默认的 gpu 相关环境变量，为多个 c2 prcessor 各自绑定唯一的可用 gpu，并将锁文件的位置分隔开\n\n等极其灵活的搭配方式。\n\n除了方便计算资源的配置和隔离之外，external processor 还使得 非通用或非公开的执行器实现 变得可能。任何满足上下文交互协议要求的可执行程序都可以作为 external processor的具体实现被集成进封装流程，这使得诸如：\n\n * 高度定制化的 c2 processor\n * 基于 gpu 外包的 pc2 processor 或 c2 processor\n\n等选项变得易于集成。使用者可以任意选择多种定制化的 external processor 进行组合，且这种选择不会受限于具体的 external processor 的开发和维护者是谁。\n\n# sealing store、processor 和 sector 的关系\n\n在之前的段落中我们提到，sealing store 和 processor 分别根据存储资源和计算资源进行规划，那么是否意味着他们之间并不需要 维持 1:1 的配比？\n\n答案是肯定的，原因如下：\n\n在任何一个特定阶段，一个 sector 所占用的资源是其所占用的计算资源（processor）和存储资源（sealing store）的组合。\n\n在 damocles-worker 的封装流程中，sector 从一个阶段进入下一个阶段时，会释放之前占用的计算资源，并尝试申请下一阶段所需要的计算资源。被释放的计算资源完全可以被分配给其他等待中的sector。\n\n举例来说：\n\n通常，基于成本和硬件规格考虑，可规划的 sealing store 数量往往多于可规划的 p1 processor 数量。在 damocles-worker 的封装中，如果一个 sector 完成了 p1 阶段的计算，那么释放出来的 p1 processor 就可以被用于等待中的其他 sealing store 上的 sector。\n\n这使得硬件资源的高密度利用变得可行。\n\n\n# damocles-manager 部分\n\n\n# 基础服务 api\n\n这是 damocles 依托的服务及其接口定义。\n\n这些服务和接口为 damocles 提供基础的、与链和其他参与者交互的能力。\n\n# chain.api\n\n这是一组定义在 venus/venus-shared 中的，链相关的接口定义。主要是向 damocles 提供基础的链服务接口。\n\n# messager.api\n\n这是一组定义在 venus/venus-shared 中的，消息相关的接口定义。主要是向 damocles 提供发送消息、确认上链和执行结果方面的基础能力。\n\n# market.api\n\n这是一组定义在 venus/venus-shared 中的，订单相关的接口定义。主要是向 damocles 提供订单分配、订单数据获取方面的基础能力。\n\n\n# randomnessapi\n\n这是对 chain.api 的一层简单封装，主要用于提供扇区封装、维持过程中所需的随机数信息。\n\n这一层封装使得 damocles-worker 及其他模块仅需要根据用途获取相应的结果，而不必在意具体的请求对象（drand or filecoin）或请求格式。\n\n\n# minerinfoapi\n\n这是对 chain.api 的一层封装，主要提供 sp 粒度的信息。\n\n\n# sectormanager\n\n这是管理扇区分配的模块。主要负责根据 damocles-worker 提交的参进行扇区编号的分配。\n\n可以为多个 sp 、不同的扇区大小提供支持。\n\n\n# dealmanager\n\n这是管理订单的模块，主要负责为空白的扇区分配可容纳的订单 piece，以及对失败的扇区中的订单进行释放。\n\n\n# commitmentmanager\n\n这是管理扇区消息上链的模块。主要负责按照预先指定的策略对 precommit 和 provecommit 消息进行单条或聚合式的提交，并观察上链结果。\n\n\n# sectorstatemanager\n\n这是管理进行中的扇区的状态的模块。主要负责接收来自 damocles-worker的状态和异常信息上报。\n\n\n# sectorindexer\n\n这是管理已完成的扇区位置的模块。主要负责定位指定的扇区，常用在 post 的计算过程中。",charsets:{cjk:!0}},{title:"venus-cluster 是什么？",frontmatter:{},regularPath:"/zh/intro/",relativePath:"zh/intro/README.md",key:"v-0d494d6c",path:"/zh/intro/",headers:[{level:2,title:"venus-cluster 是什么？",slug:"venus-cluster-是什么",normalizedTitle:"venus-cluster 是什么？",charIndex:2},{level:2,title:"功能特性",slug:"功能特性",normalizedTitle:"功能特性",charIndex:184},{level:3,title:"重新设计的任务调度",slug:"重新设计的任务调度",normalizedTitle:"重新设计的任务调度",charIndex:193},{level:3,title:"集群横向扩容",slug:"集群横向扩容",normalizedTitle:"集群横向扩容",charIndex:435},{level:3,title:"Post worker 分离",slug:"post-worker-分离",normalizedTitle:"post worker 分离",charIndex:620},{level:3,title:"池化 worker 资源",slug:"池化-worker-资源",normalizedTitle:"池化 worker 资源",charIndex:730},{level:3,title:"定制化封装任务",slug:"定制化封装任务",normalizedTitle:"定制化封装任务",charIndex:854}],lastUpdated:"2023/10/23 07:59:15",headersStr:"venus-cluster 是什么？ 功能特性 重新设计的任务调度 集群横向扩容 Post worker 分离 池化 worker 资源 定制化封装任务",content:"# venus-cluster 是什么？\n\nvenus-cluster是Venus研发团队基于大量的运维实践经验，经过不懈的技术攻坚与设计迭代，针对当前 Fielcoin 参考实现的算力服务进行大幅优化的，次世代集群算力服务方案。其三大特点为配置化，集群化，定制化。venus-cluster的整体介绍，欢迎参见这个Venus meetup的视频来了解更多。\n\n\n# 功能特性\n\n\n# 重新设计的任务调度\n\nvenus-cluster不是简单地对现有 Filecoin 参考实现的任务调度进行优化，而是打破了所有现有任务调度的格局，从零开始，重新设想了当前理想的任务调度方式。能够做到这样颠覆性的改变，得益于venus-cluster把状态机 (state machine) 的修改能力从中心化的任务调度lotus-miner/venus-sealer手中下放到了venus-worker上。这样使得worker不是被动的等待被分配任务而是主动去领取封装任务。\n\n\n# 集群横向扩容\n\n利用venus-cluster对硬件资源详尽的配置能力，一个存储提供者可以非常便捷的使用一台worker机器上的配置文件，将其运用到另外一台硬件配置相同的worker机器上，并得到与前一台worker机器相同算力产能。这样就可使得集群能够快速横向扩容，降低提高集群产能带来的运维难度以及集群风险。而不是像当前参考实现中，为了扩容牵一发而动全身。\n\n\n# Post worker 分离\n\nvenus-cluster支持部署专门运算时空证明的worker机器。这样，windowPost和winningPost就不必为与封装任务抢夺资源，导致算力和出块损失，而担心了。\n\n\n# 池化 worker 资源\n\n得益于全新的任务调度模型，使用venus-cluster的算力服务方案可以同时为多个节点服务。只需要通过简单的配置，您的worker算力机便能充分利用其价值，为多个矿工节点号提供算力服务（复制与时空证明服务）。\n\n\n# 定制化封装任务\n\n如果存储提供者有比较强的编程背景，能够针对某个封装阶段进行代码优化。那么存储提供者将无需再维护一个自己优化过的 Filecoin 参考实现，而是直接将该封装阶段的优化代码封装为一个可执行文件bin，直接配置到venus-cluster中让其运行优化后的代码。",normalizedContent:"# venus-cluster 是什么？\n\nvenus-cluster是venus研发团队基于大量的运维实践经验，经过不懈的技术攻坚与设计迭代，针对当前 fielcoin 参考实现的算力服务进行大幅优化的，次世代集群算力服务方案。其三大特点为配置化，集群化，定制化。venus-cluster的整体介绍，欢迎参见这个venus meetup的视频来了解更多。\n\n\n# 功能特性\n\n\n# 重新设计的任务调度\n\nvenus-cluster不是简单地对现有 filecoin 参考实现的任务调度进行优化，而是打破了所有现有任务调度的格局，从零开始，重新设想了当前理想的任务调度方式。能够做到这样颠覆性的改变，得益于venus-cluster把状态机 (state machine) 的修改能力从中心化的任务调度lotus-miner/venus-sealer手中下放到了venus-worker上。这样使得worker不是被动的等待被分配任务而是主动去领取封装任务。\n\n\n# 集群横向扩容\n\n利用venus-cluster对硬件资源详尽的配置能力，一个存储提供者可以非常便捷的使用一台worker机器上的配置文件，将其运用到另外一台硬件配置相同的worker机器上，并得到与前一台worker机器相同算力产能。这样就可使得集群能够快速横向扩容，降低提高集群产能带来的运维难度以及集群风险。而不是像当前参考实现中，为了扩容牵一发而动全身。\n\n\n# post worker 分离\n\nvenus-cluster支持部署专门运算时空证明的worker机器。这样，windowpost和winningpost就不必为与封装任务抢夺资源，导致算力和出块损失，而担心了。\n\n\n# 池化 worker 资源\n\n得益于全新的任务调度模型，使用venus-cluster的算力服务方案可以同时为多个节点服务。只需要通过简单的配置，您的worker算力机便能充分利用其价值，为多个矿工节点号提供算力服务（复制与时空证明服务）。\n\n\n# 定制化封装任务\n\n如果存储提供者有比较强的编程背景，能够针对某个封装阶段进行代码优化。那么存储提供者将无需再维护一个自己优化过的 filecoin 参考实现，而是直接将该封装阶段的优化代码封装为一个可执行文件bin，直接配置到venus-cluster中让其运行优化后的代码。",charsets:{cjk:!0}},{frontmatter:{},regularPath:"/zh/intro/_Footer.html",relativePath:"zh/intro/_Footer.md",key:"v-20079582",path:"/zh/intro/_Footer.html",lastUpdated:"2023/10/23 07:59:15",headersStr:null,content:"See something missing? Have tips to share? File an issue, and we'll follow up as soon as possible. (If you have write permission in this repo, feel free to edit directly.)",normalizedContent:"see something missing? have tips to share? file an issue, and we'll follow up as soon as possible. (if you have write permission in this repo, feel free to edit directly.)",charsets:{}},{title:"背景",frontmatter:{},regularPath:"/zh/intro/architecture.html",relativePath:"zh/intro/architecture.md",key:"v-4fa6a05f",path:"/zh/intro/architecture.html",headers:[{level:2,title:"背景",slug:"背景",normalizedTitle:"背景",charIndex:2},{level:2,title:"架构",slug:"架构",normalizedTitle:"架构",charIndex:456},{level:3,title:"封装管道优化",slug:"封装管道优化",normalizedTitle:"封装管道优化",charIndex:703},{level:3,title:"运维优化",slug:"运维优化",normalizedTitle:"运维优化",charIndex:1064},{level:3,title:"配置文件架构",slug:"配置文件架构",normalizedTitle:"配置文件架构",charIndex:1485}],lastUpdated:"2023/10/23 07:59:15",headersStr:"背景 架构 封装管道优化 运维优化 配置文件架构",content:"# 背景\n\nvenus-cluster诞生的大背景是，大量存储提供者加入到Filecoin存储提供网络之后，发现 filecoin 扇区封装过程中存在着种种问题和可以优化的部分。这边暂时列举了 3 个。\n\n * 第一，调度问题，lotus-miner 作为调度的核心，中心化负责扇区状态机的更新，导致管理和编排 worker 任务繁重，worker 专攻某封装阶段的模式，造成扇区临时文件在集群中的无谓传输和磁盘 io 的浪费。\n * 第二，扩容问题，调配或者扩容，封装管道的硬件和软件配置方案在现有 lotus 实现中对运维要求非常非常高，封装时需要用到的，计算和存储资源往往不能被有效利用。\n * 第三，自定义代码问题，很多有研发背景的存储提供者对扇区封装的不同阶段可能会有一些自定义的代码优化，修改 lotus 代码之后，后期网络升级等造成的代码维护都是不小的挑战。\n\nvenus-cluster便是针对上述种种疼点，Venus研发团经过大量的运维实践和对技术的攻坚，最终形成的一套算力服务方案。\n\n\n# 架构\n\nSealer是Filecoin中负责算力增长和算力维持的一个子系统。他之前由lotus-miner+lotus-worker体系亦或是由venus-sealer+venus-worker体系负责，而在venus-cluster中，Sealer子系统由venus-sector-manager+venus-worker的体系全权负责。需要注意的是由venus-sealer编译出来的venus-worker和venus-cluster中编译出来的worker是完全不一样的东西。\n\n\n# 封装管道优化\n\n需要注意的是venus-cluster的中一些独有的概念，从根本上改变了封装管道的架构。\n\n * 第一个非常重要的概念，和当前封装模式完全不一样的是，使用 venus-cluster 封装单个扇区的过程会在一台机器上完成。这意味着扇区封装所需要的临时文件不会在集群之间做无谓的传输，浪费集群磁盘读写和网络资源。\n * 第二个巨大的改变就是从以前 lotus-miner 或者 venus-sealer 主动的，中心化的管理 worker 的封装任务，到现在 venus-worker 自己能够自主的领任务，一台 worker 完成单个扇区的所有封装任务。\n * 第三个，封装需要用到的计算和存储资源，深度，灵活的配置管理，使得封装任务资源隔离，互不相争，高效利用。为稳定，高产的封装管道保驾护航！\n\n\n# 运维优化\n\n基于上述的一些根本改变，venus-cluster在运维管理存储集群事也有多方面的优化。\n\nWith architectural changes of venus-cluster, it presents new streamlined ways of managing your storage system.\n\n * 第一，时空证明 worker 与封装管道的完全解偶，windowPost 和 winningPost 可单独部署在专门的机器上，防止资源竞争导致的 windowPost 失败和出块失败。避免集群收到惩罚，确保得到挖矿收益。\n * 第二，部署复杂度大大降低，相同配置的硬件使用相同 worker 配置文件，使得部署速度大大提升，极大的方便了封装管道的横向扩容。\n * 第三，得益于优化后的封装管道，封装任务现在有了更聪明的重试机制。在不得不需要人工介入的情况下，问题的快速定位。使得运维排查更加高效。\n\n\n# 配置文件架构\n\n\n\n深入展开venus-cluster配置架构，请看上面，venus-sector-manager，venus-worker, 和配置文件，和封装所需计算与存储资源，之间关系的一张大的架构图。左边是sector-manager。他有一份自己的配置文件。右边就是多个不同的venus-worker。我们深度展开第一个venus-worker，venus-worker包含三块内容，venus-worker机器上的存储资源（像是 NVMe 或者 SSD 的硬盘），然后中间的是，venus-worker自身的配置文件，最后，最下面的这个是 venus-worker 机器上计算资源（像是 CPU，RAM，GPU）。\n\nsector-manager配置文件指向venus-worker配置文件的箭头具体指的是，venus-worker部分配置可以从sector-manager继承，也可以覆盖sector-manager的部分配置。venus-worker配置文件中非常重要的 2 个配置，需要着重关注的是，[[sealing_thread]]和[[processors]]这 2 个配置。这 2 个配置分别代表了，封装管道优化一个很大的难点，即是存储资源和计算资源匹配的问题。\n\n在venus-worker的配置文件中，[[sealing_thread]]配置了，封装任务的控制和存储资源的规划，而[[processors]]则配置了计算资源的规划。上图中在venus-worker配置文件中配置一个[[sealing_thread]]便会在venus-worker机器能够访问使用到的存储资源中画出一片区域。该区域将被用于存储单个扇区封装过程中的零食文件。每个[[sealing_thread]]画出的区域互不干扰。都将被用于存储，封装单个扇区所需的临时文件。那么单台venus-worker机器能同时封装扇区的最大数量便为 存储资源大小 / 扇区临时文件大小。举个例子，venus-worker机器有 2000G 的存储资源，封装扇区临时文件大小为 520G，那么单台venus-worker机器能同时封装扇区的最大数量便为 2000 / 520。\n\n那么最后，[[processors]]，[[processors]]配置对应的是计算资源的规划与相互隔离，上图中在venus-worker配置文件中配置一个[[processors.pc1]]便会在venus-worker机器的计算资源中画出一片区域，专门用于 pc1 任务的计算。如果像图中，配置了 3 个[[processors.pc1]]，那么该台venus-worker机器就能并行 3 个pc1。[[processors.pc2]]，[[processors.c2]]，[[processors.tree_d]]也是相同的道理。但是所有[[processors]]配置文件中的规划的计算资源总和不能超过该台venus-worker机器的计算资源。",normalizedContent:"# 背景\n\nvenus-cluster诞生的大背景是，大量存储提供者加入到filecoin存储提供网络之后，发现 filecoin 扇区封装过程中存在着种种问题和可以优化的部分。这边暂时列举了 3 个。\n\n * 第一，调度问题，lotus-miner 作为调度的核心，中心化负责扇区状态机的更新，导致管理和编排 worker 任务繁重，worker 专攻某封装阶段的模式，造成扇区临时文件在集群中的无谓传输和磁盘 io 的浪费。\n * 第二，扩容问题，调配或者扩容，封装管道的硬件和软件配置方案在现有 lotus 实现中对运维要求非常非常高，封装时需要用到的，计算和存储资源往往不能被有效利用。\n * 第三，自定义代码问题，很多有研发背景的存储提供者对扇区封装的不同阶段可能会有一些自定义的代码优化，修改 lotus 代码之后，后期网络升级等造成的代码维护都是不小的挑战。\n\nvenus-cluster便是针对上述种种疼点，venus研发团经过大量的运维实践和对技术的攻坚，最终形成的一套算力服务方案。\n\n\n# 架构\n\nsealer是filecoin中负责算力增长和算力维持的一个子系统。他之前由lotus-miner+lotus-worker体系亦或是由venus-sealer+venus-worker体系负责，而在venus-cluster中，sealer子系统由venus-sector-manager+venus-worker的体系全权负责。需要注意的是由venus-sealer编译出来的venus-worker和venus-cluster中编译出来的worker是完全不一样的东西。\n\n\n# 封装管道优化\n\n需要注意的是venus-cluster的中一些独有的概念，从根本上改变了封装管道的架构。\n\n * 第一个非常重要的概念，和当前封装模式完全不一样的是，使用 venus-cluster 封装单个扇区的过程会在一台机器上完成。这意味着扇区封装所需要的临时文件不会在集群之间做无谓的传输，浪费集群磁盘读写和网络资源。\n * 第二个巨大的改变就是从以前 lotus-miner 或者 venus-sealer 主动的，中心化的管理 worker 的封装任务，到现在 venus-worker 自己能够自主的领任务，一台 worker 完成单个扇区的所有封装任务。\n * 第三个，封装需要用到的计算和存储资源，深度，灵活的配置管理，使得封装任务资源隔离，互不相争，高效利用。为稳定，高产的封装管道保驾护航！\n\n\n# 运维优化\n\n基于上述的一些根本改变，venus-cluster在运维管理存储集群事也有多方面的优化。\n\nwith architectural changes of venus-cluster, it presents new streamlined ways of managing your storage system.\n\n * 第一，时空证明 worker 与封装管道的完全解偶，windowpost 和 winningpost 可单独部署在专门的机器上，防止资源竞争导致的 windowpost 失败和出块失败。避免集群收到惩罚，确保得到挖矿收益。\n * 第二，部署复杂度大大降低，相同配置的硬件使用相同 worker 配置文件，使得部署速度大大提升，极大的方便了封装管道的横向扩容。\n * 第三，得益于优化后的封装管道，封装任务现在有了更聪明的重试机制。在不得不需要人工介入的情况下，问题的快速定位。使得运维排查更加高效。\n\n\n# 配置文件架构\n\n\n\n深入展开venus-cluster配置架构，请看上面，venus-sector-manager，venus-worker, 和配置文件，和封装所需计算与存储资源，之间关系的一张大的架构图。左边是sector-manager。他有一份自己的配置文件。右边就是多个不同的venus-worker。我们深度展开第一个venus-worker，venus-worker包含三块内容，venus-worker机器上的存储资源（像是 nvme 或者 ssd 的硬盘），然后中间的是，venus-worker自身的配置文件，最后，最下面的这个是 venus-worker 机器上计算资源（像是 cpu，ram，gpu）。\n\nsector-manager配置文件指向venus-worker配置文件的箭头具体指的是，venus-worker部分配置可以从sector-manager继承，也可以覆盖sector-manager的部分配置。venus-worker配置文件中非常重要的 2 个配置，需要着重关注的是，[[sealing_thread]]和[[processors]]这 2 个配置。这 2 个配置分别代表了，封装管道优化一个很大的难点，即是存储资源和计算资源匹配的问题。\n\n在venus-worker的配置文件中，[[sealing_thread]]配置了，封装任务的控制和存储资源的规划，而[[processors]]则配置了计算资源的规划。上图中在venus-worker配置文件中配置一个[[sealing_thread]]便会在venus-worker机器能够访问使用到的存储资源中画出一片区域。该区域将被用于存储单个扇区封装过程中的零食文件。每个[[sealing_thread]]画出的区域互不干扰。都将被用于存储，封装单个扇区所需的临时文件。那么单台venus-worker机器能同时封装扇区的最大数量便为 存储资源大小 / 扇区临时文件大小。举个例子，venus-worker机器有 2000g 的存储资源，封装扇区临时文件大小为 520g，那么单台venus-worker机器能同时封装扇区的最大数量便为 2000 / 520。\n\n那么最后，[[processors]]，[[processors]]配置对应的是计算资源的规划与相互隔离，上图中在venus-worker配置文件中配置一个[[processors.pc1]]便会在venus-worker机器的计算资源中画出一片区域，专门用于 pc1 任务的计算。如果像图中，配置了 3 个[[processors.pc1]]，那么该台venus-worker机器就能并行 3 个pc1。[[processors.pc2]]，[[processors.c2]]，[[processors.tree_d]]也是相同的道理。但是所有[[processors]]配置文件中的规划的计算资源总和不能超过该台venus-worker机器的计算资源。",charsets:{cjk:!0}},{title:"快速启用",frontmatter:{},regularPath:"/zh/intro/getting-started.html",relativePath:"zh/intro/getting-started.md",key:"v-f05138fa",path:"/zh/intro/getting-started.html",headers:[{level:2,title:"准备工作",slug:"准备工作",normalizedTitle:"准备工作",charIndex:11},{level:2,title:"Mock 模式",slug:"mock-模式",normalizedTitle:"mock 模式",charIndex:510},{level:3,title:"venus-sector-manager",slug:"venus-sector-manager",normalizedTitle:"venus-sector-manager",charIndex:297},{level:3,title:"venus-worker",slug:"venus-worker",normalizedTitle:"venus-worker",charIndex:282},{level:2,title:"生产模式",slug:"生产模式",normalizedTitle:"生产模式",charIndex:1222},{level:3,title:"venus-sector-manager",slug:"venus-sector-manager-2",normalizedTitle:"venus-sector-manager",charIndex:297},{level:3,title:"venus-worker",slug:"venus-worker-2",normalizedTitle:"venus-worker",charIndex:282}],lastUpdated:"2023/10/23 07:59:15",headersStr:"准备工作 Mock 模式 venus-sector-manager venus-worker 生产模式 venus-sector-manager venus-worker",content:"# 快速启用\n\n\n# 准备工作\n\n 1. 安装必要的第三方库。\n    \n    这一部分可以参考 lotus 文档中的相应部分 building-from-source。\n\n 2. 下载代码库\n    \n    git clone https://github.com/ipfs-force-community/venus-cluster.git\n    \n\n 3. 编译 venus-cluster 的组件\n    \n    cd venus-cluster\n    make all\n    \n    \n    完成后，在 ./dist/bin 目录下会有 venus-worker 和 venus-sector-manager 两个可执行文件。\n\n 4. 分发可执行文件到需要的机器上。\n\n 5. 将 ./venus-worker/create-cgroup.sh 分发到 venus-worker 所在的机器上，并以准备运行 venus-worker 的系统用户身份执行。\n    \n    这会为这样的用户生成相应的 cgroup 组，以便venus-worker 为其外部执行器进程分配硬件资源。\n\n\n# Mock 模式\n\n默认情况下，可以通过一系列命令在单机上启动一组 mock 实例。\n\n\n# venus-sector-manager\n\n通过\n\n./dist/bin/venus-sector-manager mock --miner=10000 --sector-size=2KiB\n\n\n命令启动一个模拟为 Actor 为 t010000 的 SP 分配 2KiB 扇区的 venus-sector-manager 服务。\n\n这一步骤也可以通过代码目录中的 ./mock/start_smgr.sh 脚本完成。\n\n\n# venus-worker\n\n 1. 创建并初始化本地存储，初始化远程存储\n    \n    ./dist/bin/venus-worker store sealing-init -l ./mock-tmp/store1 ./mock-tmp/store2 ./mock-tmp/store3\n    ./dist/bin/venus-worker store file-init -l ./mock-tmp/remote\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/cleanup_store.sh 脚本完成。\n\n 2. 以 mock 配置启动 venus-worker\n    \n    ./dist/bin/venus-worker daemon -c ./venus-worker/assets/venus-worker.mock.toml\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/start_worker.sh 脚本完成。\n\n\n# 生产模式\n\n\n# venus-sector-manager\n\n 1. 初始化工作目录\n    \n    ./dist/bin/venus-sector-manager daemon init\n    \n\n 2. 按需配置默认配置文件 ~/.venus-sector-manager/sector-manager.cfg\n    \n    配置项、作用、配置方法可以参考文档 04.venus-sector-manager的配置解析。\n\n 3. 启动 venus-sector-manager\n    \n    ./dist/bin/venus-sector-manager daemon run\n    \n\nWARNING\n\n建议参看Poster 分离文档分别将计算windowPost和winningPost的进程启动在各自进程专用的机器上。如果想让secotr-manager运行所有Post计算，可以这样启动sector-manager:\n\n$ ./dist/bin/venus-sector-manager daemon run --miner --poster\n\n\n\n# venus-worker\n\n 1. 规划用于封装过程中数据的本地存储，并使用\n    \n    ./dist/bin/venus-worker store sealing-init -l <dir1> <dir2> <dir3> <...>\n    \n    \n    命令创建并初始化数据目录。\n\n 2. 挂载持久化数据目录，并使用\n    \n    ./dist/bin/venus-worker store file-init -l <dir1>\n    \n    \n    命令初始化数据目录。\n\n 3. 规划用于各阶段的 CPU 核、numa 区域等配置。\n    \n    按需完成配置文件。\n    \n    配置项、作用、配置方法可以参考文档 03.venus-worker的配置解析。\n\n 4. 使用\n    \n    /path/to/venus-worker daemon -c /path/to/venus-worker.toml\n    \n    \n    启动 venus-worker。",normalizedContent:"# 快速启用\n\n\n# 准备工作\n\n 1. 安装必要的第三方库。\n    \n    这一部分可以参考 lotus 文档中的相应部分 building-from-source。\n\n 2. 下载代码库\n    \n    git clone https://github.com/ipfs-force-community/venus-cluster.git\n    \n\n 3. 编译 venus-cluster 的组件\n    \n    cd venus-cluster\n    make all\n    \n    \n    完成后，在 ./dist/bin 目录下会有 venus-worker 和 venus-sector-manager 两个可执行文件。\n\n 4. 分发可执行文件到需要的机器上。\n\n 5. 将 ./venus-worker/create-cgroup.sh 分发到 venus-worker 所在的机器上，并以准备运行 venus-worker 的系统用户身份执行。\n    \n    这会为这样的用户生成相应的 cgroup 组，以便venus-worker 为其外部执行器进程分配硬件资源。\n\n\n# mock 模式\n\n默认情况下，可以通过一系列命令在单机上启动一组 mock 实例。\n\n\n# venus-sector-manager\n\n通过\n\n./dist/bin/venus-sector-manager mock --miner=10000 --sector-size=2kib\n\n\n命令启动一个模拟为 actor 为 t010000 的 sp 分配 2kib 扇区的 venus-sector-manager 服务。\n\n这一步骤也可以通过代码目录中的 ./mock/start_smgr.sh 脚本完成。\n\n\n# venus-worker\n\n 1. 创建并初始化本地存储，初始化远程存储\n    \n    ./dist/bin/venus-worker store sealing-init -l ./mock-tmp/store1 ./mock-tmp/store2 ./mock-tmp/store3\n    ./dist/bin/venus-worker store file-init -l ./mock-tmp/remote\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/cleanup_store.sh 脚本完成。\n\n 2. 以 mock 配置启动 venus-worker\n    \n    ./dist/bin/venus-worker daemon -c ./venus-worker/assets/venus-worker.mock.toml\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/start_worker.sh 脚本完成。\n\n\n# 生产模式\n\n\n# venus-sector-manager\n\n 1. 初始化工作目录\n    \n    ./dist/bin/venus-sector-manager daemon init\n    \n\n 2. 按需配置默认配置文件 ~/.venus-sector-manager/sector-manager.cfg\n    \n    配置项、作用、配置方法可以参考文档 04.venus-sector-manager的配置解析。\n\n 3. 启动 venus-sector-manager\n    \n    ./dist/bin/venus-sector-manager daemon run\n    \n\nwarning\n\n建议参看poster 分离文档分别将计算windowpost和winningpost的进程启动在各自进程专用的机器上。如果想让secotr-manager运行所有post计算，可以这样启动sector-manager:\n\n$ ./dist/bin/venus-sector-manager daemon run --miner --poster\n\n\n\n# venus-worker\n\n 1. 规划用于封装过程中数据的本地存储，并使用\n    \n    ./dist/bin/venus-worker store sealing-init -l <dir1> <dir2> <dir3> <...>\n    \n    \n    命令创建并初始化数据目录。\n\n 2. 挂载持久化数据目录，并使用\n    \n    ./dist/bin/venus-worker store file-init -l <dir1>\n    \n    \n    命令初始化数据目录。\n\n 3. 规划用于各阶段的 cpu 核、numa 区域等配置。\n    \n    按需完成配置文件。\n    \n    配置项、作用、配置方法可以参考文档 03.venus-worker的配置解析。\n\n 4. 使用\n    \n    /path/to/venus-worker daemon -c /path/to/venus-worker.toml\n    \n    \n    启动 venus-worker。",charsets:{cjk:!0}},{title:"快速启用",frontmatter:{sidebarDepth:2},regularPath:"/zh/operation/",relativePath:"zh/operation/README.md",key:"v-d0b99468",path:"/zh/operation/",headers:[{level:2,title:"准备工作",slug:"准备工作",normalizedTitle:"准备工作",charIndex:11},{level:2,title:"生产模式",slug:"生产模式",normalizedTitle:"生产模式",charIndex:507},{level:3,title:"damocles-manager",slug:"damocles-manager",normalizedTitle:"damocles-manager",charIndex:286},{level:3,title:"damocles-worker",slug:"damocles-worker",normalizedTitle:"damocles-worker",charIndex:268},{level:2,title:"Mock 模式(开发人员使用)",slug:"mock-模式-开发人员使用",normalizedTitle:"mock 模式(开发人员使用)",charIndex:2357},{level:3,title:"damocles-manager",slug:"damocles-manager-2",normalizedTitle:"damocles-manager",charIndex:286},{level:3,title:"damocles-worker",slug:"damocles-worker-2",normalizedTitle:"damocles-worker",charIndex:268}],lastUpdated:"2023/10/23 07:59:15",headersStr:"准备工作 生产模式 damocles-manager damocles-worker Mock 模式(开发人员使用) damocles-manager damocles-worker",content:'# 快速启用\n\n\n# 准备工作\n\n 1. 安装必要的第三方库。\n    \n    这一部分可以参考 lotus 文档中的相应部分 Software dependencies。\n\n 2. 下载代码库\n    \n    git clone https://github.com/ipfs-force-community/damocles.git\n    \n\n 3. 编译 damocles 的组件\n    \n    cd damocles\n    make all\n    \n    \n    完成后，在 ./dist/bin 目录下会有 damocles-worker 和 damocles-manager 两个可执行文件。\n\n 4. 分发可执行文件到需要的机器上。\n\n 5. 将 ./damocles-worker/create-cgroup.sh 分发到 damocles-worker 所在的机器上，并以准备运行 damocles-worker 的系统用户身份执行。\n    \n    这会为这样的用户生成相应的 cgroup 组，以便damocles-worker 为其外部执行器进程分配硬件资源。\n\n\n# 生产模式\n\n\n# damocles-manager\n\n 1. 初始化工作目录\n    \n    ./dist/bin/damocles-manager daemon init\n    \n\n 2. 按需配置默认配置文件 ~/.damocles-manager/sector-manager.cfg\n    \n    配置项、作用、配置方法可以参考文档 04.damocles-manager的配置解析。\n\n 3. 创建矿工号（可选；如果已有，可略过此步骤）\n    \n    $ ./damocles-manager util miner create \\\n    --from=<OWNER_ADDRESS> \\\n    --owner=<OWNER_ADDRESS> \\\n    --worker=<WORKER_ADDRESS> \\\n    --sector-size=32GiB\n    \n    \n    会得到如下返回值。\n    \n    2023-07-26T18:05:30.568+0800  INFO  cmd   internal/global.go:245  msg state: FillMsg   {"size": "32GiB", "from": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "actor": "f018528", "owner": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "worker": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "mid": "bafy2bzaceb2amcob2z6hwggtgu6de4mjebvaviwr46ew2lh5lkcfmuyvqyvno"}\n    2023-07-26T18:06:30.576+0800  INFO  cmd   internal/util_miner.go:274 miner actor: f0xxx9 (f2drcv6746m5ehwxxxxxy)  {"size": "32GiB", "from": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "actor": "f0xxx8", "owner": "f1slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "worker": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa"}\n    \n    \n    结果中的miner actor f0xxx9就是创建的miner id。\n    \n    > 注意⚠️：--from 地址要保证有足够的余额，保证上链成功。\n\n 4. 启动 damocles-manager\n    \n    ./dist/bin/damocles-manager daemon run\n    \n\n\n# damocles-worker\n\n 1. 规划用于封装过程中数据的本地存储，并使用\n    \n    ./dist/bin/damocles-worker store sealing-init -l <dir1> <dir2> <dir3> <...>\n    \n    \n    命令创建并初始化数据目录。\n\n 2. 挂载持久化数据目录，并使用\n    \n    ./dist/bin/damocles-worker store file-init -l <dir1>\n    \n    \n    命令初始化数据目录。\n\n 3. (可选) 创建 NUMA 亲和的 hugepage 内存文件\n\n 4. 规划用于各阶段的CPU核、numa 区域等配置。\n    \n    按需完成配置文件。\n    \n    配置项、作用、配置方法可以参考文档 03.damocles-worker的配置解析。\n\n 5. 启动 damocles-worker\n    \n    /path/to/damocles-worker daemon -c /path/to/damocles-worker.toml\n    \n\n\n# Mock 模式(开发人员使用)\n\n默认情况下，可以通过一系列命令在单机上启动一组 mock 实例。\n\n\n# damocles-manager\n\n通过\n\n./dist/bin/damocles-manager mock --miner=10000 --sector-size=2KiB\n\n\n命令启动一个模拟为 Actor 为 t010000 的 SP 分配 2KiB 扇区的 damocles-manager 服务。\n\n这一步骤也可以通过代码目录中的 ./mock/start_manager.sh 脚本完成。\n\n\n# damocles-worker\n\n 1. 创建并初始化本地存储，初始化远程存储\n    \n    ./dist/bin/damocles-worker store sealing-init -l ./mock-tmp/store1 ./mock-tmp/store2 ./mock-tmp/store3\n    ./dist/bin/damocles-worker store file-init -l ./mock-tmp/remote\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/cleanup_store.sh 脚本完成。\n\n 2. 以 mock 配置启动 damocles-worker\n    \n    ./dist/bin/damocles-worker daemon -c ./damocles-worker/assets/damocles-worker.mock.toml\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/start_worker.sh 脚本完成。',normalizedContent:'# 快速启用\n\n\n# 准备工作\n\n 1. 安装必要的第三方库。\n    \n    这一部分可以参考 lotus 文档中的相应部分 software dependencies。\n\n 2. 下载代码库\n    \n    git clone https://github.com/ipfs-force-community/damocles.git\n    \n\n 3. 编译 damocles 的组件\n    \n    cd damocles\n    make all\n    \n    \n    完成后，在 ./dist/bin 目录下会有 damocles-worker 和 damocles-manager 两个可执行文件。\n\n 4. 分发可执行文件到需要的机器上。\n\n 5. 将 ./damocles-worker/create-cgroup.sh 分发到 damocles-worker 所在的机器上，并以准备运行 damocles-worker 的系统用户身份执行。\n    \n    这会为这样的用户生成相应的 cgroup 组，以便damocles-worker 为其外部执行器进程分配硬件资源。\n\n\n# 生产模式\n\n\n# damocles-manager\n\n 1. 初始化工作目录\n    \n    ./dist/bin/damocles-manager daemon init\n    \n\n 2. 按需配置默认配置文件 ~/.damocles-manager/sector-manager.cfg\n    \n    配置项、作用、配置方法可以参考文档 04.damocles-manager的配置解析。\n\n 3. 创建矿工号（可选；如果已有，可略过此步骤）\n    \n    $ ./damocles-manager util miner create \\\n    --from=<owner_address> \\\n    --owner=<owner_address> \\\n    --worker=<worker_address> \\\n    --sector-size=32gib\n    \n    \n    会得到如下返回值。\n    \n    2023-07-26t18:05:30.568+0800  info  cmd   internal/global.go:245  msg state: fillmsg   {"size": "32gib", "from": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "actor": "f018528", "owner": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "worker": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "mid": "bafy2bzaceb2amcob2z6hwggtgu6de4mjebvaviwr46ew2lh5lkcfmuyvqyvno"}\n    2023-07-26t18:06:30.576+0800  info  cmd   internal/util_miner.go:274 miner actor: f0xxx9 (f2drcv6746m5ehwxxxxxy)  {"size": "32gib", "from": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "actor": "f0xxx8", "owner": "f1slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa", "worker": "f3slzp2qdxtw44l6decoutkzyc5l4hxxxxxxxxxxxxxxxxxxxxxa"}\n    \n    \n    结果中的miner actor f0xxx9就是创建的miner id。\n    \n    > 注意⚠️：--from 地址要保证有足够的余额，保证上链成功。\n\n 4. 启动 damocles-manager\n    \n    ./dist/bin/damocles-manager daemon run\n    \n\n\n# damocles-worker\n\n 1. 规划用于封装过程中数据的本地存储，并使用\n    \n    ./dist/bin/damocles-worker store sealing-init -l <dir1> <dir2> <dir3> <...>\n    \n    \n    命令创建并初始化数据目录。\n\n 2. 挂载持久化数据目录，并使用\n    \n    ./dist/bin/damocles-worker store file-init -l <dir1>\n    \n    \n    命令初始化数据目录。\n\n 3. (可选) 创建 numa 亲和的 hugepage 内存文件\n\n 4. 规划用于各阶段的cpu核、numa 区域等配置。\n    \n    按需完成配置文件。\n    \n    配置项、作用、配置方法可以参考文档 03.damocles-worker的配置解析。\n\n 5. 启动 damocles-worker\n    \n    /path/to/damocles-worker daemon -c /path/to/damocles-worker.toml\n    \n\n\n# mock 模式(开发人员使用)\n\n默认情况下，可以通过一系列命令在单机上启动一组 mock 实例。\n\n\n# damocles-manager\n\n通过\n\n./dist/bin/damocles-manager mock --miner=10000 --sector-size=2kib\n\n\n命令启动一个模拟为 actor 为 t010000 的 sp 分配 2kib 扇区的 damocles-manager 服务。\n\n这一步骤也可以通过代码目录中的 ./mock/start_manager.sh 脚本完成。\n\n\n# damocles-worker\n\n 1. 创建并初始化本地存储，初始化远程存储\n    \n    ./dist/bin/damocles-worker store sealing-init -l ./mock-tmp/store1 ./mock-tmp/store2 ./mock-tmp/store3\n    ./dist/bin/damocles-worker store file-init -l ./mock-tmp/remote\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/cleanup_store.sh 脚本完成。\n\n 2. 以 mock 配置启动 damocles-worker\n    \n    ./dist/bin/damocles-worker daemon -c ./damocles-worker/assets/damocles-worker.mock.toml\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/start_worker.sh 脚本完成。',charsets:{cjk:!0}},{title:"自定义算法和存储方案",frontmatter:{},regularPath:"/zh/operation/custom-algo.html",relativePath:"zh/operation/custom-algo.md",key:"v-7bbae69f",path:"/zh/operation/custom-algo.html",headers:[{level:2,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:17},{level:2,title:"使用",slug:"使用",normalizedTitle:"使用",charIndex:52},{level:3,title:"SP",slug:"sp",normalizedTitle:"sp",charIndex:235},{level:3,title:"damocles-worker 上的 ext-processors",slug:"damocles-worker-上的-ext-processors",normalizedTitle:"damocles-worker 上的 ext-processors",charIndex:321},{level:3,title:"damocles-manager 上的 ext-provers",slug:"damocles-manager-上的-ext-provers",normalizedTitle:"damocles-manager 上的 ext-provers",charIndex:632},{level:3,title:"自定义存储方案",slug:"自定义存储方案",normalizedTitle:"自定义存储方案",charIndex:834},{level:3,title:"开发者",slug:"开发者",normalizedTitle:"开发者",charIndex:239}],headersStr:"概述 使用 SP damocles-worker 上的 ext-processors damocles-manager 上的 ext-provers 自定义存储方案 开发者",content:"# 自定义算法和存储方案\n\n\n# 概述\n\ndamocles 希望在提供一套健壮的算力生产方案的同时，允许使用者最大限度地根据实际情况调配和定制自己的使用方式，其中就包括自定义算法和存储方案。\n\n例如，使用者可以选择：\n\n * 使用开源的优化算法\n * 购买付费授权的闭源算法\n * 购买外包计算服务\n * 使用对象存储（如 S3）作为自己的持久化存储方案\n\n等，并将这些定制方案以极小的成本集成到 damocles 中。\n\n\n# 使用\n\n对于自定义算法和存储方案的使用，SP 和开发者需要关注不同的内容。这里我们会分开阐述。\n\n\n# SP\n\n对于 SP 来说，只需要关注如何将自定义的内容集成到算力生成过程中。这里主要分成多个部分：\n\n\n# damocles-worker 上的 ext-processors\n\n关于 damocles-worker 上的 ext-processors，可以通过以下步骤集成：\n\n 1. 准备好符合交互协议的可执行文件\n 2. 在 damocles-worker 配置文件中的 [[processors.{stage_name}]] 块中正确地配上要启用的阶段、可执行文件位置、参数、环境变量等\n 3. 启动 damocles-worker 并检查自定义外部处理器的工作情况\n\n这部分内容可以参考：\n\n * 07.damocles-worker 外部执行器的配置范例\n * 03.damocles-worker 的配置解析\n\n\n# damocles-manager 上的 ext-provers\n\ndamocles-manager 上涉及算法定制的只有 wining post 和 window post 两部分，这两部分可以以 ext-prover 的形式进行定制，起作用机制、使用方法和 ext-processors 都很相似。\n\n这部分内容可以参考：\n\n09.独立运行的 poster 节点#ext-prover-执行器\n\n\n# 自定义存储方案\n\n自定义存储方案可以是完全的非文件系统存储方案，如对象存储等，也可以是基于传统的大规模文件系统方案的简化，如将 NFS 挂载简化为一种自定义的轻量数据访问方式。\n\n相比 ext-processors、ext-provers 来说，支持自定义存储相对来说要复杂一些，这种复杂度主要体现在：\n\n 1. 对自定义存储的支持涉及到多个场景，如扇区封装过程中的数据持久化阶段、扇区升级过程中的原始数据访问阶段、winning post、window post 等。\n 2. damocles-manager 和 damocles-worker 都涉及到和存储实例的一些交互行为。\n\n简单来说，为了能够使用某种自定义存储方案，需要至少进行：\n\n 1. 为 damocles-worker 配置支持这种存储方案的 transfer 阶段 ext-processor\n 2. 为 damocles-manager 配置支持这种存储方案的 winning post 和 window post ext-prover\n 3. 为 damocles-manager 配置支持这种存储方案的 objstore 插件，参考 04.damocles-manager 的配置解析\n\n在具备上述条件后，damocles 即可基于定制化的存储方案工作。\n\n\n# 开发者\n\n对于开发者来说，最重要的事情是按照 damocles 已经定义好的一系列接口和协议，开发出适用的自定义组件，如外部处理器、golang 插件等。\n\n# ext-processors / ext-provers\n\n从开发层面来说，ext-processors 和 ext-provers 是同一类实现，使用在了不同的场景中。\n\n它的基本原理，是一个可执行文件，满足：\n\n * 父进程通过 stdin / stdout 进行交互\n * 符合 vc-processors 定义的交互协议和数据格式\n\n开发者可以通过以下步骤了解基本的开发过程：\n\n 1. 了解原理和机制，参考 vc-processors: examples\n 2. 为需要定制的阶段实现相应的算法，参考 vc-processors: builtin/processors\n 3. 将已经实现的算法封装成可执行程序，参考 vc-worker: subcommand/processors\n\n# damocles-manager objstore plugin\n\n它本质是 golang 的 plugin，满足：\n\n * 满足 golang plugin 的要求和限制\n * 符合 damocles-manager objstore 定义的接口和语义\n\n开发者可以通过以下步骤了解基本的开发过程：\n\n 1. 了解 damocles-manager 插件框架\n 2. 了解接口定义和运作方式，参考：damocles-manager objstore: Store 和 objstore: spi\n 3. 了解范例实现 damocles-manager: plugin/fsstore",normalizedContent:"# 自定义算法和存储方案\n\n\n# 概述\n\ndamocles 希望在提供一套健壮的算力生产方案的同时，允许使用者最大限度地根据实际情况调配和定制自己的使用方式，其中就包括自定义算法和存储方案。\n\n例如，使用者可以选择：\n\n * 使用开源的优化算法\n * 购买付费授权的闭源算法\n * 购买外包计算服务\n * 使用对象存储（如 s3）作为自己的持久化存储方案\n\n等，并将这些定制方案以极小的成本集成到 damocles 中。\n\n\n# 使用\n\n对于自定义算法和存储方案的使用，sp 和开发者需要关注不同的内容。这里我们会分开阐述。\n\n\n# sp\n\n对于 sp 来说，只需要关注如何将自定义的内容集成到算力生成过程中。这里主要分成多个部分：\n\n\n# damocles-worker 上的 ext-processors\n\n关于 damocles-worker 上的 ext-processors，可以通过以下步骤集成：\n\n 1. 准备好符合交互协议的可执行文件\n 2. 在 damocles-worker 配置文件中的 [[processors.{stage_name}]] 块中正确地配上要启用的阶段、可执行文件位置、参数、环境变量等\n 3. 启动 damocles-worker 并检查自定义外部处理器的工作情况\n\n这部分内容可以参考：\n\n * 07.damocles-worker 外部执行器的配置范例\n * 03.damocles-worker 的配置解析\n\n\n# damocles-manager 上的 ext-provers\n\ndamocles-manager 上涉及算法定制的只有 wining post 和 window post 两部分，这两部分可以以 ext-prover 的形式进行定制，起作用机制、使用方法和 ext-processors 都很相似。\n\n这部分内容可以参考：\n\n09.独立运行的 poster 节点#ext-prover-执行器\n\n\n# 自定义存储方案\n\n自定义存储方案可以是完全的非文件系统存储方案，如对象存储等，也可以是基于传统的大规模文件系统方案的简化，如将 nfs 挂载简化为一种自定义的轻量数据访问方式。\n\n相比 ext-processors、ext-provers 来说，支持自定义存储相对来说要复杂一些，这种复杂度主要体现在：\n\n 1. 对自定义存储的支持涉及到多个场景，如扇区封装过程中的数据持久化阶段、扇区升级过程中的原始数据访问阶段、winning post、window post 等。\n 2. damocles-manager 和 damocles-worker 都涉及到和存储实例的一些交互行为。\n\n简单来说，为了能够使用某种自定义存储方案，需要至少进行：\n\n 1. 为 damocles-worker 配置支持这种存储方案的 transfer 阶段 ext-processor\n 2. 为 damocles-manager 配置支持这种存储方案的 winning post 和 window post ext-prover\n 3. 为 damocles-manager 配置支持这种存储方案的 objstore 插件，参考 04.damocles-manager 的配置解析\n\n在具备上述条件后，damocles 即可基于定制化的存储方案工作。\n\n\n# 开发者\n\n对于开发者来说，最重要的事情是按照 damocles 已经定义好的一系列接口和协议，开发出适用的自定义组件，如外部处理器、golang 插件等。\n\n# ext-processors / ext-provers\n\n从开发层面来说，ext-processors 和 ext-provers 是同一类实现，使用在了不同的场景中。\n\n它的基本原理，是一个可执行文件，满足：\n\n * 父进程通过 stdin / stdout 进行交互\n * 符合 vc-processors 定义的交互协议和数据格式\n\n开发者可以通过以下步骤了解基本的开发过程：\n\n 1. 了解原理和机制，参考 vc-processors: examples\n 2. 为需要定制的阶段实现相应的算法，参考 vc-processors: builtin/processors\n 3. 将已经实现的算法封装成可执行程序，参考 vc-worker: subcommand/processors\n\n# damocles-manager objstore plugin\n\n它本质是 golang 的 plugin，满足：\n\n * 满足 golang plugin 的要求和限制\n * 符合 damocles-manager objstore 定义的接口和语义\n\n开发者可以通过以下步骤了解基本的开发过程：\n\n 1. 了解 damocles-manager 插件框架\n 2. 了解接口定义和运作方式，参考：damocles-manager objstore: store 和 objstore: spi\n 3. 了解范例实现 damocles-manager: plugin/fsstore",charsets:{cjk:!0}},{title:"damocles-manager 的配置解析",frontmatter:{},regularPath:"/zh/operation/damocles-manager-config.html",relativePath:"zh/operation/damocles-manager-config.md",key:"v-20f290cb",path:"/zh/operation/damocles-manager-config.html",headers:[{level:2,title:"[Common]",slug:"common",normalizedTitle:"[common]",charIndex:125},{level:3,title:"[Common.API]",slug:"common-api",normalizedTitle:"[common.api]",charIndex:134},{level:3,title:"[Common.Plugins]",slug:"common-plugins",normalizedTitle:"[common.plugins]",charIndex:4573},{level:3,title:"[[Common.PieceStores]]",slug:"common-piecestores",normalizedTitle:"[[common.piecestores]]",charIndex:378},{level:3,title:"[Common.PieceStorePreset]",slug:"common-piecestorepreset",normalizedTitle:"[common.piecestorepreset]",charIndex:553},{level:3,title:"[[Common.PersistStores]]",slug:"common-persiststores",normalizedTitle:"[[common.persiststores]]",charIndex:783},{level:3,title:"[Common.MongoKVStore] 已废弃",slug:"common-mongokvstore-已废弃",normalizedTitle:"[common.mongokvstore] 已废弃",charIndex:8546},{level:3,title:"[Common.Proving]",slug:"common-proving",normalizedTitle:"[common.proving]",charIndex:1095},{level:3,title:"[Common.Proving.WorkerProver]",slug:"common-proving-workerprover",normalizedTitle:"[common.proving.workerprover]",charIndex:1201},{level:3,title:"[Common.DB]",slug:"common-db",normalizedTitle:"[common.db]",charIndex:1031},{level:2,title:"[[Miners]]",slug:"miners",normalizedTitle:"[[miners]]",charIndex:1295},{level:3,title:"主配置项",slug:"主配置项",normalizedTitle:"主配置项",charIndex:10617},{level:3,title:"[Miners.Sector]",slug:"miners-sector",normalizedTitle:"[miners.sector]",charIndex:1321},{level:3,title:"[Miners.SnapUp]",slug:"miners-snapup",normalizedTitle:"[miners.snapup]",charIndex:1464},{level:3,title:"[Miners.Commitment]",slug:"miners-commitment",normalizedTitle:"[miners.commitment]",charIndex:1841},{level:3,title:"[Miners.Commitment.Pre]",slug:"miners-commitment-pre",normalizedTitle:"[miners.commitment.pre]",charIndex:1878},{level:3,title:"[Miners.Commitment.Prove]",slug:"miners-commitment-prove",normalizedTitle:"[miners.commitment.prove]",charIndex:2256},{level:3,title:"[Miners.Commitment.Terminate]",slug:"miners-commitment-terminate",normalizedTitle:"[miners.commitment.terminate]",charIndex:2638},{level:3,title:"[Miners.PoSt]",slug:"miners-post",normalizedTitle:"[miners.post]",charIndex:3027},{level:3,title:"[Miners.Proof]",slug:"miners-proof",normalizedTitle:"[miners.proof]",charIndex:3398},{level:3,title:"[Miners.Sealing]",slug:"miners-sealing",normalizedTitle:"[miners.sealing]",charIndex:3430},{level:3,title:"[Miners.Deal] 已废弃",slug:"miners-deal-已废弃",normalizedTitle:"[miners.deal] 已废弃",charIndex:15458},{level:2,title:"一份最简可工作的配置文件范例",slug:"一份最简可工作的配置文件范例",normalizedTitle:"一份最简可工作的配置文件范例",charIndex:15556}],headersStr:"[Common] [Common.API] [Common.Plugins] [[Common.PieceStores]] [Common.PieceStorePreset] [[Common.PersistStores]] [Common.MongoKVStore] 已废弃 [Common.Proving] [Common.Proving.WorkerProver] [Common.DB] [[Miners]] 主配置项 [Miners.Sector] [Miners.SnapUp] [Miners.Commitment] [Miners.Commitment.Pre] [Miners.Commitment.Prove] [Miners.Commitment.Terminate] [Miners.PoSt] [Miners.Proof] [Miners.Sealing] [Miners.Deal] 已废弃 一份最简可工作的配置文件范例",content:'# damocles-manager 的配置解析\n\ndamocles-manager 是与链交互、维持扇区的主体，我们来了解一下它的配置文件结构和配置方式。\n\n在完成初始化之后，我们可以得到一份原始的配置内容：\n\n# Default config:\n[Common]\n[Common.API]\n#Gateway = ["/ip4/{api_host}/tcp/{api_port}"]\n#Token = "{some token}"\n#ChainEventInterval = "1m0s"\n#Chain = "/ip4/{api_host}/tcp/{api_port}"\n#Messager = "/ip4/{api_host}/tcp/{api_port}"\n#Market = "/ip4/{api_host}/tcp/{api_port}"\n#\n[[Common.PieceStores]]\n#Name = "{store_name}"\n#Path = "{store_path}"\n#ReadOnly = false\n#Plugin = ""\n#PluginName = "s3store"\n[Common.PieceStores.Meta]\n#SomeKey = "SomeValue"\n#\n[Common.PieceStorePreset]\n#Strict = false\n#ReadOnly = false\n#Weight = 1\n#AllowMiners = [1, 2]\n#DenyMiners = [3, 4]\n#StorageConfigPath = "/optional/path/to/your/storage.json"\n[Common.PieceStorePreset.Meta]\n#SomeKey = "SomeValue"\n#\n[[Common.PersistStores]]\n#Name = "{store_name}"\n#Path = "{store_path}"\n#Strict = false\n#ReadOnly = false\n#Weight = 0\n#AllowMiners = [1, 2]\n#DenyMiners = [3, 4]\n#Plugin = ""\n#PluginName = "s3store"\n[Common.PersistStores.Meta]\n#SomeKey = "SomeValue"\n[Common.DB]\n#Driver = "badger"\n[Common.DB.Badger]\n#BaseDir = ""\n[Common.Proving]\n#ParallelCheckLimit = 128\n#SingleCheckTimeout = "10m0s"\n#PartitionCheckTimeout = "20m0s"\n[Common.Proving.WorkerProver]\nJobMaxTry = 2\nHeartbeatTimeout = "15s"\nJobLifetime = "25h0m0s"\n\n[[Miners]]\n#Actor = 10086\n[Miners.Sector]\n#InitNumber = 0\n#MinNumber = 10\n#MaxNumber = 1000000\n#Enabled = true\n#EnableDeals = false\n#LifetimeDays = 540\n#Verbose = false\n[Miners.SnapUp]\n#Enabled = false\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#SendFund = true\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n#CleanupCCData = true\n#MessageConfidence = 15\n#ReleaseConfidence = 30\n[Miners.SnapUp.Retry]\n#MaxAttempts = 10\n#PollInterval = "3m0s"\n#APIFailureWait = "3m0s"\n#LocalFailureWait = "3m0s"\n[Miners.Commitment]\n#Confidence = 10\n[Miners.Commitment.Pre]\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#SendFund = true\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Pre.Batch]\n#Enabled = false\n#Threshold = 16\n#MaxWait = "1h0m0s"\n#CheckInterval = "1m0s"\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Prove]\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#SendFund = true\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Prove.Batch]\n#Enabled = false\n#Threshold = 16\n#MaxWait = "1h0m0s"\n#CheckInterval = "1m0s"\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Terminate]\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#SendFund = true\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.Commitment.Terminate.Batch]\n#Enabled = false\n#Threshold = 5\n#MaxWait = "1h0m0s"\n#CheckInterval = "1m0s"\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n[Miners.PoSt]\n#Sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#Enabled = true\n#StrictCheck = true\n#Parallel = false\n#GasOverEstimation = 1.2\n#GasOverPremium = 0.0\n#GasFeeCap = "5 nanoFIL"\n#MaxFeeCap = ""\n#Confidence = 10\n#SubmitConfidence = 0\n#ChallengeConfidence = 0\n#MaxRecoverSectorLimit = 0\n#MaxPartitionsPerPoStMessage = 0\n#MaxPartitionsPerRecoveryMessage = 0\n[Miners.Proof]\n#Enabled = false\n[Miners.Sealing]\n#SealingEpochDuration = 0\n#UseSyntheticPoRep = false\n\n\n我们将逐一分析其中的可配置项。\n\n\n# [Common]\n\nCommon 是公共配置，又分成四个子配置项：\n\n\n# [Common.API]\n\nCommon.API 是接口相关的配置，其内容包含：\n\n[Common.API]\n# 网关服务地址，必填项，字符串类型\n# 根据所使用的服务实际情况填写\n# 对于每一条信息，如果使用了合法的 "{token}:{multiaddr}" 格式，构造客户端时将使用本字段中提取的 token，否则使用通用 sophon-auth 产生的 token\nGateway = ["/ip4/{api_host}/tcp/{api_port}"]\n\n# 链服务信息，可选项，字符串类型\n# 根据所使用的服务实际情况填写\n# 如果使用了合法的 "{token}:{multiaddr}" 格式，构造客户端时将使用本字段中提取的 token，否则使用通用 sophon-auth 产生的 token\n# 如果不填写，会默认使用 Gateway 的地址和 token 作为链服务的入口 \nChain = "/ip4/{api_host}/tcp/{api_port}"\n\n# 消息服务信息，可选项，字符串类型\n# 根据所使用的服务实际情况填写\n# 如果使用了合法的 "{token}:{multiaddr}" 格式，构造客户端时将使用本字段中提取的 token，否则使用通用 sophon-auth 产生的 token\n# 如果不填写，会默认使用 Gateway 的地址和 token 作为消息服务的入口 \nMessager = "/ip4/{api_host}/tcp/{api_port}"\n\n# 市场服务信息，可选项，字符串类型\n# 根据所使用的服务实际情况填写\n# 如果使用了合法的 "{token}:{multiaddr}" 格式，构造客户端时将使用本字段中提取的 token，否则使用通用 sophon-auth 产生的 token\n# 如果不填写，会默认使用 Gateway 的地址和 token 作为消息服务的入口 \nMarket = "/ip4/{api_host}/tcp/{api_port}"\n\n# 服务 sophon-auth 产生的 token，必填项，字符串类型\n# 根据所使用的服务实际情况填写\nToken = "{some token}"\n\n# 侦测链高度变化的间隔时间，选填项，时间类型\n# 默认值为 1min\n#ChainEventInterval = "1m0s"\n\n\n\n# [Common.Plugins]\n\nCommon.Plugins 配置 damocles-manager 的插件存放路径\n\n[Common.Plugins]\n# 插件存放路径，选填项，字符串类型\n# 默认为空字符串，表示不加载任何插件\n# 建议使用绝对路径\nDir = ""\n\n\n\n# [[Common.PieceStores]]\n\nCommon.PieceStores是用于配置本地订单 piece 数据的选项。当存在可用的离线存储时，可以配置此项，避免通过公网获取订单的piece 数据。\n\n每一个本地存储目录对应一个 Common.PieceStores 配置块。\n\n# 基础配置范例\n\n[[Common.PieceStores]]\n# 名称，选填项，字符串类型\n# 默认为路径对应的绝对路径\n#Name = "remote-store1"\n\n# 路径，必填项，字符串类型\nPath = "/mnt/mass/piece1"\n\n# 插件路径，选填项，字符串类型\n# 默认为 null\n# 如果希望使用自定义存储方案，可以通过编写符合要求的 golang plugin，并在此设置。\n# 已过时，请使用 PluginName 代替\n#Plugin = "path/to/objstore-plugin"\n\n# 插件名称，选填项，字符串类型\n# 默认为空字符串\n# 如果希望使用自定义存储方案，可以通过编写符合要求的 golang plugin，并在此设置。\n#PluginName = "s3store"\n\n# 元信息，选填项，字典类型\n# 内部值为 Key = "Value" 的格式\n# 默认值为 null\n# 用于支持不同类型存储方案\n[Common.PieceStores.Meta]\n#SomeKey = "SomeValue"\n#\n\n\n\n# [Common.PieceStorePreset]\n\nPersistStore 指的是扇区持久化数据存储。与之对应的是 damocles-worker 中的 attached 概念。\n\n而 PieceStorePreset 为全局的所有 PersistStore 配置，提供了一套可自定义的预设值，与后一章节中的 [[Common.PersistStores]] 配置项相呼应。合理利用 PieceStorePreset 可以大大简化 [[Common.PersistStores]] 的配置。\n\n同时，Damocles-Manager 还支持 lotus 风格的存储配置文件（例如 lotus-miner repo 下默认生成的 storage.json 文件）。Damocles-Manager 会根据 PieceStorePreset 为该配置文件下的每一个路径在内存中生成一个对应的 PersistStore 配置项。\n\n# 基础配置范例\n\n[Common.PieceStorePreset]\n\n# 只读，选填项，布尔类型\n# 默认值为 false\n# 自 v0.4.0 起，持久化存储分配逻辑转到 damocles-manager 上\n# 可通过此配置设置存储是否可以继续写入\nReadOnly = false\n\n# 可选项，布尔类型\n# 默认 false\n# 是否验证`Path`路径是否为通常文件，true 时，`Path`为软连接等非通常文件时会报错\nStrict = false\n\n# 权重，选填项，数字类型\n# 默认值为 1\n# 当填写值为 0 时，等效于 1\n# 自 v0.4.0 起，持久化存储分配逻辑转到 damocles-manager 上\n# 可通过此配置设置多个持久化存储之间的权重配比\n# 每个持久化存储被选中的概率为 `weight / sum`, `sum` 是所有可用的持久化存储权重的和\n# 例：配置 3 个 持久化存储，weight 分别为 2, 1, 1。则被选中的概率分别为 50%, 25%, 25%\nWeight = 1\n\n\n# 允许进行分配的矿工号列表，选填项，数字数组类型\n# 默认为 null\n# 当不设置时，视为允许全部矿工号；当设置时，则相当于白名单，仅允许分配给列出的矿工号\n# 如果一个矿工号同时出现在 AllowMiners 和 DenyMiners 中时，DenyMiners 优先生效，即视为拒绝\nAllowMiners = [1, 2]\n\n# 拒绝进行分配的矿工号列表，选填项，数字数组类型\n# 默认为 null\n# 当不设置时，视为不拒绝任何矿工号；当设置时，则相当于黑名单，将拒绝为列出的矿工号分配\n# 如果一个矿工号同时出现在 AllowMiners 和 DenyMiners 中时，DenyMiners 优先生效，即视为拒绝\nDenyMiners = [3, 4]\n\n# lotus 风格的存储配置文件的路径，选填项，字符串类型\n# 默认为空字符串\n# 如果只是希望简单地沿用 lotus 的存储路径配置，可以选择填写此项\n# 但是如果希望更加深入地对每个存储路径进行细致的设置，建议使用 `[[Common.PersistStores]]` 配置项\nStorageConfigPath = "/optional/path/to/your/storage.json"\n\n# 元信息，选填项，字典类型\n# 内部值为 Key = "Value" 的格式\n# 默认值为 null\n# 用于支持不同类型存储方案的预备，目前没有任何作用\n[Common.PersistStores.Meta]\n#SomeKey = "SomeValue"\n#\n\n\n * storage.json\n\n{\n  "StoragePaths": [\n    {\n      "Name": "persist",\n      "Path": "/root/persist"\n    }\n  ]\n}\n\n\n其中 Name 属性可以省略，省略时默认使用 Path 属性的值作为 Name。\n\n\n# [[Common.PersistStores]]\n\nCommon.PersistStores 用于配置扇区持久化数据存储。与之对应的是 damocles-worker 中的 attached 概念。\n\n与 Common.PieceStores 类似，每一个持久化存储目录对应一个 Common.PersistStores 配置块。\n\n# 基础配置范例\n\n[[Common.PersistStores]]\n# 名称，选填项，字符串类型\n## 默认为路径对应的绝对路径\n#Name = "remote-store1"\n\n# 路径，必填项，字符串类型\n# 建议使用绝对路径\nPath = "/mnt/remote/10.0.0.14/store"\n\n# 只读，选填项，布尔类型\n# 默认值为 false\n# 自 v0.4.0 起，持久化存储分配逻辑转到 damocles-manager 上\n# 可通过此配置设置存储是否可以继续写入\n#ReadOnly = false\n\n# 可选项，布尔类型\n# 默认 false\n# 是否验证`Path`路径是否为通常文件，true 时，`Path`为软连接等非通常文件时会报错\n#Strict = false\n\n# 权重，选填项，数字类型\n# 默认值为 1\n# 当填写值为 0 时，等效于 1\n# 自 v0.4.0 起，持久化存储分配逻辑转到 damocles-manager 上\n# 可通过此配置设置多个持久化存储之间的权重配比\n# 每个持久化存储被选中的概率为 `weight / sum`, `sum` 是所有可用的持久化存储权重的和\n# 例：配置 3 个 持久化存储，weight 分别为 2, 1, 1。则被选中的概率分别为 50%, 25%, 25%\n#Weight = 1\n\n# 插件路径，选填项，字符串类型\n# 默认为 null\n# 如果希望使用自定义存储方案，可以通过编写符合要求的 golang plugin，并在此设置。\n# 已过时，请使用 PluginName 代替\n#Plugin = "path/to/objstore-plugin"\n\n# 允许进行分配的矿工号列表，选填项，数字数组类型\n# 默认为 null\n# 当不设置时，视为允许全部矿工号；当设置时，则相当于白名单，仅允许分配给列出的矿工号\n# 如果一个矿工号同时出现在 AllowMiners 和 DenyMiners 中时，DenyMiners 优先生效，即视为拒绝\n#AllowMiners = [1, 2]\n\n# 拒绝进行分配的矿工号列表，选填项，数字数组类型\n# 默认为 null\n# 当不设置时，视为不拒绝任何矿工号；当设置时，则相当于黑名单，将拒绝为列出的矿工号分配\n# 如果一个矿工号同时出现在 AllowMiners 和 DenyMiners 中时，DenyMiners 优先生效，即视为拒绝\n#DenyMiners = [3, 4]\n\n# 插件路径，选填项，字符串类型\n# 默认为空字符串\n# 如果希望使用自定义存储方案，可以通过编写符合要求的 golang plugin，并在此设置。\n#PluginName = "s3store"\n\n# 元信息，选填项，字典类型\n# 内部值为 Key = "Value" 的格式\n# 默认值为 null\n# 用于支持不同类型存储方案的预备，目前没有任何作用\n[Common.PersistStores.Meta]\n#SomeKey = "SomeValue"\n#\n\n\n\n# [Common.MongoKVStore] 已废弃\n\nCommon.MongoKVStore 用于配置 damocles-manager 是否启用 Mongo 作为 sealing 过程中使用的 KV 数据库。\n\n# 基础配置范例\n\n[Common.MongoKVStore]\n# 启用 Mongo 的开关，选填项，布尔类型\n# 默认值为 false\nEnable = true\n# 使用的 Mongo 的 dsn，在 Enable 为 true 的时候为必填项，字符串类型\nDSN = "mongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000"\n# 使用的 Mongo 的数据库名，在 Enable 为 true 的时候为必填项，字符串类型\nDatabaseName = "test"\n\n\n\n# [Common.Proving]\n\n用于配置证明任务的并发以及超时时间\n\n配置范例：\n\n# 检查扇区时的最大并行数量，可选项，数字类型\n# 默认值为 128。设置为 0 表示不限制并行数量\n# 注意：将此值设置得太高可能会使节点因堆栈不足而崩溃\n# 注意：将此值设置得太低可能会使扇区 challenge 读取速度变慢，导致 PoSt 失败\n#ParallelCheckLimit = 128\n# 单个扇区的验证预检查所需的最长时间 可选项，时间类型\n# 默认值为 10m0s\n# 如果检查超时，将跳过扇区。\n#SingleCheckTimeout = "10m0s"\n# 整个分区的验证预检查所需的最长时间，可选项，时间类型\n# 如果检查超时，分区中未按时检查的扇区将被跳过\n#PartitionCheckTimeout = "20m0s"\n\n\n\n# [Common.Proving.WorkerProver]\n\n用于配置 worker prover 模块\n\n配置范例：\n\n# WindowPoSt 任务的最大尝试次数，可选项，数字类型\n# 默认值为 2\n# 尝试次数超过 JobMaxTry 的 WindowPoSt 任务只能通过手动 reset 的方式被重新执行\nJobMaxTry = 2\n# WindowPoSt 任务的心跳超时时间，可选项，时间字符串类型\n# 默认值为 15s\n# 超过此时间没有发送心跳的任务将会被设置为失败并重试\nHeartbeatTimeout = "15s"\n# WindowPoSt 任务的心跳超时时间，可选项，时间字符串类型\n# 默认值为 25h\n# 创建时间超过此时间的 WindowPoSt 任务将会被删除\nJobLifetime = "25h0m0s"\n\n\n\n# [Common.DB]\n\nCommon.DB 用于配置 sealing 过程中使用的 KV 数据库。目前支持 badger 本地数据库和 mongo 数据库。\n\n# 基础配置范例：\n\n[Common.DB]\n# 指定数据库，可选项，字符串类型\n# 默认值为 badger\n# 可选填 badger | mongo | plugin\nDriver = "badger"\n[Common.DB.Badger]\n# Badger 数据库文件所在目录，可选项，字符串类型\n# 默认为空字符串\n# BaseDir 为空字符串时 damocles-manager 会使用 home dir(默认为 ~/.damocles-manager) 存放 Badger 数据库文件\n#BaseDir = ""\n\n[Common.DB.Mongo]\n# 使用的 Mongo 的 dsn，在 Enable 为 true 的时候为必填项，字符串类型\nDSN = "mongodb://127.0.0.1:27017/?directConnection=true&serverSelectionTimeoutMS=2000"\n# 使用的 Mongo 的数据库名，在 Enable 为 true 的时候为必填项，字符串类型\nDatabaseName = "test"\n\n# kvstore 插件配置\n[Common.DB.Plugin]\n# 插件名称，选填项，字符串类型\n# 默认为 null\nPluginName = "redis"\n\n# 元信息，选填项，字典类型\n# 内部值为 Key = "Value" 的格式\n# 默认值为 null\n# Meta 数据会传入插件的构造函数中\n[Common.DB.Plugin.Meta]\n#SomeKey = "SomeValue"\n#\n\n\n\n# [[Miners]]\n\nMiners 是较为重要的一个配置项，用于针对某一个 SP 定义其行为和策略。\n\ndamocles 被设计为同一套组件可以支持多个 SP ，在 damocles-manager 中的具体表现就是可以根据需要设置多个 Miners 配置块。\n\n\n# 主配置项\n\n[[Miners]]\n# `SP` actor id，必填项，数字类型\nActor = 10086\n\n\n除主配置向外， Miners 同样包含多个不同的子配置块，下面我们一一分析\n\n\n# [Miners.Sector]\n\n用于控制扇区分配的策略。\n\n[Miners.Sector]\n# 扇区起始编号，选填项，数字类型\n# 默认值为 0\n# 已废弃\nInitNumber = 0\n\n# 扇区最小编号，选填项，数字类型\n# 默认值为 null\n# 与 InitNumber 相比，当设置此项时，\n# 1. 任何时刻，分配器都不会给出小于等于此值的扇区编号。\n# 2. 此项的值可以在集群运行过程中调整。\n#    提高配置值，分配结果将始终遵循 1) 的描述。\n#    降低配置值通常不会产生效果。\n#\n# 未设置此项时，如果 InitNumber 为非 0 值，则等效于此项。\n#MinNumber = 10\n\n# 扇区编号上限，选填项，数字类型\n# 默认值为 null，表示无上限限制\n#MaxNumber = 1000000\n\n# 是否允许分配扇区，选填项，布尔类型\n# 默认值为 true，即开启分配\n# false, 不再进行封装任务\n#Enabled = true\n\n# 是否允许分配订单，选填项，布尔类型\n# 默认值为 false\n#EnableDeals = false\n\n# CC 扇区的生命周期，单位为 天，选填项，数字类型\n# 默认值为 540\n#LifetimeDays = 540\n\n# Sector 相关模块的日志详尽程度，选填项，布尔类型\n# 默认值为 false，即精简日志输出\n#Verbose = false\n\n\n\n# [Miners.SnapUp]\n\n用于控制 SnapDeal 的生产策略\n\n[Miners.SnapUp]\n# 是否启用，选填项，布尔类型\n# 默认值为 false\n#Enabled = false\n\n# 发送地址，在启用的情况下为必填项，地址类型\n#Sender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# 提交上链消息时是否从 Sender 发送必要的资金，选填项，布尔类型\n# 默认值为 true\n# false 时，资金从 miner 出\n#SendFund = true\n\n# 单条提交消息的 Gas 估算倍数，选填项，浮点数类型\n# 默认值为 1.2\n#GasOverEstimation = 1.2\n\n# 单条提交消息的 GasFeeCap 限制，选填项，FIL 值类型\n# 默认值为 5 nanoFIL\n#GasFeeCap = "5 nanoFIL"\n\n# 已废弃\n#MaxFeeCap = ""\n\n# 单条提交消息的 GasPremium 估算倍数，选填项，浮点数类型\n# 默认值为 0.0\n#GasOverPremium = "0.0"\n\n# snapdeal 消息上链后是否删除原 cc 扇区数据\n# 默认值为 true\n#CleanupCCData = true\n\n# 消息上链的确信高度，选填项，数字类型\n# 默认值为 15\n#MessageConfidence = 15\n\n# 释放旧数据存储空间的确信高度，选填项，数字类型\n# 默认值为 30\n#ReleaseConfidence = 30\n\n# SnapUp 提交重试策略\n[Miners.SnapUp.Retry]\n\n# 最大重试次数，选填项，数字类型\n# 默认为 NULL，表示不做限制\n#MaxAttempts = 10\n\n# 轮询状态的间隔，选填项，时间类型\n# 默认值为 3min\n#PollInterval = "3m0s"\n\n# API 接口异常的重试间隔，选填项，时间类型\n# 默认值为 3min\n#APIFailureWait = "3m0s"\n\n# 本地异常的重试间隔，如本地数据库异常、本地存储异常等，选填项，时间类型\n# 默认值为 3min\n#LocalFailureWait = "3m0s"\n\n\n\n# [Miners.Commitment]\n\n用于配置封装消息提交策略的通用部分。\n\n[Miners.Commitment]\n# 消息的稳定高度，选填项，数字类型\n# 默认值为 10\n#Confidence = 10\n\n\n\n# [Miners.Commitment.Pre]\n\n用于配置 PreCommit 消息提交的策略\n\n[Miners.Commitment.Pre]\n# 提交上链消息时是否从 Sender 发送必要的资金，选填项，布尔类型\n# 默认值为 true\n#SendFund = true\n\n# 发送地址，必填项，地址类型\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# 单条提交消息的 Gas 估算倍数，选填项，浮点数类型\n# 默认值为 1.2\n#GasOverEstimation = 1.2\n\n# 单条提交消息的 GasFeeCap 限制，选填项，FIL 值类型\n# 默认值为 5 nanoFIL\n#GasFeeCap = "5 nanoFIL"\n\n# 已废弃\n#MaxFeeCap = ""\n\n# 单条提交消息的 GasPremium 估算倍数，选填项，浮点数类型\n# 默认值为 0.0\n#GasOverPremium = "0.0"\n\n# 聚合提交的策略配置块\n[Miners.Commitment.Pre.Batch]\n# 是否启用聚合提交，选填项，布尔类型\n# 默认值为 false，即不启用\n#Enabled = false\n\n# 最小聚合条数，选填项，数字类型\n# 默认值为 16，即最小聚合条数为 16 条\n#Threshold = 16\n\n# 最大等待时间，选填项，时间类型\n# 默认值为 1h，即最大等待 1 小时\n#MaxWait = "1h0m0s"\n\n# 检查间隔，选填项，时间类型\n# 默认值为 1min，即每隔 1min 检查一次是否满足聚合条件\n#CheckInterval = "1m0s"\n\n# 聚合提交消息的 Gas 估算倍数，选填项，浮点数类型\n# 默认值为 1.2\n#GasOverEstimation = 1.2\n\n# 聚合提交消息的 GasFeeCap 限制，选填项，FIL 值类型\n# 默认值为 5 nanoFIL\n#GasFeeCap = "5 nanoFIL"\n\n# 已废弃\n#MaxFeeCap = ""\n\n# 聚合提交消息的 GasPremium 估算倍数，选填项，浮点数类型\n# 默认值为 0.0\n#GasOverPremium = "0.0"\n\n\n\n# [Miners.Commitment.Prove]\n\n用于配置 ProveCommit 消息提交的策略，其配置项和作用与 Miners.Commitment.Pre内的完全一致。\n\n\n# [Miners.Commitment.Terminate]\n\n用于配置 TerminateSectors 消息提交的策略，其配置项和作用与 Miners.Commitment.Pre 内的基本一致。实际场景中发送此类消息不会很频繁，建议配置单条提交模式，使用聚合提交模式时，Threshold 建议配置较小的值，保证消息及时上链。\n\n\n# [Miners.PoSt]\n\n用于配置 WindowPoSt 的相关策略。\n\n[Miners.PoSt]\n# 发送地址，必填项，地址类型\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# 是否启用，选填项，布尔类型\n# 默认值为 true\n#Enabled = true\n\n# 是否对扇区文件进行强校验，选填项，布尔类型\n# 默认值为 true\n# 开启时，除了对文件存在性进行判断外，还会尝试读取部分信息，如元数据等\n#StrictCheck = true\n\n# 是否启用并行证明，选填项，布尔类型\n# 默认值为 false\n# 开启时，同一个 deadline 内的多个 partition 将会并行展开证明\n# 注意：当设置了外部证明器 (ext-prover)，且有多个外部证明器可用时，设置此项才有正面影响\n#Parallel = false\n\n# WindowPoSt 消息的 Gas 估算倍数，选填项，浮点数类型\n# 默认值为 1.2\n#GasOverEstimation = 1.2\n\n# WindowPoSt 消息的 GasFeeCap 限制，选填项，FIL 值类型\n# 默认值为 5 nanoFIL\n#GasFeeCap = "5 nanoFIL"\n\n# 已废弃\n#MaxFeeCap = ""\n\n# WindowPoSt 消息的 GasPremium 估算倍数，选填项，浮点数类型\n# 默认值为 0.0\n#GasOverPremium = "0.0"\n\n# 消息的稳定高度，选填项，数字类型\n# 默认值为 10\n#Confidence = 10\n\n# 提交 WindowPoSt 证明结果的稳定高度，选填项，数字类型\n# 这个值决定了需要等待多少个高度才认定链进入稳定状态，可以提交 WindowPoSt 证明结果\n# 提交高度为 deadline.Open + SubmitConfidence\n# 此值设定越小，会越早启动，但同时也越容易受到分叉影响\n# 当设置为 0 时，会使用默认值 4\n#SubmitConfidence = 0\n\n# 启动 WindowPoSt 的稳定高度，选填项，数字类型\n# 这个值决定了需要等待多少个高度才认定链进入稳定状态，可以启动 WindowPoSt 任务\n# 启动高度为 deadline.Challenge + ChallengeConfidence\n# 此值设定越小，会越早启动，但同时也越容易受到分叉影响\n# 当设置为 0 时，会使用默认值 10\n#ChallengeConfidence = 0\n\n# 单次 Recover 允许包含的扇区数量上限，选填项，数字类型\n# 默认值为 0\n# 设置为 0 时，不会进行限制\n#MaxRecoverSectorLimit = 0\n\n# 单条 PoSt 消息中允许的最大 Partition 数量，选填项，数字类型\n# 默认值为 0\n# 设置为 0 时，会使用默认最大值\n#MaxPartitionsPerPoStMessage = 0\n\n# 单条 Recover 消息中允许的最大 Partition 数量，选填项，数字类型\n# 默认值为 0\n# 设置为 0 时，不限制\n#MaxPartitionsPerRecoveryMessage = 0\n\n\n\n# [Miners.Proof]\n\n用于配置 WinningPoSt Proof 相关的策略\n\n[Miners.Proof]\n# 是否启用，选填项，布尔类型\n# 默认值为 false\n#Enabled = false\n\n\n\n# [Miners.Sealing]\n\n用于配置 sealing 过程中相关的策略\n\n[Miners.Sealing]\n# sealing 过程需要持续的高度，在筛选订单的时候会将订单的开始限定为当前高度 + 该值，选填项，整数类型\n# 默认为 0，表示没配置\n#SealingEpochDuration = 0\n#\n# 是否启用 SyntheticPoRep , 选填项，布尔类型\n# 默认值为 false\n#UseSyntheticPoRep = false\n\n\n\n# [Miners.Deal] 已废弃\n\n用于配置订单相关的策略。\n\n[Miners.Deal]\n# 是否启用，选填项，布尔类型\n# 默认值为 false\n#Enabled = false\n\n\n\n# 一份最简可工作的配置文件范例\n\n我们以启动支持一个 SP 运作的 damocles-manager 为例，\n\n[Common]\n[Common.API]\nGateway = ["/ip4/{api_host}/tcp/{api_port}"]\nToken = "{some token}"\nChain = "/ip4/{api_host}/tcp/{api_port}"\nMessager = "/ip4/{api_host}/tcp/{api_port}"\nMarket = "/ip4/{api_host}/tcp/{api_port}"\n\n[[Common.PieceStores]]\nPath = "{store_path}"\n\n[[Common.PersistStores]]\nName = "{store_name1}"\nPath = "{store_path1}"\n\n[[Common.PersistStores]]\nName = "{store_name2}"\nPath = "{store_path2}"\n\n[[Common.PersistStores]]\nName = "{store_name3}"\nPath = "{store_path3}"\n\n[[Common.PersistStores]]\nName = "{store_name4}"\nPath = "{store_path4}"\n\n[[Miners]]\nActor = 10086\n[Miners.Sector]\nInitNumber = 1000\nEnabled = true\nEnableDeals = true\n\n[Miners.Commitment]\n[Miners.Commitment.Pre]\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n[Miners.Commitment.Pre.Batch]\nEnabled = false\n\n[Miners.Commitment.Prove]\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n[Miners.Commitment.Prove.Batch]\nEnabled = true\n\n[Miners.PoSt]\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\nEnabled = true\n\n[Miners.Proof]\nEnabled = true\n\n\n\n这样就激活了一个：\n\n * 拥有 1 个本地 PieceStore\n * 拥有 4 个本地 持久化 Store\n * 启用扇区分配，其初始编号为 1000\n * 不启用聚合 PreCommit\n * 启用聚合 ProveCommit\n * 启用 WinningPoSt 模块\n * 启用订单\n\n的 damocles-manager 实例。',normalizedContent:'# damocles-manager 的配置解析\n\ndamocles-manager 是与链交互、维持扇区的主体，我们来了解一下它的配置文件结构和配置方式。\n\n在完成初始化之后，我们可以得到一份原始的配置内容：\n\n# default config:\n[common]\n[common.api]\n#gateway = ["/ip4/{api_host}/tcp/{api_port}"]\n#token = "{some token}"\n#chaineventinterval = "1m0s"\n#chain = "/ip4/{api_host}/tcp/{api_port}"\n#messager = "/ip4/{api_host}/tcp/{api_port}"\n#market = "/ip4/{api_host}/tcp/{api_port}"\n#\n[[common.piecestores]]\n#name = "{store_name}"\n#path = "{store_path}"\n#readonly = false\n#plugin = ""\n#pluginname = "s3store"\n[common.piecestores.meta]\n#somekey = "somevalue"\n#\n[common.piecestorepreset]\n#strict = false\n#readonly = false\n#weight = 1\n#allowminers = [1, 2]\n#denyminers = [3, 4]\n#storageconfigpath = "/optional/path/to/your/storage.json"\n[common.piecestorepreset.meta]\n#somekey = "somevalue"\n#\n[[common.persiststores]]\n#name = "{store_name}"\n#path = "{store_path}"\n#strict = false\n#readonly = false\n#weight = 0\n#allowminers = [1, 2]\n#denyminers = [3, 4]\n#plugin = ""\n#pluginname = "s3store"\n[common.persiststores.meta]\n#somekey = "somevalue"\n[common.db]\n#driver = "badger"\n[common.db.badger]\n#basedir = ""\n[common.proving]\n#parallelchecklimit = 128\n#singlechecktimeout = "10m0s"\n#partitionchecktimeout = "20m0s"\n[common.proving.workerprover]\njobmaxtry = 2\nheartbeattimeout = "15s"\njoblifetime = "25h0m0s"\n\n[[miners]]\n#actor = 10086\n[miners.sector]\n#initnumber = 0\n#minnumber = 10\n#maxnumber = 1000000\n#enabled = true\n#enabledeals = false\n#lifetimedays = 540\n#verbose = false\n[miners.snapup]\n#enabled = false\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#sendfund = true\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n#cleanupccdata = true\n#messageconfidence = 15\n#releaseconfidence = 30\n[miners.snapup.retry]\n#maxattempts = 10\n#pollinterval = "3m0s"\n#apifailurewait = "3m0s"\n#localfailurewait = "3m0s"\n[miners.commitment]\n#confidence = 10\n[miners.commitment.pre]\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#sendfund = true\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.pre.batch]\n#enabled = false\n#threshold = 16\n#maxwait = "1h0m0s"\n#checkinterval = "1m0s"\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.prove]\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#sendfund = true\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.prove.batch]\n#enabled = false\n#threshold = 16\n#maxwait = "1h0m0s"\n#checkinterval = "1m0s"\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.terminate]\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#sendfund = true\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.commitment.terminate.batch]\n#enabled = false\n#threshold = 5\n#maxwait = "1h0m0s"\n#checkinterval = "1m0s"\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n[miners.post]\n#sender = "f1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n#enabled = true\n#strictcheck = true\n#parallel = false\n#gasoverestimation = 1.2\n#gasoverpremium = 0.0\n#gasfeecap = "5 nanofil"\n#maxfeecap = ""\n#confidence = 10\n#submitconfidence = 0\n#challengeconfidence = 0\n#maxrecoversectorlimit = 0\n#maxpartitionsperpostmessage = 0\n#maxpartitionsperrecoverymessage = 0\n[miners.proof]\n#enabled = false\n[miners.sealing]\n#sealingepochduration = 0\n#usesyntheticporep = false\n\n\n我们将逐一分析其中的可配置项。\n\n\n# [common]\n\ncommon 是公共配置，又分成四个子配置项：\n\n\n# [common.api]\n\ncommon.api 是接口相关的配置，其内容包含：\n\n[common.api]\n# 网关服务地址，必填项，字符串类型\n# 根据所使用的服务实际情况填写\n# 对于每一条信息，如果使用了合法的 "{token}:{multiaddr}" 格式，构造客户端时将使用本字段中提取的 token，否则使用通用 sophon-auth 产生的 token\ngateway = ["/ip4/{api_host}/tcp/{api_port}"]\n\n# 链服务信息，可选项，字符串类型\n# 根据所使用的服务实际情况填写\n# 如果使用了合法的 "{token}:{multiaddr}" 格式，构造客户端时将使用本字段中提取的 token，否则使用通用 sophon-auth 产生的 token\n# 如果不填写，会默认使用 gateway 的地址和 token 作为链服务的入口 \nchain = "/ip4/{api_host}/tcp/{api_port}"\n\n# 消息服务信息，可选项，字符串类型\n# 根据所使用的服务实际情况填写\n# 如果使用了合法的 "{token}:{multiaddr}" 格式，构造客户端时将使用本字段中提取的 token，否则使用通用 sophon-auth 产生的 token\n# 如果不填写，会默认使用 gateway 的地址和 token 作为消息服务的入口 \nmessager = "/ip4/{api_host}/tcp/{api_port}"\n\n# 市场服务信息，可选项，字符串类型\n# 根据所使用的服务实际情况填写\n# 如果使用了合法的 "{token}:{multiaddr}" 格式，构造客户端时将使用本字段中提取的 token，否则使用通用 sophon-auth 产生的 token\n# 如果不填写，会默认使用 gateway 的地址和 token 作为消息服务的入口 \nmarket = "/ip4/{api_host}/tcp/{api_port}"\n\n# 服务 sophon-auth 产生的 token，必填项，字符串类型\n# 根据所使用的服务实际情况填写\ntoken = "{some token}"\n\n# 侦测链高度变化的间隔时间，选填项，时间类型\n# 默认值为 1min\n#chaineventinterval = "1m0s"\n\n\n\n# [common.plugins]\n\ncommon.plugins 配置 damocles-manager 的插件存放路径\n\n[common.plugins]\n# 插件存放路径，选填项，字符串类型\n# 默认为空字符串，表示不加载任何插件\n# 建议使用绝对路径\ndir = ""\n\n\n\n# [[common.piecestores]]\n\ncommon.piecestores是用于配置本地订单 piece 数据的选项。当存在可用的离线存储时，可以配置此项，避免通过公网获取订单的piece 数据。\n\n每一个本地存储目录对应一个 common.piecestores 配置块。\n\n# 基础配置范例\n\n[[common.piecestores]]\n# 名称，选填项，字符串类型\n# 默认为路径对应的绝对路径\n#name = "remote-store1"\n\n# 路径，必填项，字符串类型\npath = "/mnt/mass/piece1"\n\n# 插件路径，选填项，字符串类型\n# 默认为 null\n# 如果希望使用自定义存储方案，可以通过编写符合要求的 golang plugin，并在此设置。\n# 已过时，请使用 pluginname 代替\n#plugin = "path/to/objstore-plugin"\n\n# 插件名称，选填项，字符串类型\n# 默认为空字符串\n# 如果希望使用自定义存储方案，可以通过编写符合要求的 golang plugin，并在此设置。\n#pluginname = "s3store"\n\n# 元信息，选填项，字典类型\n# 内部值为 key = "value" 的格式\n# 默认值为 null\n# 用于支持不同类型存储方案\n[common.piecestores.meta]\n#somekey = "somevalue"\n#\n\n\n\n# [common.piecestorepreset]\n\npersiststore 指的是扇区持久化数据存储。与之对应的是 damocles-worker 中的 attached 概念。\n\n而 piecestorepreset 为全局的所有 persiststore 配置，提供了一套可自定义的预设值，与后一章节中的 [[common.persiststores]] 配置项相呼应。合理利用 piecestorepreset 可以大大简化 [[common.persiststores]] 的配置。\n\n同时，damocles-manager 还支持 lotus 风格的存储配置文件（例如 lotus-miner repo 下默认生成的 storage.json 文件）。damocles-manager 会根据 piecestorepreset 为该配置文件下的每一个路径在内存中生成一个对应的 persiststore 配置项。\n\n# 基础配置范例\n\n[common.piecestorepreset]\n\n# 只读，选填项，布尔类型\n# 默认值为 false\n# 自 v0.4.0 起，持久化存储分配逻辑转到 damocles-manager 上\n# 可通过此配置设置存储是否可以继续写入\nreadonly = false\n\n# 可选项，布尔类型\n# 默认 false\n# 是否验证`path`路径是否为通常文件，true 时，`path`为软连接等非通常文件时会报错\nstrict = false\n\n# 权重，选填项，数字类型\n# 默认值为 1\n# 当填写值为 0 时，等效于 1\n# 自 v0.4.0 起，持久化存储分配逻辑转到 damocles-manager 上\n# 可通过此配置设置多个持久化存储之间的权重配比\n# 每个持久化存储被选中的概率为 `weight / sum`, `sum` 是所有可用的持久化存储权重的和\n# 例：配置 3 个 持久化存储，weight 分别为 2, 1, 1。则被选中的概率分别为 50%, 25%, 25%\nweight = 1\n\n\n# 允许进行分配的矿工号列表，选填项，数字数组类型\n# 默认为 null\n# 当不设置时，视为允许全部矿工号；当设置时，则相当于白名单，仅允许分配给列出的矿工号\n# 如果一个矿工号同时出现在 allowminers 和 denyminers 中时，denyminers 优先生效，即视为拒绝\nallowminers = [1, 2]\n\n# 拒绝进行分配的矿工号列表，选填项，数字数组类型\n# 默认为 null\n# 当不设置时，视为不拒绝任何矿工号；当设置时，则相当于黑名单，将拒绝为列出的矿工号分配\n# 如果一个矿工号同时出现在 allowminers 和 denyminers 中时，denyminers 优先生效，即视为拒绝\ndenyminers = [3, 4]\n\n# lotus 风格的存储配置文件的路径，选填项，字符串类型\n# 默认为空字符串\n# 如果只是希望简单地沿用 lotus 的存储路径配置，可以选择填写此项\n# 但是如果希望更加深入地对每个存储路径进行细致的设置，建议使用 `[[common.persiststores]]` 配置项\nstorageconfigpath = "/optional/path/to/your/storage.json"\n\n# 元信息，选填项，字典类型\n# 内部值为 key = "value" 的格式\n# 默认值为 null\n# 用于支持不同类型存储方案的预备，目前没有任何作用\n[common.persiststores.meta]\n#somekey = "somevalue"\n#\n\n\n * storage.json\n\n{\n  "storagepaths": [\n    {\n      "name": "persist",\n      "path": "/root/persist"\n    }\n  ]\n}\n\n\n其中 name 属性可以省略，省略时默认使用 path 属性的值作为 name。\n\n\n# [[common.persiststores]]\n\ncommon.persiststores 用于配置扇区持久化数据存储。与之对应的是 damocles-worker 中的 attached 概念。\n\n与 common.piecestores 类似，每一个持久化存储目录对应一个 common.persiststores 配置块。\n\n# 基础配置范例\n\n[[common.persiststores]]\n# 名称，选填项，字符串类型\n## 默认为路径对应的绝对路径\n#name = "remote-store1"\n\n# 路径，必填项，字符串类型\n# 建议使用绝对路径\npath = "/mnt/remote/10.0.0.14/store"\n\n# 只读，选填项，布尔类型\n# 默认值为 false\n# 自 v0.4.0 起，持久化存储分配逻辑转到 damocles-manager 上\n# 可通过此配置设置存储是否可以继续写入\n#readonly = false\n\n# 可选项，布尔类型\n# 默认 false\n# 是否验证`path`路径是否为通常文件，true 时，`path`为软连接等非通常文件时会报错\n#strict = false\n\n# 权重，选填项，数字类型\n# 默认值为 1\n# 当填写值为 0 时，等效于 1\n# 自 v0.4.0 起，持久化存储分配逻辑转到 damocles-manager 上\n# 可通过此配置设置多个持久化存储之间的权重配比\n# 每个持久化存储被选中的概率为 `weight / sum`, `sum` 是所有可用的持久化存储权重的和\n# 例：配置 3 个 持久化存储，weight 分别为 2, 1, 1。则被选中的概率分别为 50%, 25%, 25%\n#weight = 1\n\n# 插件路径，选填项，字符串类型\n# 默认为 null\n# 如果希望使用自定义存储方案，可以通过编写符合要求的 golang plugin，并在此设置。\n# 已过时，请使用 pluginname 代替\n#plugin = "path/to/objstore-plugin"\n\n# 允许进行分配的矿工号列表，选填项，数字数组类型\n# 默认为 null\n# 当不设置时，视为允许全部矿工号；当设置时，则相当于白名单，仅允许分配给列出的矿工号\n# 如果一个矿工号同时出现在 allowminers 和 denyminers 中时，denyminers 优先生效，即视为拒绝\n#allowminers = [1, 2]\n\n# 拒绝进行分配的矿工号列表，选填项，数字数组类型\n# 默认为 null\n# 当不设置时，视为不拒绝任何矿工号；当设置时，则相当于黑名单，将拒绝为列出的矿工号分配\n# 如果一个矿工号同时出现在 allowminers 和 denyminers 中时，denyminers 优先生效，即视为拒绝\n#denyminers = [3, 4]\n\n# 插件路径，选填项，字符串类型\n# 默认为空字符串\n# 如果希望使用自定义存储方案，可以通过编写符合要求的 golang plugin，并在此设置。\n#pluginname = "s3store"\n\n# 元信息，选填项，字典类型\n# 内部值为 key = "value" 的格式\n# 默认值为 null\n# 用于支持不同类型存储方案的预备，目前没有任何作用\n[common.persiststores.meta]\n#somekey = "somevalue"\n#\n\n\n\n# [common.mongokvstore] 已废弃\n\ncommon.mongokvstore 用于配置 damocles-manager 是否启用 mongo 作为 sealing 过程中使用的 kv 数据库。\n\n# 基础配置范例\n\n[common.mongokvstore]\n# 启用 mongo 的开关，选填项，布尔类型\n# 默认值为 false\nenable = true\n# 使用的 mongo 的 dsn，在 enable 为 true 的时候为必填项，字符串类型\ndsn = "mongodb://127.0.0.1:27017/?directconnection=true&serverselectiontimeoutms=2000"\n# 使用的 mongo 的数据库名，在 enable 为 true 的时候为必填项，字符串类型\ndatabasename = "test"\n\n\n\n# [common.proving]\n\n用于配置证明任务的并发以及超时时间\n\n配置范例：\n\n# 检查扇区时的最大并行数量，可选项，数字类型\n# 默认值为 128。设置为 0 表示不限制并行数量\n# 注意：将此值设置得太高可能会使节点因堆栈不足而崩溃\n# 注意：将此值设置得太低可能会使扇区 challenge 读取速度变慢，导致 post 失败\n#parallelchecklimit = 128\n# 单个扇区的验证预检查所需的最长时间 可选项，时间类型\n# 默认值为 10m0s\n# 如果检查超时，将跳过扇区。\n#singlechecktimeout = "10m0s"\n# 整个分区的验证预检查所需的最长时间，可选项，时间类型\n# 如果检查超时，分区中未按时检查的扇区将被跳过\n#partitionchecktimeout = "20m0s"\n\n\n\n# [common.proving.workerprover]\n\n用于配置 worker prover 模块\n\n配置范例：\n\n# windowpost 任务的最大尝试次数，可选项，数字类型\n# 默认值为 2\n# 尝试次数超过 jobmaxtry 的 windowpost 任务只能通过手动 reset 的方式被重新执行\njobmaxtry = 2\n# windowpost 任务的心跳超时时间，可选项，时间字符串类型\n# 默认值为 15s\n# 超过此时间没有发送心跳的任务将会被设置为失败并重试\nheartbeattimeout = "15s"\n# windowpost 任务的心跳超时时间，可选项，时间字符串类型\n# 默认值为 25h\n# 创建时间超过此时间的 windowpost 任务将会被删除\njoblifetime = "25h0m0s"\n\n\n\n# [common.db]\n\ncommon.db 用于配置 sealing 过程中使用的 kv 数据库。目前支持 badger 本地数据库和 mongo 数据库。\n\n# 基础配置范例：\n\n[common.db]\n# 指定数据库，可选项，字符串类型\n# 默认值为 badger\n# 可选填 badger | mongo | plugin\ndriver = "badger"\n[common.db.badger]\n# badger 数据库文件所在目录，可选项，字符串类型\n# 默认为空字符串\n# basedir 为空字符串时 damocles-manager 会使用 home dir(默认为 ~/.damocles-manager) 存放 badger 数据库文件\n#basedir = ""\n\n[common.db.mongo]\n# 使用的 mongo 的 dsn，在 enable 为 true 的时候为必填项，字符串类型\ndsn = "mongodb://127.0.0.1:27017/?directconnection=true&serverselectiontimeoutms=2000"\n# 使用的 mongo 的数据库名，在 enable 为 true 的时候为必填项，字符串类型\ndatabasename = "test"\n\n# kvstore 插件配置\n[common.db.plugin]\n# 插件名称，选填项，字符串类型\n# 默认为 null\npluginname = "redis"\n\n# 元信息，选填项，字典类型\n# 内部值为 key = "value" 的格式\n# 默认值为 null\n# meta 数据会传入插件的构造函数中\n[common.db.plugin.meta]\n#somekey = "somevalue"\n#\n\n\n\n# [[miners]]\n\nminers 是较为重要的一个配置项，用于针对某一个 sp 定义其行为和策略。\n\ndamocles 被设计为同一套组件可以支持多个 sp ，在 damocles-manager 中的具体表现就是可以根据需要设置多个 miners 配置块。\n\n\n# 主配置项\n\n[[miners]]\n# `sp` actor id，必填项，数字类型\nactor = 10086\n\n\n除主配置向外， miners 同样包含多个不同的子配置块，下面我们一一分析\n\n\n# [miners.sector]\n\n用于控制扇区分配的策略。\n\n[miners.sector]\n# 扇区起始编号，选填项，数字类型\n# 默认值为 0\n# 已废弃\ninitnumber = 0\n\n# 扇区最小编号，选填项，数字类型\n# 默认值为 null\n# 与 initnumber 相比，当设置此项时，\n# 1. 任何时刻，分配器都不会给出小于等于此值的扇区编号。\n# 2. 此项的值可以在集群运行过程中调整。\n#    提高配置值，分配结果将始终遵循 1) 的描述。\n#    降低配置值通常不会产生效果。\n#\n# 未设置此项时，如果 initnumber 为非 0 值，则等效于此项。\n#minnumber = 10\n\n# 扇区编号上限，选填项，数字类型\n# 默认值为 null，表示无上限限制\n#maxnumber = 1000000\n\n# 是否允许分配扇区，选填项，布尔类型\n# 默认值为 true，即开启分配\n# false, 不再进行封装任务\n#enabled = true\n\n# 是否允许分配订单，选填项，布尔类型\n# 默认值为 false\n#enabledeals = false\n\n# cc 扇区的生命周期，单位为 天，选填项，数字类型\n# 默认值为 540\n#lifetimedays = 540\n\n# sector 相关模块的日志详尽程度，选填项，布尔类型\n# 默认值为 false，即精简日志输出\n#verbose = false\n\n\n\n# [miners.snapup]\n\n用于控制 snapdeal 的生产策略\n\n[miners.snapup]\n# 是否启用，选填项，布尔类型\n# 默认值为 false\n#enabled = false\n\n# 发送地址，在启用的情况下为必填项，地址类型\n#sender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# 提交上链消息时是否从 sender 发送必要的资金，选填项，布尔类型\n# 默认值为 true\n# false 时，资金从 miner 出\n#sendfund = true\n\n# 单条提交消息的 gas 估算倍数，选填项，浮点数类型\n# 默认值为 1.2\n#gasoverestimation = 1.2\n\n# 单条提交消息的 gasfeecap 限制，选填项，fil 值类型\n# 默认值为 5 nanofil\n#gasfeecap = "5 nanofil"\n\n# 已废弃\n#maxfeecap = ""\n\n# 单条提交消息的 gaspremium 估算倍数，选填项，浮点数类型\n# 默认值为 0.0\n#gasoverpremium = "0.0"\n\n# snapdeal 消息上链后是否删除原 cc 扇区数据\n# 默认值为 true\n#cleanupccdata = true\n\n# 消息上链的确信高度，选填项，数字类型\n# 默认值为 15\n#messageconfidence = 15\n\n# 释放旧数据存储空间的确信高度，选填项，数字类型\n# 默认值为 30\n#releaseconfidence = 30\n\n# snapup 提交重试策略\n[miners.snapup.retry]\n\n# 最大重试次数，选填项，数字类型\n# 默认为 null，表示不做限制\n#maxattempts = 10\n\n# 轮询状态的间隔，选填项，时间类型\n# 默认值为 3min\n#pollinterval = "3m0s"\n\n# api 接口异常的重试间隔，选填项，时间类型\n# 默认值为 3min\n#apifailurewait = "3m0s"\n\n# 本地异常的重试间隔，如本地数据库异常、本地存储异常等，选填项，时间类型\n# 默认值为 3min\n#localfailurewait = "3m0s"\n\n\n\n# [miners.commitment]\n\n用于配置封装消息提交策略的通用部分。\n\n[miners.commitment]\n# 消息的稳定高度，选填项，数字类型\n# 默认值为 10\n#confidence = 10\n\n\n\n# [miners.commitment.pre]\n\n用于配置 precommit 消息提交的策略\n\n[miners.commitment.pre]\n# 提交上链消息时是否从 sender 发送必要的资金，选填项，布尔类型\n# 默认值为 true\n#sendfund = true\n\n# 发送地址，必填项，地址类型\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# 单条提交消息的 gas 估算倍数，选填项，浮点数类型\n# 默认值为 1.2\n#gasoverestimation = 1.2\n\n# 单条提交消息的 gasfeecap 限制，选填项，fil 值类型\n# 默认值为 5 nanofil\n#gasfeecap = "5 nanofil"\n\n# 已废弃\n#maxfeecap = ""\n\n# 单条提交消息的 gaspremium 估算倍数，选填项，浮点数类型\n# 默认值为 0.0\n#gasoverpremium = "0.0"\n\n# 聚合提交的策略配置块\n[miners.commitment.pre.batch]\n# 是否启用聚合提交，选填项，布尔类型\n# 默认值为 false，即不启用\n#enabled = false\n\n# 最小聚合条数，选填项，数字类型\n# 默认值为 16，即最小聚合条数为 16 条\n#threshold = 16\n\n# 最大等待时间，选填项，时间类型\n# 默认值为 1h，即最大等待 1 小时\n#maxwait = "1h0m0s"\n\n# 检查间隔，选填项，时间类型\n# 默认值为 1min，即每隔 1min 检查一次是否满足聚合条件\n#checkinterval = "1m0s"\n\n# 聚合提交消息的 gas 估算倍数，选填项，浮点数类型\n# 默认值为 1.2\n#gasoverestimation = 1.2\n\n# 聚合提交消息的 gasfeecap 限制，选填项，fil 值类型\n# 默认值为 5 nanofil\n#gasfeecap = "5 nanofil"\n\n# 已废弃\n#maxfeecap = ""\n\n# 聚合提交消息的 gaspremium 估算倍数，选填项，浮点数类型\n# 默认值为 0.0\n#gasoverpremium = "0.0"\n\n\n\n# [miners.commitment.prove]\n\n用于配置 provecommit 消息提交的策略，其配置项和作用与 miners.commitment.pre内的完全一致。\n\n\n# [miners.commitment.terminate]\n\n用于配置 terminatesectors 消息提交的策略，其配置项和作用与 miners.commitment.pre 内的基本一致。实际场景中发送此类消息不会很频繁，建议配置单条提交模式，使用聚合提交模式时，threshold 建议配置较小的值，保证消息及时上链。\n\n\n# [miners.post]\n\n用于配置 windowpost 的相关策略。\n\n[miners.post]\n# 发送地址，必填项，地址类型\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n# 是否启用，选填项，布尔类型\n# 默认值为 true\n#enabled = true\n\n# 是否对扇区文件进行强校验，选填项，布尔类型\n# 默认值为 true\n# 开启时，除了对文件存在性进行判断外，还会尝试读取部分信息，如元数据等\n#strictcheck = true\n\n# 是否启用并行证明，选填项，布尔类型\n# 默认值为 false\n# 开启时，同一个 deadline 内的多个 partition 将会并行展开证明\n# 注意：当设置了外部证明器 (ext-prover)，且有多个外部证明器可用时，设置此项才有正面影响\n#parallel = false\n\n# windowpost 消息的 gas 估算倍数，选填项，浮点数类型\n# 默认值为 1.2\n#gasoverestimation = 1.2\n\n# windowpost 消息的 gasfeecap 限制，选填项，fil 值类型\n# 默认值为 5 nanofil\n#gasfeecap = "5 nanofil"\n\n# 已废弃\n#maxfeecap = ""\n\n# windowpost 消息的 gaspremium 估算倍数，选填项，浮点数类型\n# 默认值为 0.0\n#gasoverpremium = "0.0"\n\n# 消息的稳定高度，选填项，数字类型\n# 默认值为 10\n#confidence = 10\n\n# 提交 windowpost 证明结果的稳定高度，选填项，数字类型\n# 这个值决定了需要等待多少个高度才认定链进入稳定状态，可以提交 windowpost 证明结果\n# 提交高度为 deadline.open + submitconfidence\n# 此值设定越小，会越早启动，但同时也越容易受到分叉影响\n# 当设置为 0 时，会使用默认值 4\n#submitconfidence = 0\n\n# 启动 windowpost 的稳定高度，选填项，数字类型\n# 这个值决定了需要等待多少个高度才认定链进入稳定状态，可以启动 windowpost 任务\n# 启动高度为 deadline.challenge + challengeconfidence\n# 此值设定越小，会越早启动，但同时也越容易受到分叉影响\n# 当设置为 0 时，会使用默认值 10\n#challengeconfidence = 0\n\n# 单次 recover 允许包含的扇区数量上限，选填项，数字类型\n# 默认值为 0\n# 设置为 0 时，不会进行限制\n#maxrecoversectorlimit = 0\n\n# 单条 post 消息中允许的最大 partition 数量，选填项，数字类型\n# 默认值为 0\n# 设置为 0 时，会使用默认最大值\n#maxpartitionsperpostmessage = 0\n\n# 单条 recover 消息中允许的最大 partition 数量，选填项，数字类型\n# 默认值为 0\n# 设置为 0 时，不限制\n#maxpartitionsperrecoverymessage = 0\n\n\n\n# [miners.proof]\n\n用于配置 winningpost proof 相关的策略\n\n[miners.proof]\n# 是否启用，选填项，布尔类型\n# 默认值为 false\n#enabled = false\n\n\n\n# [miners.sealing]\n\n用于配置 sealing 过程中相关的策略\n\n[miners.sealing]\n# sealing 过程需要持续的高度，在筛选订单的时候会将订单的开始限定为当前高度 + 该值，选填项，整数类型\n# 默认为 0，表示没配置\n#sealingepochduration = 0\n#\n# 是否启用 syntheticporep , 选填项，布尔类型\n# 默认值为 false\n#usesyntheticporep = false\n\n\n\n# [miners.deal] 已废弃\n\n用于配置订单相关的策略。\n\n[miners.deal]\n# 是否启用，选填项，布尔类型\n# 默认值为 false\n#enabled = false\n\n\n\n# 一份最简可工作的配置文件范例\n\n我们以启动支持一个 sp 运作的 damocles-manager 为例，\n\n[common]\n[common.api]\ngateway = ["/ip4/{api_host}/tcp/{api_port}"]\ntoken = "{some token}"\nchain = "/ip4/{api_host}/tcp/{api_port}"\nmessager = "/ip4/{api_host}/tcp/{api_port}"\nmarket = "/ip4/{api_host}/tcp/{api_port}"\n\n[[common.piecestores]]\npath = "{store_path}"\n\n[[common.persiststores]]\nname = "{store_name1}"\npath = "{store_path1}"\n\n[[common.persiststores]]\nname = "{store_name2}"\npath = "{store_path2}"\n\n[[common.persiststores]]\nname = "{store_name3}"\npath = "{store_path3}"\n\n[[common.persiststores]]\nname = "{store_name4}"\npath = "{store_path4}"\n\n[[miners]]\nactor = 10086\n[miners.sector]\ninitnumber = 1000\nenabled = true\nenabledeals = true\n\n[miners.commitment]\n[miners.commitment.pre]\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n[miners.commitment.pre.batch]\nenabled = false\n\n[miners.commitment.prove]\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n[miners.commitment.prove.batch]\nenabled = true\n\n[miners.post]\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\nenabled = true\n\n[miners.proof]\nenabled = true\n\n\n\n这样就激活了一个：\n\n * 拥有 1 个本地 piecestore\n * 拥有 4 个本地 持久化 store\n * 启用扇区分配，其初始编号为 1000\n * 不启用聚合 precommit\n * 启用聚合 provecommit\n * 启用 winningpost 模块\n * 启用订单\n\n的 damocles-manager 实例。',charsets:{cjk:!0}},{title:"damocles-worker 的配置解析",frontmatter:{},regularPath:"/zh/operation/damocles-worker-config.html",relativePath:"zh/operation/damocles-worker-config.md",key:"v-a98bfbc2",path:"/zh/operation/damocles-worker-config.html",headers:[{level:2,title:"[worker]",slug:"worker",normalizedTitle:"[worker]",charIndex:171},{level:3,title:"基础配置范例",slug:"基础配置范例",normalizedTitle:"基础配置范例",charIndex:2760},{level:2,title:"[metrics]",slug:"metrics",normalizedTitle:"[metrics]",charIndex:264},{level:3,title:"基础配置范例",slug:"基础配置范例-2",normalizedTitle:"基础配置范例",charIndex:2760},{level:2,title:"[sector_manager]",slug:"sector-manager",normalizedTitle:"[sector_manager]",charIndex:321},{level:3,title:"基础配置范例",slug:"基础配置范例-3",normalizedTitle:"基础配置范例",charIndex:2760},{level:2,title:"[sealing]",slug:"sealing",normalizedTitle:"[sealing]",charIndex:597},{level:3,title:"基础配置范例",slug:"基础配置范例-4",normalizedTitle:"基础配置范例",charIndex:2760},{level:3,title:"特殊配置范例",slug:"特殊配置范例",normalizedTitle:"特殊配置范例",charIndex:5860},{level:2,title:"[[sealing_thread]]",slug:"sealing-thread",normalizedTitle:"[[sealing_thread]]",charIndex:896},{level:3,title:"基础配置范例",slug:"基础配置范例-5",normalizedTitle:"基础配置范例",charIndex:2760},{level:3,title:"特殊配置范例",slug:"特殊配置范例-2",normalizedTitle:"特殊配置范例",charIndex:5860},{level:3,title:"sealing_thread 配置热更新",slug:"sealing-thread-配置热更新",normalizedTitle:"sealing_thread 配置热更新",charIndex:7615},{level:2,title:"[[attached]]",slug:"attached",normalizedTitle:"[[attached]]",charIndex:1447},{level:3,title:"基础配置范例",slug:"基础配置范例-7",normalizedTitle:"基础配置范例",charIndex:2760},{level:2,title:"[processors]",slug:"processors",normalizedTitle:"[processors]",charIndex:9342},{level:3,title:"[processors.limitation.concurrent]",slug:"processors-limitation-concurrent",normalizedTitle:"[processors.limitation.concurrent]",charIndex:1518},{level:3,title:"[processors.limitation.staggered]",slug:"processors-limitation-staggered",normalizedTitle:"[processors.limitation.staggered]",charIndex:1600},{level:3,title:"[processors.ext_locks]",slug:"processors-ext-locks",normalizedTitle:"[processors.ext_locks]",charIndex:1665},{level:3,title:"[processors.statictreed]",slug:"processors-static-tree-d",normalizedTitle:"[processors.statictreed]",charIndex:null},{level:3,title:"[[processors.{stage_name}]]",slug:"processors-stage-name",normalizedTitle:"[[processors.{stage_name}]]",charIndex:10303},{level:2,title:"一份最简可工作的配置文件范例",slug:"一份最简可工作的配置文件范例",normalizedTitle:"一份最简可工作的配置文件范例",charIndex:17929}],headersStr:"[worker] 基础配置范例 [metrics] 基础配置范例 [sector_manager] 基础配置范例 [sealing] 基础配置范例 特殊配置范例 [[sealing_thread]] 基础配置范例 特殊配置范例 sealing_thread 配置热更新 [[attached]] 基础配置范例 [processors] [processors.limitation.concurrent] [processors.limitation.staggered] [processors.ext_locks] [processors.statictreed] [[processors.{stage_name}]] 一份最简可工作的配置文件范例",content:'# damocles-worker 的配置解析\n\ndamocles-worker 是数据封装的执行主体，我们来了解一下它的配置文件结构和配置方式。\n\ndamocles-worker 的配置文件采用了 toml 格式，需要注意的是，这种格式中，以 # 开头的行将被视为注释，不会生效。\n\n以 mock 实例为例，一份基础的配置大概会是这样：\n\n[worker]\n# name = "worker-#1"\n# rpc_server.host = "192.168.1.100"\n# rpc_server.port = 17891\n\n[metrics]\n#enable = false\n#http_listen = "0.0.0.0:9000"\n\n[sector_manager]\nrpc_client.addr = "/ip4/127.0.0.1/tcp/1789"\n# rpc_client.headers = { User-Agent = "jsonrpc-core-client" }\n# piece_token = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiMS0xMjUiLCJwZXJtIjoic2lnbiIsImV4dCI6IiJ9.JenwgK0JZcxFDin3cyhBUN41VXNvYw-_0UUT2ZOohM0"\n\n[sealing]\n# allowed_miners = [10123, 10124, 10125]\n# allowed_sizes = ["32GiB", "64GiB"]\nenable_deals = true\n# disable_cc = true\n# max_deals = 3\n# min_deal_space = "8GiB"\nmax_retries = 3\n# seal_interval = "30s"\n# recover_interval = "60s"\n# rpc_polling_interval = "180s"\n# ignore_proof_check = false\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store1"\n# plan = "snapup"\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32GiB", "64GiB"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_deals = 3\n# sealing.min_deal_space = "8GiB"\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store2"\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store3"\n\n[[attached]]\n# name = "persist-store1"\nlocation = "./mock-tmp/remote"\n\n[processors.limitation.concurrent]\n# add_pieces = 5\n# pc1 = 3\n# pc2 = 2\n# c2 = 1\n\n[processors.limitation.staggered]\n# pc1 = "5min"\n# pc2 = "4min"\n\n[processors.ext_locks]\n# gpu1 = 1\n\n[processors.static_tree_d]\n# 2KiB = "./tmp/2k/sc-02-data-tree-d.dat"\n\n# fields for the add_pieces processor\n# [[processors.add_pieces]]\n\n# fields for tree_d processor\n[[processors.tree_d]]\n# auto_restart = true\n\n# fields for pc1 processors\n[[processors.pc1]]\n# bin = "./dist/bin/damocles-worker-plugin-pc1"\n# args = ["--args-1", "1", --"args-2", "2"]\nnuma_preferred = 0\ncgroup.cpuset = "4-5"\nenvs = { RUST_LOG = "info" }\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "6-7"\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-13"\n# auto_restart = true\n# inherit_envs = true\n\n# fields for pc2 processors\n[[processors.pc2]]\n# cgroup.cpuset = "24-27"\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc2]]\ncgroup.cpuset = "28-31"\n# auto_restart = true\n# inherit_envs = true\n\n# fields for c2 processor\n[[processors.c2]]\ncgroup.cpuset = "32-47"\n# auto_restart = true\n# inherit_envs = true\n\n\n下面我们将逐一分析其中的可配置项。\n\n\n# [worker]\n\nworker 配置项用于配置本实例的一些基础信息。\n\n\n# 基础配置范例\n\n[worker]\n# 实例名，选填项，字符串类型\n# 默认以连接 `damocles-manager` 所使用的网卡 IP 地址作为实例名\n# name = "worker-#1"\n\n# rpc 服务监听地址，选填项，字符串类型\n# 默认为 "0.0.0.0"，即监听本机所有地址\n# rpc_server.host = "192.168.1.100"\n\n# rpc 服务监听端口，选填项，数字类型\n# 默认为 17890\n# rpc_server.port = 17891\n\n# 本地 piece 文件目录, 选填项, 字符串数组类型\n# 如果设置了此项, worker 会从设置的目录中加载 piece 文件\n# 否则将会从 damocles-manager 加载远程 piece 文件\n# 如果 "/path/to/{your_local_pieces_dir01, your_local_pieces_dir02, ...}/piece_file_name" 文件不存在, worker 也会从 damocles-manager 加载\n# local_pieces_dirs = ["/path/to/your_local_pieces_dir01", "/path/to/your_local_pieces_dir02"]\n\n\n绝大多数情况下，本配置项内的各个字段无需手工配置。\n\n仅在一些特殊情况，诸如：\n\n * 希望按照自己的编排习惯命名每个 damocles-worker 实例\n * 不希望监听所有网卡 IP，仅允许本地的 rpc 请求\n * 一台机器上部署了多个 damocles-worker，为避免端口冲突，需要进行区分\n * damocles-worker 可以直接访问 piece_store 目录，可配置 local_pieces_dir 从本地加载 piece 文件\n\n等场景，需要按需手动配置这里的选项。\n\n\n# [metrics]\n\nmetrics 提供了监控指标 (prometheus) 相关的选项。\n\n\n# 基础配置范例\n\n[metrics]\n# 是否启用 prometheus exporter, 选填项，布尔类型\n# 默认值为 false\n# 当启用时，会监听 "http_listen" 所设置的 IP 和端口，提供 prometheus exporter\n#enable = false\n\n# prometheus exporter 监听的地址，选填项，地址类型\n# 默认值为 "0.0.0.0:9000"\n#http_listen = "0.0.0.0:9000"\n\n\n\n# [sector_manager]\n\nsector_manager 用于配置 damocles-manager 相关的信息，以使得 damocles-worker 可以正确的连接到对应的服务。\n\n\n# 基础配置范例\n\n[sector_manager]\n# 构造 rpc 客户端时使用的连接地址，必填项，字符串类型\n# 可以接受 `multiaddr` 格式，也可以接受诸如 `http://127.0.0.1:1789`，`ws://127.0.0.1:1789` 这样的 url 格式\n# 通常情况下，使用 `multiaddr` 格式以和其他组件保持一致\nrpc_client.addr = "/ip4/127.0.0.1/tcp/1789"\n\n# 构造 rpc 客户端时使用的 http 头信息，选填项，字典类型\n# 默认为 null\n# rpc_client.headers = { User-Agent = "jsonrpc-core-client" }\n\n# 请求订单 piece 数据时携带的校验 token， 选填项，字符串类型\n# 默认为 null\n# 当本实例允许封装带有订单数据的扇区时，通常需要设置此项\n# 此项的值通常即为所使用的 venus 系列服务的 sophon-auth组件产生的 token 值\n# piece_token = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJuYW1lIjoiMS0xMjUiLCJwZXJtIjoic2lnbiIsImV4dCI6IiJ9.JenwgK0JZcxFDin3cyhBUN41VXNvYw-_0UUT2ZOohM0"\n\n\n\n# [sealing]\n\nsealing 用于配置封装过程中的通用参数选项。\n\n\n# 基础配置范例\n\n[sealing]\n# 允许的`SP`，选填项，数字数组格式\n# 默认为 null，允许来自任何 `SP` 的任务\n# 配置后，仅可执行来自数组中罗列的 `SP` 的封装任务\n# allowed_miners = [10123, 10124, 10125]\n\n# 允许的扇区大小，选填项，字符串数组格式\n# 默认为 null， 允许任意大小的扇区任务\n# 配置后，仅可执行符合数组中罗列的扇区大小的任务\n# allowed_sizes = ["32GiB", "64GiB"]\n\n# 是否允许向扇区内添加订单，选填项，布尔类型\n# 默认为 false\n# 当设置为 true 时，通常需要同时设置 `sector_manager` 中的 `piece_token` 项\n# enable_deals = true\n\n# 是否禁用 cc 扇区，选填项，布尔类型\n# 默认为 false\n# enable_deals 为 true 时，开启此选项，将持续等待，直到获得分配的订单，而不是启动 cc 扇区\n# disable_cc = true\n\n# 允许向扇区内添加的最大订单数量，选填项，数字类型\n# 默认为 null\n# max_deals = 3\n\n# 一个扇区中填充的订单的最小体积，选填项，字节字符串格式\n# 默认为 null\n# min_deal_space = "8GiB"\n\n# 封装过程中遇到 temp 类型的错误时，重试的次数，选填项，数字格式\n# 默认为 5\n# max_retries = 3\n\n# 封装过程中遇到 temp 类型的错误时，重试的间隔，选填项，时间字符串格式\n# 默认为 "30s"， 即30秒\n# recover_interval = "30s"\n\n# 空闲的 `sealing_thread` 申请封装任务的间隔， 选填项，时间字符串格式\n# 默认为 "30s"， 即30秒\n# seal_interval = "30s"\n\n# rpc 状态轮询请求的间隔，选填项，时间字符串格式\n# 默认为 "180s"， 即180秒\n# 封装过程中，部分环节使用了轮询方式来获取非实时的信息，如消息上链等。\n# 这个值有助于避免过于频繁的请求占用网络资源\n# rpc_polling_interval = "180s"\n\n# 是否跳过 proof 的本地校验环节，选填项，布尔格式\n# 默认为 false\n# 通常只在诸如测试之类的情况下设置此项\n# ignore_proof_check = false\n\n# 无法从 `damocles_manager` 获取任务时，重试的次数，选填项，数字类型\n# 默认为 3\n# request_task_max_retries = 3\n\n\nsealing 中的配置项通常有根据经验预设的默认项，这使得我们在绝大多数情况下无需自行配置。\n\n\n# 特殊配置范例\n\n# 1. 测试网络，仅为特定 SP 提供服务\n\nallowed_miners = [2234, 2236, 2238]\n\n\n# 2. 大规模集群，降低网络占用\n\n# 在可恢复的异常中，有相当一部分是网络抖动带来的，增大自动恢复的间隔时间降低请求频率\nrecover_interval = "90s"\n\n# 正常过程中的轮询请求也增大间隔时间降低请求频率\nrpc_polling_interval = "300s"\n\n\n# 3. 增大扇区异常自愈的可能性\n\n# 增大自动恢复的尝试次数\nmax_retries = 10\n\n# 增大自动恢复的间隔时间\nrecover_interval = "60s"\n\n\n\n# [[sealing_thread]]\n\nsealing_thread 用于为每个扇区工作线程进行配置。一份配置文件中可以存在多个 sealing_thread 配置组。\n\nsealing_thread 会继承 [sealing] 中的配置。可以使用 sealing.* 覆盖掉 [sealing] 的配置。\n\n\n# 基础配置范例\n\n[[sealing_thread]]\n# 扇区数据目录路径，必填项，字符串类型\n# 建议使用绝对路径，数据目录和工作线程是一对一绑定的\nlocation = "/mnt/nvme1/store"\n\n# 任务类型，选填项，字符串类型\n# 默认值为 null\n# 可选项: sealer | snapup | rebuild | unseal | wdpost, 当不填写时，默认等效为 sealer\n# plan = "snapup"\n\n# 封装过程的定制参数，仅对当前工作线程生效\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32GiB", "64GiB"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n# sealing.request_task_max_retries = 3\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\n\n\n\nsealing_thread 的数量和对应的数据路径需要根据规划情况编排。\n\n为了方便组合搭配，每个 sealing_thread 可以配置独立的 sealing 子项，它满足：\n\n * 可配置项的命名、类型、效果与通用的 sealing 项保持一致\n\n * 仅对当前工作线程生效\n\n * 未配置时使用通用的 sealing 项内的值\n\n\n# 特殊配置范例\n\n# 1. 两个工作线程，分别为不同的 SP 服务\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\nsealing.allowed_miners = [1357]\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\nsealing.allowed_miners = [2468]\n\n\n# 2. 两个工作线程，分别为不同的扇区大小服务\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\nsealing.allowed_sizes = ["32GiB"]\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\nsealing.allowed_sizes = ["64GiB"]\n\n\n\n# sealing_thread 配置热更新\n\n在 damocles v0.5.0 之前版本，我们只能通过修改 sealing_thread 配置后并重启 damocles-worker 完成配置更新。 在有些场景下很不方便，例如：扇区重建时我们希望能够在不重启 damocles-worker 的情况下修改指定的 sealing_thread 的 plan 配置项。\n\nv0.5.0 之后支持 sealing_thread 配置热更新。在指定的 sealing_thread 中的 location 目录下创建名为 config.toml 的热更新配置文件，该配置文件的内容与 [[sealing_thread]] 内容完全一致，此配置文件中的配置项会覆盖 damocles-worker 中对应的 [[sealing_thread]] 的配置项，并且修改此配置文件不需要重启 damocles-worker 即可生效。\n\ndamocles-worker 中的 sealing_thread 会在新的扇区任务开始之前检查 location 目录下的 config.toml 文件，如果 config.toml 文件的内容发生了变化或者此文件删除了都会重新加载或移除此文件的配置。\n\n注意：\n\n * 热更新配置文件 config.toml 无法覆盖 sealing_thread 中的 location 配置项。\n * damocles-worker 主配置文件不支持热更新。\n\n# 基础配置范例\n\n# /path/to/the_sealing_thread_location/config.toml\n\n# 任务类型，选填项，字符串类型\n# 默认值为 null\n# 可选项: sealer | snapup | rebuild | unseal | wdpost, 当不填写时，默认等效为 sealer\n# plan = "rebuild"\n\n# 封装过程的定制参数，仅对当前工作线程生效\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32GiB", "64GiB"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n# sealing.request_task_max_retries = 3\n\n\n\n# [[attached]]\n\nattached 用于配置已完成的扇区持久化数据保存的位置，允许同时配置多个。\n\n\n# 基础配置范例\n\n[[attached]]\n# 名称， 选填项，字符串类型\n# 默认为路径对应的绝对路径\n# name = "remote-store1"\n\n# 路径，必填项，字符串类型\n# 建议直接填写绝对路径\nlocation = "/mnt/remote/10.0.0.14/store"\n\n# 只读，选填项，布尔类型\n# 默认值为 false\n# readonly = true\n\n\n\n由于需要在 damocles-worker 和 damocles-manager 之间协调存储位置信息，而在很多情况下，同一个持久化存储目录在damocles-worker 机器和 damocles-manager 机器上的挂载路径不完全一致，因此我们决定使用 name 作为协调的基础信息。\n\n如果持久化存储目录在所有机器上的挂载路径都统一的话，配置时也可以选择在 damocles-worker 和damocles-manager 两侧都不配置 name。这种情况下，两者都会使用绝对路径作为 name，也能匹配。\n\n\n# [processors]\n\nprocessors 用于配置封装执行器，和封装计算过程中的一些信息。\n\n这个配置项实际上分为三个子项，我们逐一分析。\n\n\n# [processors.limitation.concurrent]\n\nprocessors.limitation.concurrent 用于配置指定封装阶段的并行任务数量控制。这是为了降低指定阶段的资源相互争抢的情况。\n\n需要注意的是，当配置了外部执行器时，外部执行器的数量和允许的并发总量也会影响并行任务数量。\n\n# 基础配置范例\n\n[processors.limitation.concurrent]\n# add_pieces 阶段的并发数限制，选填项，数字类型\n# add_pieces = 5\n\n# tree_d 阶段的并发数限制，选填项，数字类型\n# tree_d = 1\n\n# pc1 阶段的并发数限制，选填项，数字类型\n# pc1 = 3\n\n# pc2 阶段的并发数限制，选填项，数字类型\n# pc2 = 1\n\n# c2 阶段的并发数限制，选填项，数字类型\n# c2 = 1\n\n\n举例来说，如果设置了 pc2 = 2，那么同一时间最多只会有两个扇区可以执行 pc2 阶段的任务。\n\n\n# [processors.limitation.staggered]\n\nprocessors.limitation.staggered 用于配置指定封装阶段并行任务错开启动的时间间隔。配置此项后当指定阶段有多个任务同时启动时，damocles-worker 会依次根据配置的时间间隔启动任务，以避免任务同时启动造成磁盘 IO 等资源紧张的问题。\n\n# 基础配置范例\n\n[processors.limitation.staggered]\n# 多个 pc1 任务依次启动的时间间隔，选填项，字符串类型 (e.g. "1s", "2min")\n# pc1 = "5min"\n# pc2 = "4min"\n\n\n举例来说，如果设置了 pc1 = "5min"，当两个 pc1 任务同时启动时，会先执行第一个任务 5 分钟后执行第二个任务。\n\n\n# [processors.ext_locks]\n\nprocessors.ext_locks 用于配置一些自定义的锁限制，它是和 [[processors.{stage_name}]] 中的 locks 配置项联动使用的。 这个配置项允许使用者自定一些限制条件，并令不同的外部处理器受其约束。\n\n# 基础配置范例\n\n[processors.ext_locks]\n# some_name = some_number\n\n\n# 特殊配置范例\n\nprocessors.ext_locks 自身是不能独立生效的。\n\n# 一块 GPU，pc2 和 c2 公用\n\n[processors.ext_locks]\ngpu = 1\n\n[[processors.pc2]]\nlocks = ["gpu"]\n\n[[processors.c2]]\nlocks = ["gpu"]\n\n\n这样，pc2 c2 会各启动一个外部处理器，两者将会产生竞争关系，也就意味着两者将不会同时发生。\n\n# 两块 GPU，pc2 和 c2 公用\n\n[processors.ext_locks]\ngpu1 = 1\ngpu2 = 1\n\n[[processors.pc2]]\nlocks = ["gpu1"]\n\n[[processors.pc2]]\nlocks = ["gpu2"]\n\n[[processors.c2]]\nlocks = ["gpu1"]\n\n[[processors.c2]]\nlocks = ["gpu2"]\n\n\n这样，pc2 c2 会各启动两个外部处理器，将会产生两两竞争的关系，从而允许限制一块 GPU 上只能执行其中一个阶段的任务。\n\n\n# [processors.static_tree_d]\n\nprocessors.static_tree_d 是为了提升 cc 扇区 的效率而引入的配置项。\n\n当为相应扇区大小配置了静态文件路径时，将会直接使用此文件作为 cc 扇区 的 tree_d 文件，而不会尝试再次生成。\n\n# 基础配置范例\n\n[processors.static_tree_d]\n2KiB = "/var/tmp/2k/sc-02-data-tree-d.dat"\n32GiB = "/var/tmp/32g/sc-02-data-tree-d.dat"\n64GiB = "/var/tmp/64g/sc-02-data-tree-d.dat"\n\n\n\n\n# [[processors.{stage_name}]]\n\n这是用于配置外部执行器的配置组。\n\n目前 {stage_name} 可选\n\n * add_pieces 用于 Add pieces 阶段\n * tree_d 用于 Tree D 的生成阶段\n * pc1 用于 PreCommit1 阶段\n * pc2 用于 PreCommit2 阶段\n * synth_proof 用于生成 Synthetic proof 阶段\n * c2：用于 Commit2 阶段\n * transfer：用于自定义本地数据和持久化数据存储之间的传输方式\n * unseal: 用于 Unseal 阶段\n\n每一个这样的配置组意味着将启动一个对应阶段的外部执行器。如果没有为上述的某个 {stage_name} 配置组配置任何内容，且配置中不存在对应的 [[processors.{stage_name}]] 这一行配置， 则 damocles-worker 不会为此 {stage_name} 启动子进程，damocles-worker 会使用内建的执行器代码在 sealing_thread 中直接执行对应的 {stage_name} 任务。这样 {stage_name} 任务的并发数量取决于对应的 sealing_thread 数量 和 [processors.limitation.concurrent] 中配置的 {stage_name} 并发数量。不配置外部执行器省去了序列化任务参数和任务输出等额外步骤，但是失去了更强大的并发控制，cgroup 控制和自定义算法等能力。可以根据使用场景自行取舍。\n\n[[processors.{stage_name}]] 可选的配置项包含：\n\n[[processors.pc1]]\n# 自定义外部执行器可执行文件路径，选填项，字符串类型\n# 默认会使用主进程对应的可执行文件路径，执行 damocles-worker 内建的执行器\n# bin = "./dist/bin/damocles-worker-plugin-pc1"\n\n# 自定义外部执行器的参数，选填项，字符串数组类型\n# 默认值为 null，将使用 `damocles-worker` 自己的执行器默认参数\n# args = ["--args-1", "1", --"args-2", "2"]\n\n# 外部执行器子进程准备就绪的时间，选填项，时间类型\n# 默认为 5s\n# stable_wait = "5s"\n\n# numa 亲和性分区 id，选填项，数字类型\n# 默认值为 null，不会设置亲和性\n# 需要根据宿主机的 numa 分区进行填写\n# numa_preferred = 0\n\n# cpu 核绑定和限制选项，选填项，字符串类型\n# 默认值为 null，不设置绑定\n# 值的格式遵循标准 cgroup.cpuset 格式\n# cgroup.cpuset = "4-5"\n\n# 外部执行器的附加环境变量，选填项，字典类型\n# 默认值为 null\n# envs = { RUST_LOG = "info" }\n\n# 本执行器允许的并发任务数量上限\n# 默认值为 null，无限制，但任务具体是否并发执行，视使用的外部执行器实现而定\n# 主要使用在 pc1 这样可以多个并行的环节，可以有效节约共享内存、线程池等资源\n# concurrent = 4\n\n# 自定义的外部限制锁名称，选填项，字符串数组类型\n# 默认值为 null\n# locks = ["gpu1"]\n\n# 当子进程退出时，是否自动重启，选填项，布尔类型\n# 默认值为 true\n# auto_restart = true\n\n# 是否继承 worker 守护进程的环境变量，选填项，布尔类型\n# 默认值为 true\ninherit_envs = true\n\n\n# 基础配置范例\n\n[processors.limitation.concurrent]\nadd_pieces = 8\npc1 = 4\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-7"\nconcurrent = 2\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\nauto_restart = true\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-19"\nconcurrent = 2\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\nauto_restart = true\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }\nauto_restart = true\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-39"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "1" }\nauto_restart = true\n\n[[processors.c2]]\ncgroup.cpuset = "28-35"\nenvs = { CUDA_VISIBLE_DEVICES = "2,3" }\nauto_restart = true\n\n\n[[processors.tree_d]]\ncgroup.cpuset = "40-45"\nauto_restart = true\n\n\n以上是基于一台 48C + 4GPU 的设备的 processors.{stage_name} 配置范例，在这套配置下，将启动：\n\n * 2 个 pc1 外部执行器，采用 MULTICORE_SDR 模式，各分配 8 核，允许 2 个并发任务，且内存分配优先使用本 numa 分区\n * 2 个 pc2 外部执行器，各分配 8 核，各使用一块 GPU\n * 1 个 c2 外部执行器，分配 8 核，使用一块 GPU\n * 1 个 tree_d 外部执行器，分配 6 核\n\n# 特殊配置范例\n\n# 1. 使用 patch 了闭源的、经过算法优化的 c2 外部执行器\n\n[[processors.c2]]\nbin = "/usr/local/bin/damocles-worker-c2-optimized"\ncgroup.cpuset = "40-47"\nenvs = { CUDA_VISIBLE_DEVICES = "2,3" }\n\n\n# 2. 使用外包模式的 c2 外部执行器\n\n[[processors.c2]]\nbin = "/usr/local/bin/damocles-worker-c2-outsource"\nargs = ["--url", "/ip4/apis.filecoin.io/tcp/10086/https", "--timeout", "10s"]\nenvs = { LICENCE_PATH = "/var/tmp/c2.licence.dev" }\n\n\n# 3. GPU 不足的情况下使用 CPU 模式弥补 pc2 计算能力\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-45"\n\n\n# 4. 最优配比下，pc1 总量为奇数，无法平分\n\n[processors.limitation.concurrent]\npc1 = 29\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-41"\nconcurrent = 14\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-92"\nconcurrent = 15\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n\n\n# 5. 希望优先集中使用 numa 0 区完成 pc1\n\n[processors.limitation.concurrent]\npc1 = 29\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-47"\nconcurrent = 16\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-86"\nconcurrent = 13\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n\n\n# cgroup.cpuset 配置注意事项\n\n * 针对启用 multicore sdr 的 PC1 外部处理器，cgroup.cpuset 尽量以整个 L3 cache 下的 CPU cores 为单位配置。如果需要配置 L3 cache 下的部分 CPU cores 必须保证每个 L3 cache 下 CPU cores 数量一致。\n   \n   rust-fil-proofs 中规定，当启用 multicore sdr 时，CPU 核心数量和这些 CPU 核心对应的 CPU 共享缓存 (通常是 L3 cache) 的数量必须是整数倍数关系。如果外部执行器使用的是 rust-fil-proofs 或者基于 rust-fil-proofs 开发，则必须遵守这个规则，否则可以忽略。damocles-worker 默认的执行器是基于 rust-fil-proofs 开发的。\n\n * cgroup.cpuset 中的 CPU cores 尽量不要跨 NUMA 节点，跨 NUMA 节点会使得 CPU 访问内存速度变慢 (damocles-cluster v0.5.0 之后，支持加载 NUMA 亲和的 hugepage 内存文件，如果启用该功能可以跨 NUMA 节点分配 cpuset 不会产生影响)\n   \n   如果配置的 CPU cores 均在同一 NUMA 节点，可以将 processors.{stage_name}.numa_preferred 配置为对应的 NUMA 节点 id。\n\n使用 damocles-worker-util 查看 CPU 信息\n\n./damocles-worker-util hwinfo\n\n\nOutput:\n\nCPU topology:\nMachine (503.55 GiB)\n├── Package (251.57 GiB) (*** *** *** 32-Core Processor)\n│   ├── NUMANode (#0 251.57 GiB)\n│   ├── L3 (#0 16 MiB)\n│   │   └── PU #0 + PU #1 + PU #2 + PU #3\n│   ├── L3 (#1 16 MiB)\n│   │   └── PU #4 + PU #5 + PU #6 + PU #7\n│   ├── L3 (#2 16 MiB)\n│   │   └── PU #8 + PU #9 + PU #10 + PU #11\n│   ├── L3 (#3 16 MiB)                       \n│   │   └── PU #12 + PU #13 + PU #14 + PU #15\n│   ├── L3 (#4 16 MiB)                       \n│   │   └── PU #16 + PU #17 + PU #18 + PU #19\n│   ├── L3 (#5 16 MiB)\n│   │   └── PU #20 + PU #21 + PU #22 + PU #23\n│   ├── L3 (#6 16 MiB)\n│   │   └── PU #24 + PU #25 + PU #26 + PU #27\n│   └── L3 (#7 16 MiB)\n│       └── PU #28 + PU #29 + PU #30 + PU #31\n└── Package (251.98 GiB) (*** *** *** 32-Core Processor)\n    ├── NUMANode (#1 251.98 GiB)\n    ├── L3 (#8 16 MiB)\n    │   └── PU #32 + PU #33 + PU #34 + PU #35\n    ├── L3 (#9 16 MiB)\n    │   └── PU #36 + PU #37 + PU #38 + PU #39\n    ├── L3 (#10 16 MiB)\n    │   └── PU #40 + PU #41 + PU #42 + PU #43\n    ├── L3 (#11 16 MiB)\n    │   └── PU #44 + PU #45 + PU #46 + PU #47\n    ├── L3 (#12 16 MiB)\n    │   └── PU #48 + PU #49 + PU #50 + PU #51\n    ├── L3 (#13 16 MiB)\n    │   └── PU #52 + PU #53 + PU #54 + PU #55\n    ├── L3 (#14 16 MiB)\n    │   └── PU #56 + PU #57 + PU #58 + PU #59\n    └── L3 (#15 16 MiB)\n        └── PU #60 + PU #61 + PU #62 + PU #63\n\n...\n\n\n从输出的信息可以看到这台机器有两个 NUMANode, 每个 NUMANode 上有 8 个 L3 Cache，每个 L3 cache 下有 4 个 CPU 核。\n\n * NUMANode #0 上的 CPU cores: 0-31\n * NUMANode #1 上的 CPU cores: 31-63\n\n以这台机器为例，如果配置 processors.pc1.cgroup.cpuset = "0-6" 是不满足 rust-fil-proofs 规则的。\n\n 1. PU#0, PU#1, PU#2, PU#3, 这 4 个 CPU cores 属于 L3#0\n 2. PU#4, PU#5, PU#6, 这 3 个 CPU cores 属于 L3#1\n\n此时 CPU 的共享缓存数量为 2 (L3#0, L3#1), 并且配置的 CPU cores 数量不一致，不满足 rust-fil-proofs 规则，无法启动。正确的配置可以是 processors.pc1.cgroup.cpuset = "0-7"。\n\n\n# 一份最简可工作的配置文件范例\n\n[sector_manager]\nrpc_client.addr = "/ip4/{some_ip}/tcp/1789"\n\n# 根据实际资源规划\n[[sealing_thread]]\nlocation = "{path to sealing store1}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store2}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store3}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store4}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store5}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store6}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store7}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store8}"\n\n\n[remote_store]\nname = "{remote store name}"\nlocation = "{path to remote store}"\n\n[processors.static_tree_d]\n32GiB = "{path to static tree_d for 32GiB}"\n64GiB = "{path to static tree_d for 64GiB}"\n\n# 根据实际资源规划\n[processors.limitation.concurrent]\npc1 = 4\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-7"\nconcurrent = 2\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-19"\nconcurrent = 2\nenvs = { FIL_PROOFS_USE_MULTICORE_SDR = "1" }\n\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-39"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "1" }\n\n\n[[processors.c2]]\ncgroup.cpuset = "28-35"\nenvs = { CUDA_VISIBLE_DEVICES = "2,3" }\n\n\n[[processors.tree_d]]\ncgroup.cpuset = "40-45"\n\n\n在按实际情况进行规划并填写相应信息后，以上就是一份：\n\n * 只进行 cc 扇区\n * 32GiB 和 64GiB 扇区免 tree_d\n * 一体化资源分配\n\n的最简配置文件了。\n\n# 参考 venus 社区用户测试案例\n\n参考案例 1 特点：PC1 精确限核，C2 采用 gpuproxy 方式完成，具有很强的可扩展性。缺点是配置复杂，需要根据实际环境调整任务数\n\n参考案例 2 特点：PC2 和 C2 共享 1 个 GPU，可能会产生一些 C2 任务将积压\n\n参考案例 3 特点：2 组 PC2 与分别与 2 组 C2 共享 GPU 资源\n\n参考案例 4 特点：适用于低配置机器使用，在 NVMe 上创建 96G 的 swap 空间，但这可能会导致某些任务做得比较慢',normalizedContent:'# damocles-worker 的配置解析\n\ndamocles-worker 是数据封装的执行主体，我们来了解一下它的配置文件结构和配置方式。\n\ndamocles-worker 的配置文件采用了 toml 格式，需要注意的是，这种格式中，以 # 开头的行将被视为注释，不会生效。\n\n以 mock 实例为例，一份基础的配置大概会是这样：\n\n[worker]\n# name = "worker-#1"\n# rpc_server.host = "192.168.1.100"\n# rpc_server.port = 17891\n\n[metrics]\n#enable = false\n#http_listen = "0.0.0.0:9000"\n\n[sector_manager]\nrpc_client.addr = "/ip4/127.0.0.1/tcp/1789"\n# rpc_client.headers = { user-agent = "jsonrpc-core-client" }\n# piece_token = "eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyjuyw1lijoims0xmjuilcjwzxjtijoic2lnbiisimv4dci6iij9.jenwgk0jzcxfdin3cyhbun41vxnvyw-_0uut2zoohm0"\n\n[sealing]\n# allowed_miners = [10123, 10124, 10125]\n# allowed_sizes = ["32gib", "64gib"]\nenable_deals = true\n# disable_cc = true\n# max_deals = 3\n# min_deal_space = "8gib"\nmax_retries = 3\n# seal_interval = "30s"\n# recover_interval = "60s"\n# rpc_polling_interval = "180s"\n# ignore_proof_check = false\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store1"\n# plan = "snapup"\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32gib", "64gib"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_deals = 3\n# sealing.min_deal_space = "8gib"\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store2"\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store3"\n\n[[attached]]\n# name = "persist-store1"\nlocation = "./mock-tmp/remote"\n\n[processors.limitation.concurrent]\n# add_pieces = 5\n# pc1 = 3\n# pc2 = 2\n# c2 = 1\n\n[processors.limitation.staggered]\n# pc1 = "5min"\n# pc2 = "4min"\n\n[processors.ext_locks]\n# gpu1 = 1\n\n[processors.static_tree_d]\n# 2kib = "./tmp/2k/sc-02-data-tree-d.dat"\n\n# fields for the add_pieces processor\n# [[processors.add_pieces]]\n\n# fields for tree_d processor\n[[processors.tree_d]]\n# auto_restart = true\n\n# fields for pc1 processors\n[[processors.pc1]]\n# bin = "./dist/bin/damocles-worker-plugin-pc1"\n# args = ["--args-1", "1", --"args-2", "2"]\nnuma_preferred = 0\ncgroup.cpuset = "4-5"\nenvs = { rust_log = "info" }\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "6-7"\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-13"\n# auto_restart = true\n# inherit_envs = true\n\n# fields for pc2 processors\n[[processors.pc2]]\n# cgroup.cpuset = "24-27"\n# auto_restart = true\n# inherit_envs = true\n\n[[processors.pc2]]\ncgroup.cpuset = "28-31"\n# auto_restart = true\n# inherit_envs = true\n\n# fields for c2 processor\n[[processors.c2]]\ncgroup.cpuset = "32-47"\n# auto_restart = true\n# inherit_envs = true\n\n\n下面我们将逐一分析其中的可配置项。\n\n\n# [worker]\n\nworker 配置项用于配置本实例的一些基础信息。\n\n\n# 基础配置范例\n\n[worker]\n# 实例名，选填项，字符串类型\n# 默认以连接 `damocles-manager` 所使用的网卡 ip 地址作为实例名\n# name = "worker-#1"\n\n# rpc 服务监听地址，选填项，字符串类型\n# 默认为 "0.0.0.0"，即监听本机所有地址\n# rpc_server.host = "192.168.1.100"\n\n# rpc 服务监听端口，选填项，数字类型\n# 默认为 17890\n# rpc_server.port = 17891\n\n# 本地 piece 文件目录, 选填项, 字符串数组类型\n# 如果设置了此项, worker 会从设置的目录中加载 piece 文件\n# 否则将会从 damocles-manager 加载远程 piece 文件\n# 如果 "/path/to/{your_local_pieces_dir01, your_local_pieces_dir02, ...}/piece_file_name" 文件不存在, worker 也会从 damocles-manager 加载\n# local_pieces_dirs = ["/path/to/your_local_pieces_dir01", "/path/to/your_local_pieces_dir02"]\n\n\n绝大多数情况下，本配置项内的各个字段无需手工配置。\n\n仅在一些特殊情况，诸如：\n\n * 希望按照自己的编排习惯命名每个 damocles-worker 实例\n * 不希望监听所有网卡 ip，仅允许本地的 rpc 请求\n * 一台机器上部署了多个 damocles-worker，为避免端口冲突，需要进行区分\n * damocles-worker 可以直接访问 piece_store 目录，可配置 local_pieces_dir 从本地加载 piece 文件\n\n等场景，需要按需手动配置这里的选项。\n\n\n# [metrics]\n\nmetrics 提供了监控指标 (prometheus) 相关的选项。\n\n\n# 基础配置范例\n\n[metrics]\n# 是否启用 prometheus exporter, 选填项，布尔类型\n# 默认值为 false\n# 当启用时，会监听 "http_listen" 所设置的 ip 和端口，提供 prometheus exporter\n#enable = false\n\n# prometheus exporter 监听的地址，选填项，地址类型\n# 默认值为 "0.0.0.0:9000"\n#http_listen = "0.0.0.0:9000"\n\n\n\n# [sector_manager]\n\nsector_manager 用于配置 damocles-manager 相关的信息，以使得 damocles-worker 可以正确的连接到对应的服务。\n\n\n# 基础配置范例\n\n[sector_manager]\n# 构造 rpc 客户端时使用的连接地址，必填项，字符串类型\n# 可以接受 `multiaddr` 格式，也可以接受诸如 `http://127.0.0.1:1789`，`ws://127.0.0.1:1789` 这样的 url 格式\n# 通常情况下，使用 `multiaddr` 格式以和其他组件保持一致\nrpc_client.addr = "/ip4/127.0.0.1/tcp/1789"\n\n# 构造 rpc 客户端时使用的 http 头信息，选填项，字典类型\n# 默认为 null\n# rpc_client.headers = { user-agent = "jsonrpc-core-client" }\n\n# 请求订单 piece 数据时携带的校验 token， 选填项，字符串类型\n# 默认为 null\n# 当本实例允许封装带有订单数据的扇区时，通常需要设置此项\n# 此项的值通常即为所使用的 venus 系列服务的 sophon-auth组件产生的 token 值\n# piece_token = "eyjhbgcioijiuzi1niisinr5cci6ikpxvcj9.eyjuyw1lijoims0xmjuilcjwzxjtijoic2lnbiisimv4dci6iij9.jenwgk0jzcxfdin3cyhbun41vxnvyw-_0uut2zoohm0"\n\n\n\n# [sealing]\n\nsealing 用于配置封装过程中的通用参数选项。\n\n\n# 基础配置范例\n\n[sealing]\n# 允许的`sp`，选填项，数字数组格式\n# 默认为 null，允许来自任何 `sp` 的任务\n# 配置后，仅可执行来自数组中罗列的 `sp` 的封装任务\n# allowed_miners = [10123, 10124, 10125]\n\n# 允许的扇区大小，选填项，字符串数组格式\n# 默认为 null， 允许任意大小的扇区任务\n# 配置后，仅可执行符合数组中罗列的扇区大小的任务\n# allowed_sizes = ["32gib", "64gib"]\n\n# 是否允许向扇区内添加订单，选填项，布尔类型\n# 默认为 false\n# 当设置为 true 时，通常需要同时设置 `sector_manager` 中的 `piece_token` 项\n# enable_deals = true\n\n# 是否禁用 cc 扇区，选填项，布尔类型\n# 默认为 false\n# enable_deals 为 true 时，开启此选项，将持续等待，直到获得分配的订单，而不是启动 cc 扇区\n# disable_cc = true\n\n# 允许向扇区内添加的最大订单数量，选填项，数字类型\n# 默认为 null\n# max_deals = 3\n\n# 一个扇区中填充的订单的最小体积，选填项，字节字符串格式\n# 默认为 null\n# min_deal_space = "8gib"\n\n# 封装过程中遇到 temp 类型的错误时，重试的次数，选填项，数字格式\n# 默认为 5\n# max_retries = 3\n\n# 封装过程中遇到 temp 类型的错误时，重试的间隔，选填项，时间字符串格式\n# 默认为 "30s"， 即30秒\n# recover_interval = "30s"\n\n# 空闲的 `sealing_thread` 申请封装任务的间隔， 选填项，时间字符串格式\n# 默认为 "30s"， 即30秒\n# seal_interval = "30s"\n\n# rpc 状态轮询请求的间隔，选填项，时间字符串格式\n# 默认为 "180s"， 即180秒\n# 封装过程中，部分环节使用了轮询方式来获取非实时的信息，如消息上链等。\n# 这个值有助于避免过于频繁的请求占用网络资源\n# rpc_polling_interval = "180s"\n\n# 是否跳过 proof 的本地校验环节，选填项，布尔格式\n# 默认为 false\n# 通常只在诸如测试之类的情况下设置此项\n# ignore_proof_check = false\n\n# 无法从 `damocles_manager` 获取任务时，重试的次数，选填项，数字类型\n# 默认为 3\n# request_task_max_retries = 3\n\n\nsealing 中的配置项通常有根据经验预设的默认项，这使得我们在绝大多数情况下无需自行配置。\n\n\n# 特殊配置范例\n\n# 1. 测试网络，仅为特定 sp 提供服务\n\nallowed_miners = [2234, 2236, 2238]\n\n\n# 2. 大规模集群，降低网络占用\n\n# 在可恢复的异常中，有相当一部分是网络抖动带来的，增大自动恢复的间隔时间降低请求频率\nrecover_interval = "90s"\n\n# 正常过程中的轮询请求也增大间隔时间降低请求频率\nrpc_polling_interval = "300s"\n\n\n# 3. 增大扇区异常自愈的可能性\n\n# 增大自动恢复的尝试次数\nmax_retries = 10\n\n# 增大自动恢复的间隔时间\nrecover_interval = "60s"\n\n\n\n# [[sealing_thread]]\n\nsealing_thread 用于为每个扇区工作线程进行配置。一份配置文件中可以存在多个 sealing_thread 配置组。\n\nsealing_thread 会继承 [sealing] 中的配置。可以使用 sealing.* 覆盖掉 [sealing] 的配置。\n\n\n# 基础配置范例\n\n[[sealing_thread]]\n# 扇区数据目录路径，必填项，字符串类型\n# 建议使用绝对路径，数据目录和工作线程是一对一绑定的\nlocation = "/mnt/nvme1/store"\n\n# 任务类型，选填项，字符串类型\n# 默认值为 null\n# 可选项: sealer | snapup | rebuild | unseal | wdpost, 当不填写时，默认等效为 sealer\n# plan = "snapup"\n\n# 封装过程的定制参数，仅对当前工作线程生效\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32gib", "64gib"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n# sealing.request_task_max_retries = 3\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\n\n\n\nsealing_thread 的数量和对应的数据路径需要根据规划情况编排。\n\n为了方便组合搭配，每个 sealing_thread 可以配置独立的 sealing 子项，它满足：\n\n * 可配置项的命名、类型、效果与通用的 sealing 项保持一致\n\n * 仅对当前工作线程生效\n\n * 未配置时使用通用的 sealing 项内的值\n\n\n# 特殊配置范例\n\n# 1. 两个工作线程，分别为不同的 sp 服务\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\nsealing.allowed_miners = [1357]\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\nsealing.allowed_miners = [2468]\n\n\n# 2. 两个工作线程，分别为不同的扇区大小服务\n\n[[sealing_thread]]\nlocation = "/mnt/nvme2/store"\nsealing.allowed_sizes = ["32gib"]\n\n\n[[sealing_thread]]\nlocation = "/mnt/nvme3/store"\nsealing.allowed_sizes = ["64gib"]\n\n\n\n# sealing_thread 配置热更新\n\n在 damocles v0.5.0 之前版本，我们只能通过修改 sealing_thread 配置后并重启 damocles-worker 完成配置更新。 在有些场景下很不方便，例如：扇区重建时我们希望能够在不重启 damocles-worker 的情况下修改指定的 sealing_thread 的 plan 配置项。\n\nv0.5.0 之后支持 sealing_thread 配置热更新。在指定的 sealing_thread 中的 location 目录下创建名为 config.toml 的热更新配置文件，该配置文件的内容与 [[sealing_thread]] 内容完全一致，此配置文件中的配置项会覆盖 damocles-worker 中对应的 [[sealing_thread]] 的配置项，并且修改此配置文件不需要重启 damocles-worker 即可生效。\n\ndamocles-worker 中的 sealing_thread 会在新的扇区任务开始之前检查 location 目录下的 config.toml 文件，如果 config.toml 文件的内容发生了变化或者此文件删除了都会重新加载或移除此文件的配置。\n\n注意：\n\n * 热更新配置文件 config.toml 无法覆盖 sealing_thread 中的 location 配置项。\n * damocles-worker 主配置文件不支持热更新。\n\n# 基础配置范例\n\n# /path/to/the_sealing_thread_location/config.toml\n\n# 任务类型，选填项，字符串类型\n# 默认值为 null\n# 可选项: sealer | snapup | rebuild | unseal | wdpost, 当不填写时，默认等效为 sealer\n# plan = "rebuild"\n\n# 封装过程的定制参数，仅对当前工作线程生效\n# sealing.allowed_miners = [10123, 10124, 10125]\n# sealing.allowed_sizes = ["32gib", "64gib"]\n# sealing.enable_deals = true\n# sealing.disable_cc = true\n# sealing.max_retries = 3\n# sealing.seal_interval = "30s"\n# sealing.recover_interval = "60s"\n# sealing.rpc_polling_interval = "180s"\n# sealing.ignore_proof_check = false\n# sealing.request_task_max_retries = 3\n\n\n\n# [[attached]]\n\nattached 用于配置已完成的扇区持久化数据保存的位置，允许同时配置多个。\n\n\n# 基础配置范例\n\n[[attached]]\n# 名称， 选填项，字符串类型\n# 默认为路径对应的绝对路径\n# name = "remote-store1"\n\n# 路径，必填项，字符串类型\n# 建议直接填写绝对路径\nlocation = "/mnt/remote/10.0.0.14/store"\n\n# 只读，选填项，布尔类型\n# 默认值为 false\n# readonly = true\n\n\n\n由于需要在 damocles-worker 和 damocles-manager 之间协调存储位置信息，而在很多情况下，同一个持久化存储目录在damocles-worker 机器和 damocles-manager 机器上的挂载路径不完全一致，因此我们决定使用 name 作为协调的基础信息。\n\n如果持久化存储目录在所有机器上的挂载路径都统一的话，配置时也可以选择在 damocles-worker 和damocles-manager 两侧都不配置 name。这种情况下，两者都会使用绝对路径作为 name，也能匹配。\n\n\n# [processors]\n\nprocessors 用于配置封装执行器，和封装计算过程中的一些信息。\n\n这个配置项实际上分为三个子项，我们逐一分析。\n\n\n# [processors.limitation.concurrent]\n\nprocessors.limitation.concurrent 用于配置指定封装阶段的并行任务数量控制。这是为了降低指定阶段的资源相互争抢的情况。\n\n需要注意的是，当配置了外部执行器时，外部执行器的数量和允许的并发总量也会影响并行任务数量。\n\n# 基础配置范例\n\n[processors.limitation.concurrent]\n# add_pieces 阶段的并发数限制，选填项，数字类型\n# add_pieces = 5\n\n# tree_d 阶段的并发数限制，选填项，数字类型\n# tree_d = 1\n\n# pc1 阶段的并发数限制，选填项，数字类型\n# pc1 = 3\n\n# pc2 阶段的并发数限制，选填项，数字类型\n# pc2 = 1\n\n# c2 阶段的并发数限制，选填项，数字类型\n# c2 = 1\n\n\n举例来说，如果设置了 pc2 = 2，那么同一时间最多只会有两个扇区可以执行 pc2 阶段的任务。\n\n\n# [processors.limitation.staggered]\n\nprocessors.limitation.staggered 用于配置指定封装阶段并行任务错开启动的时间间隔。配置此项后当指定阶段有多个任务同时启动时，damocles-worker 会依次根据配置的时间间隔启动任务，以避免任务同时启动造成磁盘 io 等资源紧张的问题。\n\n# 基础配置范例\n\n[processors.limitation.staggered]\n# 多个 pc1 任务依次启动的时间间隔，选填项，字符串类型 (e.g. "1s", "2min")\n# pc1 = "5min"\n# pc2 = "4min"\n\n\n举例来说，如果设置了 pc1 = "5min"，当两个 pc1 任务同时启动时，会先执行第一个任务 5 分钟后执行第二个任务。\n\n\n# [processors.ext_locks]\n\nprocessors.ext_locks 用于配置一些自定义的锁限制，它是和 [[processors.{stage_name}]] 中的 locks 配置项联动使用的。 这个配置项允许使用者自定一些限制条件，并令不同的外部处理器受其约束。\n\n# 基础配置范例\n\n[processors.ext_locks]\n# some_name = some_number\n\n\n# 特殊配置范例\n\nprocessors.ext_locks 自身是不能独立生效的。\n\n# 一块 gpu，pc2 和 c2 公用\n\n[processors.ext_locks]\ngpu = 1\n\n[[processors.pc2]]\nlocks = ["gpu"]\n\n[[processors.c2]]\nlocks = ["gpu"]\n\n\n这样，pc2 c2 会各启动一个外部处理器，两者将会产生竞争关系，也就意味着两者将不会同时发生。\n\n# 两块 gpu，pc2 和 c2 公用\n\n[processors.ext_locks]\ngpu1 = 1\ngpu2 = 1\n\n[[processors.pc2]]\nlocks = ["gpu1"]\n\n[[processors.pc2]]\nlocks = ["gpu2"]\n\n[[processors.c2]]\nlocks = ["gpu1"]\n\n[[processors.c2]]\nlocks = ["gpu2"]\n\n\n这样，pc2 c2 会各启动两个外部处理器，将会产生两两竞争的关系，从而允许限制一块 gpu 上只能执行其中一个阶段的任务。\n\n\n# [processors.static_tree_d]\n\nprocessors.static_tree_d 是为了提升 cc 扇区 的效率而引入的配置项。\n\n当为相应扇区大小配置了静态文件路径时，将会直接使用此文件作为 cc 扇区 的 tree_d 文件，而不会尝试再次生成。\n\n# 基础配置范例\n\n[processors.static_tree_d]\n2kib = "/var/tmp/2k/sc-02-data-tree-d.dat"\n32gib = "/var/tmp/32g/sc-02-data-tree-d.dat"\n64gib = "/var/tmp/64g/sc-02-data-tree-d.dat"\n\n\n\n\n# [[processors.{stage_name}]]\n\n这是用于配置外部执行器的配置组。\n\n目前 {stage_name} 可选\n\n * add_pieces 用于 add pieces 阶段\n * tree_d 用于 tree d 的生成阶段\n * pc1 用于 precommit1 阶段\n * pc2 用于 precommit2 阶段\n * synth_proof 用于生成 synthetic proof 阶段\n * c2：用于 commit2 阶段\n * transfer：用于自定义本地数据和持久化数据存储之间的传输方式\n * unseal: 用于 unseal 阶段\n\n每一个这样的配置组意味着将启动一个对应阶段的外部执行器。如果没有为上述的某个 {stage_name} 配置组配置任何内容，且配置中不存在对应的 [[processors.{stage_name}]] 这一行配置， 则 damocles-worker 不会为此 {stage_name} 启动子进程，damocles-worker 会使用内建的执行器代码在 sealing_thread 中直接执行对应的 {stage_name} 任务。这样 {stage_name} 任务的并发数量取决于对应的 sealing_thread 数量 和 [processors.limitation.concurrent] 中配置的 {stage_name} 并发数量。不配置外部执行器省去了序列化任务参数和任务输出等额外步骤，但是失去了更强大的并发控制，cgroup 控制和自定义算法等能力。可以根据使用场景自行取舍。\n\n[[processors.{stage_name}]] 可选的配置项包含：\n\n[[processors.pc1]]\n# 自定义外部执行器可执行文件路径，选填项，字符串类型\n# 默认会使用主进程对应的可执行文件路径，执行 damocles-worker 内建的执行器\n# bin = "./dist/bin/damocles-worker-plugin-pc1"\n\n# 自定义外部执行器的参数，选填项，字符串数组类型\n# 默认值为 null，将使用 `damocles-worker` 自己的执行器默认参数\n# args = ["--args-1", "1", --"args-2", "2"]\n\n# 外部执行器子进程准备就绪的时间，选填项，时间类型\n# 默认为 5s\n# stable_wait = "5s"\n\n# numa 亲和性分区 id，选填项，数字类型\n# 默认值为 null，不会设置亲和性\n# 需要根据宿主机的 numa 分区进行填写\n# numa_preferred = 0\n\n# cpu 核绑定和限制选项，选填项，字符串类型\n# 默认值为 null，不设置绑定\n# 值的格式遵循标准 cgroup.cpuset 格式\n# cgroup.cpuset = "4-5"\n\n# 外部执行器的附加环境变量，选填项，字典类型\n# 默认值为 null\n# envs = { rust_log = "info" }\n\n# 本执行器允许的并发任务数量上限\n# 默认值为 null，无限制，但任务具体是否并发执行，视使用的外部执行器实现而定\n# 主要使用在 pc1 这样可以多个并行的环节，可以有效节约共享内存、线程池等资源\n# concurrent = 4\n\n# 自定义的外部限制锁名称，选填项，字符串数组类型\n# 默认值为 null\n# locks = ["gpu1"]\n\n# 当子进程退出时，是否自动重启，选填项，布尔类型\n# 默认值为 true\n# auto_restart = true\n\n# 是否继承 worker 守护进程的环境变量，选填项，布尔类型\n# 默认值为 true\ninherit_envs = true\n\n\n# 基础配置范例\n\n[processors.limitation.concurrent]\nadd_pieces = 8\npc1 = 4\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-7"\nconcurrent = 2\nenvs = { fil_proofs_use_multicore_sdr = "1" }\nauto_restart = true\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-19"\nconcurrent = 2\nenvs = { fil_proofs_use_multicore_sdr = "1" }\nauto_restart = true\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0" }\nauto_restart = true\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-39"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "1" }\nauto_restart = true\n\n[[processors.c2]]\ncgroup.cpuset = "28-35"\nenvs = { cuda_visible_devices = "2,3" }\nauto_restart = true\n\n\n[[processors.tree_d]]\ncgroup.cpuset = "40-45"\nauto_restart = true\n\n\n以上是基于一台 48c + 4gpu 的设备的 processors.{stage_name} 配置范例，在这套配置下，将启动：\n\n * 2 个 pc1 外部执行器，采用 multicore_sdr 模式，各分配 8 核，允许 2 个并发任务，且内存分配优先使用本 numa 分区\n * 2 个 pc2 外部执行器，各分配 8 核，各使用一块 gpu\n * 1 个 c2 外部执行器，分配 8 核，使用一块 gpu\n * 1 个 tree_d 外部执行器，分配 6 核\n\n# 特殊配置范例\n\n# 1. 使用 patch 了闭源的、经过算法优化的 c2 外部执行器\n\n[[processors.c2]]\nbin = "/usr/local/bin/damocles-worker-c2-optimized"\ncgroup.cpuset = "40-47"\nenvs = { cuda_visible_devices = "2,3" }\n\n\n# 2. 使用外包模式的 c2 外部执行器\n\n[[processors.c2]]\nbin = "/usr/local/bin/damocles-worker-c2-outsource"\nargs = ["--url", "/ip4/apis.filecoin.io/tcp/10086/https", "--timeout", "10s"]\nenvs = { licence_path = "/var/tmp/c2.licence.dev" }\n\n\n# 3. gpu 不足的情况下使用 cpu 模式弥补 pc2 计算能力\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-45"\n\n\n# 4. 最优配比下，pc1 总量为奇数，无法平分\n\n[processors.limitation.concurrent]\npc1 = 29\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-41"\nconcurrent = 14\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-92"\nconcurrent = 15\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n\n\n# 5. 希望优先集中使用 numa 0 区完成 pc1\n\n[processors.limitation.concurrent]\npc1 = 29\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-47"\nconcurrent = 16\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-86"\nconcurrent = 13\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n\n\n# cgroup.cpuset 配置注意事项\n\n * 针对启用 multicore sdr 的 pc1 外部处理器，cgroup.cpuset 尽量以整个 l3 cache 下的 cpu cores 为单位配置。如果需要配置 l3 cache 下的部分 cpu cores 必须保证每个 l3 cache 下 cpu cores 数量一致。\n   \n   rust-fil-proofs 中规定，当启用 multicore sdr 时，cpu 核心数量和这些 cpu 核心对应的 cpu 共享缓存 (通常是 l3 cache) 的数量必须是整数倍数关系。如果外部执行器使用的是 rust-fil-proofs 或者基于 rust-fil-proofs 开发，则必须遵守这个规则，否则可以忽略。damocles-worker 默认的执行器是基于 rust-fil-proofs 开发的。\n\n * cgroup.cpuset 中的 cpu cores 尽量不要跨 numa 节点，跨 numa 节点会使得 cpu 访问内存速度变慢 (damocles-cluster v0.5.0 之后，支持加载 numa 亲和的 hugepage 内存文件，如果启用该功能可以跨 numa 节点分配 cpuset 不会产生影响)\n   \n   如果配置的 cpu cores 均在同一 numa 节点，可以将 processors.{stage_name}.numa_preferred 配置为对应的 numa 节点 id。\n\n使用 damocles-worker-util 查看 cpu 信息\n\n./damocles-worker-util hwinfo\n\n\noutput:\n\ncpu topology:\nmachine (503.55 gib)\n├── package (251.57 gib) (*** *** *** 32-core processor)\n│   ├── numanode (#0 251.57 gib)\n│   ├── l3 (#0 16 mib)\n│   │   └── pu #0 + pu #1 + pu #2 + pu #3\n│   ├── l3 (#1 16 mib)\n│   │   └── pu #4 + pu #5 + pu #6 + pu #7\n│   ├── l3 (#2 16 mib)\n│   │   └── pu #8 + pu #9 + pu #10 + pu #11\n│   ├── l3 (#3 16 mib)                       \n│   │   └── pu #12 + pu #13 + pu #14 + pu #15\n│   ├── l3 (#4 16 mib)                       \n│   │   └── pu #16 + pu #17 + pu #18 + pu #19\n│   ├── l3 (#5 16 mib)\n│   │   └── pu #20 + pu #21 + pu #22 + pu #23\n│   ├── l3 (#6 16 mib)\n│   │   └── pu #24 + pu #25 + pu #26 + pu #27\n│   └── l3 (#7 16 mib)\n│       └── pu #28 + pu #29 + pu #30 + pu #31\n└── package (251.98 gib) (*** *** *** 32-core processor)\n    ├── numanode (#1 251.98 gib)\n    ├── l3 (#8 16 mib)\n    │   └── pu #32 + pu #33 + pu #34 + pu #35\n    ├── l3 (#9 16 mib)\n    │   └── pu #36 + pu #37 + pu #38 + pu #39\n    ├── l3 (#10 16 mib)\n    │   └── pu #40 + pu #41 + pu #42 + pu #43\n    ├── l3 (#11 16 mib)\n    │   └── pu #44 + pu #45 + pu #46 + pu #47\n    ├── l3 (#12 16 mib)\n    │   └── pu #48 + pu #49 + pu #50 + pu #51\n    ├── l3 (#13 16 mib)\n    │   └── pu #52 + pu #53 + pu #54 + pu #55\n    ├── l3 (#14 16 mib)\n    │   └── pu #56 + pu #57 + pu #58 + pu #59\n    └── l3 (#15 16 mib)\n        └── pu #60 + pu #61 + pu #62 + pu #63\n\n...\n\n\n从输出的信息可以看到这台机器有两个 numanode, 每个 numanode 上有 8 个 l3 cache，每个 l3 cache 下有 4 个 cpu 核。\n\n * numanode #0 上的 cpu cores: 0-31\n * numanode #1 上的 cpu cores: 31-63\n\n以这台机器为例，如果配置 processors.pc1.cgroup.cpuset = "0-6" 是不满足 rust-fil-proofs 规则的。\n\n 1. pu#0, pu#1, pu#2, pu#3, 这 4 个 cpu cores 属于 l3#0\n 2. pu#4, pu#5, pu#6, 这 3 个 cpu cores 属于 l3#1\n\n此时 cpu 的共享缓存数量为 2 (l3#0, l3#1), 并且配置的 cpu cores 数量不一致，不满足 rust-fil-proofs 规则，无法启动。正确的配置可以是 processors.pc1.cgroup.cpuset = "0-7"。\n\n\n# 一份最简可工作的配置文件范例\n\n[sector_manager]\nrpc_client.addr = "/ip4/{some_ip}/tcp/1789"\n\n# 根据实际资源规划\n[[sealing_thread]]\nlocation = "{path to sealing store1}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store2}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store3}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store4}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store5}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store6}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store7}"\n\n[[sealing_thread]]\nlocation = "{path to sealing store8}"\n\n\n[remote_store]\nname = "{remote store name}"\nlocation = "{path to remote store}"\n\n[processors.static_tree_d]\n32gib = "{path to static tree_d for 32gib}"\n64gib = "{path to static tree_d for 64gib}"\n\n# 根据实际资源规划\n[processors.limitation.concurrent]\npc1 = 4\npc2 = 2\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-7"\nconcurrent = 2\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "12-19"\nconcurrent = 2\nenvs = { fil_proofs_use_multicore_sdr = "1" }\n\n\n[[processors.pc2]]\ncgroup.cpuset = "8-11,24-27"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset = "20-23,36-39"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "1" }\n\n\n[[processors.c2]]\ncgroup.cpuset = "28-35"\nenvs = { cuda_visible_devices = "2,3" }\n\n\n[[processors.tree_d]]\ncgroup.cpuset = "40-45"\n\n\n在按实际情况进行规划并填写相应信息后，以上就是一份：\n\n * 只进行 cc 扇区\n * 32gib 和 64gib 扇区免 tree_d\n * 一体化资源分配\n\n的最简配置文件了。\n\n# 参考 venus 社区用户测试案例\n\n参考案例 1 特点：pc1 精确限核，c2 采用 gpuproxy 方式完成，具有很强的可扩展性。缺点是配置复杂，需要根据实际环境调整任务数\n\n参考案例 2 特点：pc2 和 c2 共享 1 个 gpu，可能会产生一些 c2 任务将积压\n\n参考案例 3 特点：2 组 pc2 与分别与 2 组 c2 共享 gpu 资源\n\n参考案例 4 特点：适用于低配置机器使用，在 nvme 上创建 96g 的 swap 空间，但这可能会导致某些任务做得比较慢',charsets:{cjk:!0}},{title:"damocles-cluster Q&A",frontmatter:{},regularPath:"/zh/operation/faq.html",relativePath:"zh/operation/faq.md",key:"v-104d39de",path:"/zh/operation/faq.html",headers:[{level:2,title:"Q: load state: key not found 是什么异常？是密钥配置问题么？",slug:"q-load-state-key-not-found-是什么异常-是密钥配置问题么",normalizedTitle:"q: load state: key not found 是什么异常？是密钥配置问题么？",charIndex:27},{level:2,title:"Q: 编译出的 damocles-worker 可执行文件特别大，是什么原因？",slug:"q-编译出的-damocles-worker-可执行文件特别大-是什么原因",normalizedTitle:"q: 编译出的 damocles-worker 可执行文件特别大，是什么原因？",charIndex:558},{level:2,title:"Q:  Once instance has previously been poisoned 是什么错误？如何处理？",slug:"q-once-instance-has-previously-been-poisoned-是什么错误-如何处理",normalizedTitle:"q:  once instance has previously been poisoned 是什么错误？如何处理？",charIndex:null},{level:2,title:"Q: Too many open files (os error 24) 是什么错误？如何处理？",slug:"q-too-many-open-files-os-error-24-是什么错误-如何处理",normalizedTitle:"q: too many open files (os error 24) 是什么错误？如何处理？",charIndex:1797},{level:2,title:"Q：为什么我期望的结果是两张 GPU 都被使用，但是实际情况有一张 GPU 始终空闲？",slug:"q-为什么我期望的结果是两张-gpu-都被使用-但是实际情况有一张-gpu-始终空闲",normalizedTitle:"q：为什么我期望的结果是两张 gpu 都被使用，但是实际情况有一张 gpu 始终空闲？",charIndex:2134},{level:2,title:"Q：memory map must have a non-zero length 是什么错误？如何处理？",slug:"q-memory-map-must-have-a-non-zero-length-是什么错误-如何处理",normalizedTitle:"q：memory map must have a non-zero length 是什么错误？如何处理？",charIndex:4041},{level:2,title:"Q: vc_processors::core::ext::producer: failed to unmarshal response string 是什么错误？ 如何处理？",slug:"q-vc-processors-core-ext-producer-failed-to-unmarshal-response-string-是什么错误-如何处理",normalizedTitle:"q: vc_processors::core::ext::producer: failed to unmarshal response string 是什么错误？ 如何处理？",charIndex:4313},{level:2,title:"Q:",slug:"q",normalizedTitle:"q:",charIndex:27}],headersStr:"Q: load state: key not found 是什么异常？是密钥配置问题么？ Q: 编译出的 damocles-worker 可执行文件特别大，是什么原因？ Q:  Once instance has previously been poisoned 是什么错误？如何处理？ Q: Too many open files (os error 24) 是什么错误？如何处理？ Q：为什么我期望的结果是两张 GPU 都被使用，但是实际情况有一张 GPU 始终空闲？ Q：memory map must have a non-zero length 是什么错误？如何处理？ Q: vc_processors::core::ext::producer: failed to unmarshal response string 是什么错误？ 如何处理？ Q:",content:'# damocles-cluster Q&A\n\n\n# Q: load state: key not found 是什么异常？是密钥配置问题么？\n\nA: load state: key not found 发生在扇区密封或升级过程中，是由于扇区对应的状态记录未找到导致的。\n\n这里的 key not found 异常由底层的组件传导上来，其中的 key 是指 kv 数据库中的键。\n\n这种异常通常发生在以下场景：\n\n 1. 已经在 damocles-manager 一侧通过类似 util sealer sectors abort 这样的命令终止了某个扇区，而对应的 damocles-worker 仍在继续执行这个扇区的任务；\n 2. damocles-manager 更换了 home 目录，导致无法读取之前的扇区记录数据；\n 3. damocles-worker 连接到了错误的 damocles-manager 实例；\n\n对于 1)，可以先通过 util sealer sectors list 命令观察已终止的扇区列表，确认是否存在问题扇区对应的记录，如果存在的话，再通过 util sealer sectors restore 命令进行恢复。\n\n对于其他情况，需要按照实际情况更正配置或连接信息。\n\n\n# Q: 编译出的 damocles-worker 可执行文件特别大，是什么原因？\n\nA: 这里的特别大，通常是指可执行文件的体积达到上 G。正常来说，damocles-worker 可执行文件的体积在数十 M 这个量级。上 G 的文件肯定已经超出了正常的范畴。\n\n这种情况通常是编译过程中意外启用了 debug 信息导致的，通常有几种可能性：\n\n 1. 在各层级的 cargo config 文件 中设置了 [profile.<name>.debug]；\n 2. 在编译指令中引入了启用 debug 信息的参数，这种参数可能出现在以下位置：\n    * 环境变量：以 RUSTFLAG 为代表的各类 XXXFLAG 环境变量\n    * 编译器参数：以 rustc 的 -g 参数为代表的各类参数\n    * 编译配置项：在各层级的 cargo config 文件中存在的、以 rustflags 为代表的各类配置项\n\n关于这个问题，我们注意到，在 lotus 的官方文档 INSTALL-linux 中提到的环境变量建议：\n\nexport RUSTFLAGS="-C target-cpu=native -g"\nexport FFI_BUILD_FROM_SOURCE=1\n\n\n其中，-C target-cpu=native 的作用是针对本机 CPU 进行优化，而 -g 的作用就是启用 debug 信息。\n\n如果用户按照 lotus 的经验，可能就会发现可执行文件体积特别大的情况。针对这种情况，我们推荐使用者仅配置\n\nexport RUSTFLAGS="-C target-cpu=native"\n\n\n感谢来自社区的 caijian76 提供反馈和线索。\n\n\n# Q: Once instance has previously been poisoned 是什么错误？如何处理？\n\nA: 从最基本的原理来说，Once 是一类为了确保线程安全而产生的编程基础类型，它通常会使用到操作系统层面的锁等底层实现，被用于非常多的场合和类库中。\n\n出现 Once instance has previously been poisoned 这类异常是出现在系统调用中，它的原因可能有很多种。这里提出的只是其中一种已经被验证了的情况，描述如下：\n\n * 当需要为外部处理器限核时，我们会使用 cgroup.cpuset 配置项\n * 当外部处理器的工作内容是内存敏感类型，如 pc1 时，我们通常还会启用内存亲和性即 numa_preferred 配置项\n * 当上述配置同时启用，而 cgroup.cpuset 中的 cpu 核心不符合 numa_preferred 指定的物理分区时，有较高的可能性出现此类异常\n\n当然，上面这种情况也许只是众多可能性中的一种。我们会在发现其他情况之后补充到这里来。\n\n感谢来自社区的 steven 提供反馈，Dennis Zou 提供解答。\n\n\n# Q: Too many open files (os error 24) 是什么错误？如何处理？\n\nA：Too many open files (os error 24) 通常出现在 linux 系统中，表明当前进程开启了太多的文件句柄。\n\n这种情况在 WindowPoSt 过程中较为常见，原因是每个 WindowPoSt 任务都有可能要读取大量的扇区文件。\n\n通常来说，这种问题可以通过改变提高文件句柄限制来解决。但是我们很难确定一个具体的上限值。\n\n因此，在大多数场景中，我们直接将文件句柄限制设置为无限来规避这种问题。\n\n具体操作方式可以参考 [Tutorial] Permanently Setting Your ULIMIT System Value。\n\n\n# Q：为什么我期望的结果是两张 GPU 都被使用，但是实际情况有一张 GPU 始终空闲？\n\nA：发生这种问题的原因有很多，但是一般来说，刚接触 damocles-cluster 的用户在进行资源编排的时候误理解和配置了一些参数导致这种结果的情况比较多。换句话说，这通常是由对硬件和调度配置的误解 导致的。\n\n这里我们从原理说起。\n\n一般来说，配置一个外部处理器的时候，我们需要考虑几个方面的实际情况：\n\n 1. 这个外部处理器能使用哪些硬件资源\n    \n    这一块主要受诸如 cgroup.cpuset 、numa_preferred 以及环境变量 CUDA_VISIBLE_DEVICE 等影响。\n    \n    换句话说，这些主要是针对这一个外部处理器子进程的、系统或启动级别的设置。\n\n 2. 使用这个外部处理器时的调度原则\n    \n    1. 这个外部处理器自身的处理能力设定\n       \n       这一块目前主要是 concurrent 配置项\n    \n    2. 这个外部处理器和其他外部处理器的协调情况\n       \n       这种情况相对复杂，还可以再细分为：\n       \n       * 和其他相同阶段的外部处理器协调\n         \n         如 processors.limitation.concurrent、processors.limitation.staggered\n       \n       * 和其他不同阶段的外部处理器协调\n         \n         如 processors.ext_locks\n\n以题目中的、在一台双 GPU 的机器上实行的这样一份配置为例：\n\n[processors.ext_locks]\ngpu1 = 1\ngpu2 = 2\n\n\n[[processors.pc2]]\ncgroup.cpuset =  "2,5,8,11,14,17,20,23"\nlocks = ["gpu1"]\nenvs = { CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "50,53,56,59,62,65,68,71"\nlocks = ["gpu2"]\nenvs = { CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.c2]]\nbin = /usr/local/bin/gpuproxy\nargs = ["args1", "args2", "args3"]\n\n\n其中存在这样几点常见误解：\n\n 1. 关于 ext_locks 的配置格式，它是一个完全由用户自行定义的锁，其格式为 <锁名> = <同时持锁的外部处理器数量>。\n    \n    在这个场景下，gpu2 = 2 很有可能来自于对数字含义的误解。\n\n 2. ext_locks 通常用在 不同阶段的处理器需要独占使用同一个硬件 的场景，比较常见的是 pc2 和 c2 共用一块 GPU。\n    \n    在这个范例中，c2 使用了代理 GPU 的方案，不使用本地 GPU，因此 pc2 的 ext_locks 设置并无效果。\n\n 3. 两个 pc2 的外部处理器都设定了 CUDA_VISIBLE_DEVICE = "0"，这是第二块 GPU 空闲的根本原因，即两个 pc2 外部处理器都只能看到序号为 0 的 GPU，也就是说他们始终在使用同一块 GPU。\n    \n    CUDA_VISIBLE_DEVICE 是 nvidia 官方驱动中提供的一个环境变量，它的解读可以参考 CUDA Pro 技巧：使用 CUDA_VISIBLE_DEVICES 控制 GPU 的可见性\n\n那么经过修正后的配置应当为：\n\n[[processors.pc2]]\ncgroup.cpuset =  "2,5,8,11,14,17,20,23"\nenvs = { CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "50,53,56,59,62,65,68,71"\nenvs = { CUDA_VISIBLE_DEVICES = "1" }\n\n[[processors.c2]]\nbin = /usr/local/bin/gpuproxy\nargs = ["args1", "args2", "args3"]\n\n\n感谢社区的 steven 提供案例。\n\n\n# Q：memory map must have a non-zero length 是什么错误？如何处理？\n\nA：这种报错信息表达的是，是使用 mmap 的过程中，目标文件大小为 0。\n\n根据经验，在目前 Filecoin 的各类算法中， mmap 的使用主要出现在各类静态文件的读取，如：\n\n * vk 和 params 文件\n * 各类事先生成的 cache 文件，如 parent cache 等\n\n那么出现这种错误，通常表示目标文件不存在，或为空文件。\n\n例如，在 WindowPoSt 场景，如果没有准备好参数文件或密钥文件。\n\n\n# Q: vc_processors::core::ext::producer: failed to unmarshal response string 是什么错误？ 如何处理？\n\nA：发生这种错误是因为外部处理器产生了错误格式的响应。可以通过 damocles-worker 提供的命令运行时的开启或关闭 dump 功能，查看错误格式的响应。\n\n开启 dump\n\ndamocles-worker worker --config="path/to/your_config_file.toml" enable_dump --child_pid=<target_ext_processor_pid> --dump_dir="path/to/dump_dir"\n\n\n关闭 dump\n\ndamocles-worker worker --config="path/to/your_config_file.toml" disable_dump --child_pid=<target_ext_processor_pid>\n\n\n可根据 dump 文件进行 debug。\n\n\n# Q:\n\n...storage_proofs_porep::stacked::vanilla::create_label::multi: create labels\nthread \'<unnamed>\' panicked at \'assertion failed: `(left == right)`\n  left: `0`,\n right: `2`, .../storage-proofs-porep-11.1.1/src/stacked/vanilla/cores.rs:151:5\n\n\n是什么错误？如何处理？\n\nA: 这个错误大概率是 damocles-worker 中 PC1 的 cgroup.cpuset 配置问题引发的。参考：cgroup.cpuset 配置注意事项',normalizedContent:'# damocles-cluster q&a\n\n\n# q: load state: key not found 是什么异常？是密钥配置问题么？\n\na: load state: key not found 发生在扇区密封或升级过程中，是由于扇区对应的状态记录未找到导致的。\n\n这里的 key not found 异常由底层的组件传导上来，其中的 key 是指 kv 数据库中的键。\n\n这种异常通常发生在以下场景：\n\n 1. 已经在 damocles-manager 一侧通过类似 util sealer sectors abort 这样的命令终止了某个扇区，而对应的 damocles-worker 仍在继续执行这个扇区的任务；\n 2. damocles-manager 更换了 home 目录，导致无法读取之前的扇区记录数据；\n 3. damocles-worker 连接到了错误的 damocles-manager 实例；\n\n对于 1)，可以先通过 util sealer sectors list 命令观察已终止的扇区列表，确认是否存在问题扇区对应的记录，如果存在的话，再通过 util sealer sectors restore 命令进行恢复。\n\n对于其他情况，需要按照实际情况更正配置或连接信息。\n\n\n# q: 编译出的 damocles-worker 可执行文件特别大，是什么原因？\n\na: 这里的特别大，通常是指可执行文件的体积达到上 g。正常来说，damocles-worker 可执行文件的体积在数十 m 这个量级。上 g 的文件肯定已经超出了正常的范畴。\n\n这种情况通常是编译过程中意外启用了 debug 信息导致的，通常有几种可能性：\n\n 1. 在各层级的 cargo config 文件 中设置了 [profile.<name>.debug]；\n 2. 在编译指令中引入了启用 debug 信息的参数，这种参数可能出现在以下位置：\n    * 环境变量：以 rustflag 为代表的各类 xxxflag 环境变量\n    * 编译器参数：以 rustc 的 -g 参数为代表的各类参数\n    * 编译配置项：在各层级的 cargo config 文件中存在的、以 rustflags 为代表的各类配置项\n\n关于这个问题，我们注意到，在 lotus 的官方文档 install-linux 中提到的环境变量建议：\n\nexport rustflags="-c target-cpu=native -g"\nexport ffi_build_from_source=1\n\n\n其中，-c target-cpu=native 的作用是针对本机 cpu 进行优化，而 -g 的作用就是启用 debug 信息。\n\n如果用户按照 lotus 的经验，可能就会发现可执行文件体积特别大的情况。针对这种情况，我们推荐使用者仅配置\n\nexport rustflags="-c target-cpu=native"\n\n\n感谢来自社区的 caijian76 提供反馈和线索。\n\n\n# q: once instance has previously been poisoned 是什么错误？如何处理？\n\na: 从最基本的原理来说，once 是一类为了确保线程安全而产生的编程基础类型，它通常会使用到操作系统层面的锁等底层实现，被用于非常多的场合和类库中。\n\n出现 once instance has previously been poisoned 这类异常是出现在系统调用中，它的原因可能有很多种。这里提出的只是其中一种已经被验证了的情况，描述如下：\n\n * 当需要为外部处理器限核时，我们会使用 cgroup.cpuset 配置项\n * 当外部处理器的工作内容是内存敏感类型，如 pc1 时，我们通常还会启用内存亲和性即 numa_preferred 配置项\n * 当上述配置同时启用，而 cgroup.cpuset 中的 cpu 核心不符合 numa_preferred 指定的物理分区时，有较高的可能性出现此类异常\n\n当然，上面这种情况也许只是众多可能性中的一种。我们会在发现其他情况之后补充到这里来。\n\n感谢来自社区的 steven 提供反馈，dennis zou 提供解答。\n\n\n# q: too many open files (os error 24) 是什么错误？如何处理？\n\na：too many open files (os error 24) 通常出现在 linux 系统中，表明当前进程开启了太多的文件句柄。\n\n这种情况在 windowpost 过程中较为常见，原因是每个 windowpost 任务都有可能要读取大量的扇区文件。\n\n通常来说，这种问题可以通过改变提高文件句柄限制来解决。但是我们很难确定一个具体的上限值。\n\n因此，在大多数场景中，我们直接将文件句柄限制设置为无限来规避这种问题。\n\n具体操作方式可以参考 [tutorial] permanently setting your ulimit system value。\n\n\n# q：为什么我期望的结果是两张 gpu 都被使用，但是实际情况有一张 gpu 始终空闲？\n\na：发生这种问题的原因有很多，但是一般来说，刚接触 damocles-cluster 的用户在进行资源编排的时候误理解和配置了一些参数导致这种结果的情况比较多。换句话说，这通常是由对硬件和调度配置的误解 导致的。\n\n这里我们从原理说起。\n\n一般来说，配置一个外部处理器的时候，我们需要考虑几个方面的实际情况：\n\n 1. 这个外部处理器能使用哪些硬件资源\n    \n    这一块主要受诸如 cgroup.cpuset 、numa_preferred 以及环境变量 cuda_visible_device 等影响。\n    \n    换句话说，这些主要是针对这一个外部处理器子进程的、系统或启动级别的设置。\n\n 2. 使用这个外部处理器时的调度原则\n    \n    1. 这个外部处理器自身的处理能力设定\n       \n       这一块目前主要是 concurrent 配置项\n    \n    2. 这个外部处理器和其他外部处理器的协调情况\n       \n       这种情况相对复杂，还可以再细分为：\n       \n       * 和其他相同阶段的外部处理器协调\n         \n         如 processors.limitation.concurrent、processors.limitation.staggered\n       \n       * 和其他不同阶段的外部处理器协调\n         \n         如 processors.ext_locks\n\n以题目中的、在一台双 gpu 的机器上实行的这样一份配置为例：\n\n[processors.ext_locks]\ngpu1 = 1\ngpu2 = 2\n\n\n[[processors.pc2]]\ncgroup.cpuset =  "2,5,8,11,14,17,20,23"\nlocks = ["gpu1"]\nenvs = { cuda_visible_devices = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "50,53,56,59,62,65,68,71"\nlocks = ["gpu2"]\nenvs = { cuda_visible_devices = "0" }\n\n[[processors.c2]]\nbin = /usr/local/bin/gpuproxy\nargs = ["args1", "args2", "args3"]\n\n\n其中存在这样几点常见误解：\n\n 1. 关于 ext_locks 的配置格式，它是一个完全由用户自行定义的锁，其格式为 <锁名> = <同时持锁的外部处理器数量>。\n    \n    在这个场景下，gpu2 = 2 很有可能来自于对数字含义的误解。\n\n 2. ext_locks 通常用在 不同阶段的处理器需要独占使用同一个硬件 的场景，比较常见的是 pc2 和 c2 共用一块 gpu。\n    \n    在这个范例中，c2 使用了代理 gpu 的方案，不使用本地 gpu，因此 pc2 的 ext_locks 设置并无效果。\n\n 3. 两个 pc2 的外部处理器都设定了 cuda_visible_device = "0"，这是第二块 gpu 空闲的根本原因，即两个 pc2 外部处理器都只能看到序号为 0 的 gpu，也就是说他们始终在使用同一块 gpu。\n    \n    cuda_visible_device 是 nvidia 官方驱动中提供的一个环境变量，它的解读可以参考 cuda pro 技巧：使用 cuda_visible_devices 控制 gpu 的可见性\n\n那么经过修正后的配置应当为：\n\n[[processors.pc2]]\ncgroup.cpuset =  "2,5,8,11,14,17,20,23"\nenvs = { cuda_visible_devices = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "50,53,56,59,62,65,68,71"\nenvs = { cuda_visible_devices = "1" }\n\n[[processors.c2]]\nbin = /usr/local/bin/gpuproxy\nargs = ["args1", "args2", "args3"]\n\n\n感谢社区的 steven 提供案例。\n\n\n# q：memory map must have a non-zero length 是什么错误？如何处理？\n\na：这种报错信息表达的是，是使用 mmap 的过程中，目标文件大小为 0。\n\n根据经验，在目前 filecoin 的各类算法中， mmap 的使用主要出现在各类静态文件的读取，如：\n\n * vk 和 params 文件\n * 各类事先生成的 cache 文件，如 parent cache 等\n\n那么出现这种错误，通常表示目标文件不存在，或为空文件。\n\n例如，在 windowpost 场景，如果没有准备好参数文件或密钥文件。\n\n\n# q: vc_processors::core::ext::producer: failed to unmarshal response string 是什么错误？ 如何处理？\n\na：发生这种错误是因为外部处理器产生了错误格式的响应。可以通过 damocles-worker 提供的命令运行时的开启或关闭 dump 功能，查看错误格式的响应。\n\n开启 dump\n\ndamocles-worker worker --config="path/to/your_config_file.toml" enable_dump --child_pid=<target_ext_processor_pid> --dump_dir="path/to/dump_dir"\n\n\n关闭 dump\n\ndamocles-worker worker --config="path/to/your_config_file.toml" disable_dump --child_pid=<target_ext_processor_pid>\n\n\n可根据 dump 文件进行 debug。\n\n\n# q:\n\n...storage_proofs_porep::stacked::vanilla::create_label::multi: create labels\nthread \'<unnamed>\' panicked at \'assertion failed: `(left == right)`\n  left: `0`,\n right: `2`, .../storage-proofs-porep-11.1.1/src/stacked/vanilla/cores.rs:151:5\n\n\n是什么错误？如何处理？\n\na: 这个错误大概率是 damocles-worker 中 pc1 的 cgroup.cpuset 配置问题引发的。参考：cgroup.cpuset 配置注意事项',charsets:{cjk:!0}},{title:"damocles-worker PC1 HugeTLB Pages 支持",frontmatter:{},regularPath:"/zh/operation/hugeTLB.html",relativePath:"zh/operation/hugeTLB.md",key:"v-39b38d02",path:"/zh/operation/hugeTLB.html",headers:[{level:2,title:"damocles-worker PC1 HugeTLB Pages 支持",slug:"damocles-worker-pc1-hugetlb-pages-支持",normalizedTitle:"damocles-worker pc1 hugetlb pages 支持",charIndex:2},{level:3,title:"使用方法",slug:"使用方法",normalizedTitle:"使用方法",charIndex:442}],headersStr:"damocles-worker PC1 HugeTLB Pages 支持 使用方法",content:'# damocles-worker PC1 HugeTLB Pages 支持\n\n在 PC1 阶段 rust-fil-proofs 会 mmap 两块和扇区大小一致的大块内存。这两块内存如果和 PC1 工作线程在同一 NUMA 节点会有效提升性能，使用 HugeTLB Pages 也会带来一定的性能提升。\n\n经过测试，即便配置了 numa_preferred 在运行一段时间后，系统也经常会跨 NUMA 节点申请这两块内存。使用我们改造后的 rust-fil-proofs (当前版本的 damocles-worker 已支持) 可以使用预先创建好的和扇区大小一致的 hugepage 内存文件，该内存文件仅供上述 PC1 阶段的两大块内存的需求使用。可以一定程度上解决问题。\n\n提示：原本的 mmap 方式优势在于由于 mmap 申请的内存由操作系统管理，即便在内存不足的情况下也能正常工作（内存严重不足可能会导致性能下降），用户可以根据自己的需要自行选择是否使用此功能。\n\n\n# 使用方法\n\n下文将演示启用两个 PC1 外部执行器子进程，两个 PC1 子进程分别运行在 NUMA0 和 NUMA1。并且每个 PC1 子进程的并发数为 1。\n\n# 1. 为每个 NUMA node 分配足够的 Persistent hugepages.\n\n单个 PC1 任务需要 64 GiB 的 persistent hugepages, 用户应该按照自己的需求分配足够的 persistent hugepages。\n\n下面命令演示分配 128 GiB 的 persistent hugepages:\n\n$ # 为 NUMA node 0 分配 64GiB, PageSize 为 1GiB 的 Persistent HugeTLB Pages 内存\n$ echo "64" | sudo tee /sys/devices/system/node/node0/hugepages/hugepages-1048576kB/nr_hugepages\n$ # 为 NUMA node 1 分配 64GiB, PageSize 为 1GiB Persistent HugeTLB Pages 内存\n$ echo "64" | sudo tee /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages\n\n\n更多信息请参考 HugeTLB Pages\n\n# 2. 挂载 hugetlbfs\n\nsudo mkdir /mnt/huge_1g\nsudo mount -t hugetlbfs -o pagesize=1g none /mnt/huge_1g\n\n\n上面的 mount 命令会挂载 (虚拟) 文件系统 hugetlbfs 到 /mnt/huge_1g 目录。在 /mnt/huge_1g 目录中创建的任何文件都会使用 huge pages。\n\n更多信息请参考 Using Huge Pages\n\n# 3. 使用 damocles-worker-store-hugepage-init 工具创建 hugepage 内存文件\n\nUsage:\n\ndamocles-worker-store-hugepage-file-init \n\nUSAGE:\n   damocles-worker store hugepage-file-init --node <numa_node_index> --num <number_of_files> --path <path> --path_pattern <path_pattern> --size <size>\n\nFLAGS:\n   -h, --help       \n            Prints help information\n\n   -V, --version    \n            Prints version information\n\n\nOPTIONS:\n   -n, --node <numa_node_index>         \n            Specify the numa node\n\n   -c, --num <number_of_files>          \n            Specify the number of hugepage memory files to be created\n\n      --path <path>                    \n            Specify the path to the output hugepage memory files and using the default pattern\n            (/specified_hugepage_file_path/numa_$NUMA_NODE_INDEX).\n            The created files looks like this:\n            /specified_hugepage_file_path/numa_0/file\n            /specified_hugepage_file_path/numa_1/file\n            /specified_hugepage_file_path/numa_2/file\n            ...\n            \n            This argument will be ignored if `path_pattern` is specified.\n      --path_pattern <path_pattern>    \n            Specify the path pattern for the output hugepage memory files where $NUMA_NODE_INDEX represents \n            the numa node index placeholder, which extracts the number in the folder name as the numa node index.\n            \n            If both the argument `path` and the argument `path_pattern` are specified, the argument `path` will be\n            ignored.\n   -s, --size <size>                    \n            Specify the size of each hugepage memory file. (e.g., 1B, 2KB, 3kiB, 1MB, 2MiB, 3GB, 1GiB, ...)\n\n\n\nExample:\n\n$ # 在 NUMA 节点 0 上创建 2 个大小为 32GiB 的 hugepage 内存文件\n$ sudo ./dist/bin/damocles-worker store hugepage-file-init --node=0 --num=2 --size=32GiB --path="/mnt/huge_1g/pc1_1"\n\n$ # 在 NUMA 节点 1 上创建 2 个大小为 32GiB 的 hugepage 内存文件\n$ sudo ./dist/bin/damocles-worker store hugepage-file-init --node=1 --num=2 --size=32GiB --path="/mnt/huge_1g/pc1_2"\n\n$ tree /mnt/huge_1g\n/mnt/huge_1g\n├── pc1_1\n│   └── numa_0\n│       ├── 32.0_GiB_0\n│       └── 32.0_GiB_1\n└── pc1_2\n    └── numa_1\n        ├── 32.0_GiB_0\n        └── 32.0_GiB_1\n\n4 directories, 4 files\n\n\n# 4. 配置 pc1 processor\n\n# damocles-worker.toml\n\n[[processors.pc1]]\n# ...\nenvs = { HUGEPAGE_FILES_PATH = "/mnt/huge_1g/pc1_1", ... }\nconcurrent = 1\n# ...\n\n[[processors.pc1]]\n# ...\nenvs = { HUGEPAGE_FILES_PATH = "/mnt/huge_1g/pc1_2", ... }\nconcurrent = 1\n# ...\n\n\n\n# 5. 启动 damocles-worker\n\n启动 damocles-worker 后，如果日志级别为 trace 并且出现以下日志则表示 HugeTLb files 配置成功。\n\n2022-08-19T10:12:14.731440277+08:00 TRACE ThreadId(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: loaded memory file: /mnt/huge_1g/processor_a/numa_0/32.0_GiB_0\n2022-08-19T10:12:14.749523696+08:00 TRACE ThreadId(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: loaded memory file: /mnt/huge_1g/processor_a/numa_0/32.0_GiB_1\n2022-08-19T10:12:14.767479765+08:00 TRACE ThreadId(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: loaded memory file: /mnt/huge_1g/processor_a/numa_1/32.0_GiB_0\n2022-08-19T10:12:14.785486639+08:00 TRACE ThreadId(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: loaded memory file: /mnt/huge_1g/processor_a/numa_1/32.0_GiB_1\n2022-08-19T10:12:14.78549745+08:00 TRACE ThreadId(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: number of loaded memory files: numa_id: 0, loaded: 2; numa_id: 1, loaded: 2\n',normalizedContent:'# damocles-worker pc1 hugetlb pages 支持\n\n在 pc1 阶段 rust-fil-proofs 会 mmap 两块和扇区大小一致的大块内存。这两块内存如果和 pc1 工作线程在同一 numa 节点会有效提升性能，使用 hugetlb pages 也会带来一定的性能提升。\n\n经过测试，即便配置了 numa_preferred 在运行一段时间后，系统也经常会跨 numa 节点申请这两块内存。使用我们改造后的 rust-fil-proofs (当前版本的 damocles-worker 已支持) 可以使用预先创建好的和扇区大小一致的 hugepage 内存文件，该内存文件仅供上述 pc1 阶段的两大块内存的需求使用。可以一定程度上解决问题。\n\n提示：原本的 mmap 方式优势在于由于 mmap 申请的内存由操作系统管理，即便在内存不足的情况下也能正常工作（内存严重不足可能会导致性能下降），用户可以根据自己的需要自行选择是否使用此功能。\n\n\n# 使用方法\n\n下文将演示启用两个 pc1 外部执行器子进程，两个 pc1 子进程分别运行在 numa0 和 numa1。并且每个 pc1 子进程的并发数为 1。\n\n# 1. 为每个 numa node 分配足够的 persistent hugepages.\n\n单个 pc1 任务需要 64 gib 的 persistent hugepages, 用户应该按照自己的需求分配足够的 persistent hugepages。\n\n下面命令演示分配 128 gib 的 persistent hugepages:\n\n$ # 为 numa node 0 分配 64gib, pagesize 为 1gib 的 persistent hugetlb pages 内存\n$ echo "64" | sudo tee /sys/devices/system/node/node0/hugepages/hugepages-1048576kb/nr_hugepages\n$ # 为 numa node 1 分配 64gib, pagesize 为 1gib persistent hugetlb pages 内存\n$ echo "64" | sudo tee /sys/devices/system/node/node1/hugepages/hugepages-1048576kb/nr_hugepages\n\n\n更多信息请参考 hugetlb pages\n\n# 2. 挂载 hugetlbfs\n\nsudo mkdir /mnt/huge_1g\nsudo mount -t hugetlbfs -o pagesize=1g none /mnt/huge_1g\n\n\n上面的 mount 命令会挂载 (虚拟) 文件系统 hugetlbfs 到 /mnt/huge_1g 目录。在 /mnt/huge_1g 目录中创建的任何文件都会使用 huge pages。\n\n更多信息请参考 using huge pages\n\n# 3. 使用 damocles-worker-store-hugepage-init 工具创建 hugepage 内存文件\n\nusage:\n\ndamocles-worker-store-hugepage-file-init \n\nusage:\n   damocles-worker store hugepage-file-init --node <numa_node_index> --num <number_of_files> --path <path> --path_pattern <path_pattern> --size <size>\n\nflags:\n   -h, --help       \n            prints help information\n\n   -v, --version    \n            prints version information\n\n\noptions:\n   -n, --node <numa_node_index>         \n            specify the numa node\n\n   -c, --num <number_of_files>          \n            specify the number of hugepage memory files to be created\n\n      --path <path>                    \n            specify the path to the output hugepage memory files and using the default pattern\n            (/specified_hugepage_file_path/numa_$numa_node_index).\n            the created files looks like this:\n            /specified_hugepage_file_path/numa_0/file\n            /specified_hugepage_file_path/numa_1/file\n            /specified_hugepage_file_path/numa_2/file\n            ...\n            \n            this argument will be ignored if `path_pattern` is specified.\n      --path_pattern <path_pattern>    \n            specify the path pattern for the output hugepage memory files where $numa_node_index represents \n            the numa node index placeholder, which extracts the number in the folder name as the numa node index.\n            \n            if both the argument `path` and the argument `path_pattern` are specified, the argument `path` will be\n            ignored.\n   -s, --size <size>                    \n            specify the size of each hugepage memory file. (e.g., 1b, 2kb, 3kib, 1mb, 2mib, 3gb, 1gib, ...)\n\n\n\nexample:\n\n$ # 在 numa 节点 0 上创建 2 个大小为 32gib 的 hugepage 内存文件\n$ sudo ./dist/bin/damocles-worker store hugepage-file-init --node=0 --num=2 --size=32gib --path="/mnt/huge_1g/pc1_1"\n\n$ # 在 numa 节点 1 上创建 2 个大小为 32gib 的 hugepage 内存文件\n$ sudo ./dist/bin/damocles-worker store hugepage-file-init --node=1 --num=2 --size=32gib --path="/mnt/huge_1g/pc1_2"\n\n$ tree /mnt/huge_1g\n/mnt/huge_1g\n├── pc1_1\n│   └── numa_0\n│       ├── 32.0_gib_0\n│       └── 32.0_gib_1\n└── pc1_2\n    └── numa_1\n        ├── 32.0_gib_0\n        └── 32.0_gib_1\n\n4 directories, 4 files\n\n\n# 4. 配置 pc1 processor\n\n# damocles-worker.toml\n\n[[processors.pc1]]\n# ...\nenvs = { hugepage_files_path = "/mnt/huge_1g/pc1_1", ... }\nconcurrent = 1\n# ...\n\n[[processors.pc1]]\n# ...\nenvs = { hugepage_files_path = "/mnt/huge_1g/pc1_2", ... }\nconcurrent = 1\n# ...\n\n\n\n# 5. 启动 damocles-worker\n\n启动 damocles-worker 后，如果日志级别为 trace 并且出现以下日志则表示 hugetlb files 配置成功。\n\n2022-08-19t10:12:14.731440277+08:00 trace threadid(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: loaded memory file: /mnt/huge_1g/processor_a/numa_0/32.0_gib_0\n2022-08-19t10:12:14.749523696+08:00 trace threadid(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: loaded memory file: /mnt/huge_1g/processor_a/numa_0/32.0_gib_1\n2022-08-19t10:12:14.767479765+08:00 trace threadid(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: loaded memory file: /mnt/huge_1g/processor_a/numa_1/32.0_gib_0\n2022-08-19t10:12:14.785486639+08:00 trace threadid(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: loaded memory file: /mnt/huge_1g/processor_a/numa_1/32.0_gib_1\n2022-08-19t10:12:14.78549745+08:00 trace threadid(01) storage_proofs_porep::stacked::vanilla::memory_handling::numa_mem_pool: number of loaded memory files: numa_id: 0, loaded: 2; numa_id: 1, loaded: 2\n',charsets:{cjk:!0}},{title:"damocles-manager 的 metrics 使用",frontmatter:{},regularPath:"/zh/operation/metrics.html",relativePath:"zh/operation/metrics.md",key:"v-6f0a5512",path:"/zh/operation/metrics.html",headers:[{level:2,title:"exporter info",slug:"exporter-info",normalizedTitle:"exporter info",charIndex:152},{level:2,title:"metrics type",slug:"metrics-type",normalizedTitle:"metrics type",charIndex:275}],headersStr:"exporter info metrics type",content:"# damocles-manager 的 metrics 使用\n\ndamocles 使用 metrics 来记录程序运行过程中的性能指标，本篇主要讲 damocles-manager 中 metrics 的意义，关于如何使用 metrics 进行进程的监控， 可以找到很多资料，不在此进行赘述。\n\n\n# exporter info\n\ndamocles-manager 的 exporter 和 rpc 使用同样的端口，url 为/metrics, 因此对于默认的部署方式，exporter 的 url 为 host:1789/metrics\n\n\n# metrics type\n\n# VenusClusterInfo\n\ndamocles-manager 启动的时候会将这个标记置成 1。\n\n# SectorManagerNewSector\n\nsector manager 记录新建扇区的计数器，存在 miner 的 tag，根据不同的 miner 分开统计。\n\n# SectorManagerPreCommitSector\n\nsector manager 记录扇区 preCommit 次数的计数器，存在 miner 的 tag，根据不同的 miner 分开统计。\n\n# SectorManagerCommitSector\n\nsector manager 记录扇区 commit 次数的计数器，存在 miner 的 tag，根据不同的 miner 分开统计。\n\n# ProverWinningPostDuration\n\nprover 侧记录 winningPost 时间跨度的计数器，存在 miner 的 tag，根据不同的 miner 分开统计，并且计算时间会按 s 作单位，进行分段统计。目前还没有启用。\n\n# ProverWindowPostDuration\n\nprover 侧记录 windowPost 时间跨度的计数器，存在 miner 的 tag，根据不同的 miner 分开统计，并且计算时间会按 minute 作单位，进行分段统计。目前还没有启用。\n\n# ProverWindowPostCompleteRate\n\nprover 侧记录 windowPost 完成率的计数器，在 miner 进入当前 deadline 倒数 20 个 epoch 的时候会开始显示 partition 的完成率，在没有进入倒计时状态的时候都显示 1， 进入之后显示完成率的小数，比如 10 个 partition 里有 9 个完成提交了，那么显示为 0.9。存在 miner 的 tag，根据不同的 miner 分开统计。目前还没有启用。\n\n# APIRequestDuration\n\ndamocles-manager 的 API 都会记录其响应的时间，并且响应时间会按 ms 作单位，进行分段统计。",normalizedContent:"# damocles-manager 的 metrics 使用\n\ndamocles 使用 metrics 来记录程序运行过程中的性能指标，本篇主要讲 damocles-manager 中 metrics 的意义，关于如何使用 metrics 进行进程的监控， 可以找到很多资料，不在此进行赘述。\n\n\n# exporter info\n\ndamocles-manager 的 exporter 和 rpc 使用同样的端口，url 为/metrics, 因此对于默认的部署方式，exporter 的 url 为 host:1789/metrics\n\n\n# metrics type\n\n# venusclusterinfo\n\ndamocles-manager 启动的时候会将这个标记置成 1。\n\n# sectormanagernewsector\n\nsector manager 记录新建扇区的计数器，存在 miner 的 tag，根据不同的 miner 分开统计。\n\n# sectormanagerprecommitsector\n\nsector manager 记录扇区 precommit 次数的计数器，存在 miner 的 tag，根据不同的 miner 分开统计。\n\n# sectormanagercommitsector\n\nsector manager 记录扇区 commit 次数的计数器，存在 miner 的 tag，根据不同的 miner 分开统计。\n\n# proverwinningpostduration\n\nprover 侧记录 winningpost 时间跨度的计数器，存在 miner 的 tag，根据不同的 miner 分开统计，并且计算时间会按 s 作单位，进行分段统计。目前还没有启用。\n\n# proverwindowpostduration\n\nprover 侧记录 windowpost 时间跨度的计数器，存在 miner 的 tag，根据不同的 miner 分开统计，并且计算时间会按 minute 作单位，进行分段统计。目前还没有启用。\n\n# proverwindowpostcompleterate\n\nprover 侧记录 windowpost 完成率的计数器，在 miner 进入当前 deadline 倒数 20 个 epoch 的时候会开始显示 partition 的完成率，在没有进入倒计时状态的时候都显示 1， 进入之后显示完成率的小数，比如 10 个 partition 里有 9 个完成提交了，那么显示为 0.9。存在 miner 的 tag，根据不同的 miner 分开统计。目前还没有启用。\n\n# apirequestduration\n\ndamocles-manager 的 api 都会记录其响应的时间，并且响应时间会按 ms 作单位，进行分段统计。",charsets:{cjk:!0}},{title:"lotus-miner 切换 damocles 流程",frontmatter:{},regularPath:"/zh/operation/migrate-miner.html",relativePath:"zh/operation/migrate-miner.md",key:"v-ef86c67a",path:"/zh/operation/migrate-miner.html",headers:[{level:2,title:"lotus-miner 切换 damocles 流程",slug:"lotus-miner-切换-damocles-流程",normalizedTitle:"lotus-miner 切换 damocles 流程",charIndex:2},{level:3,title:"切换流程",slug:"切换流程",normalizedTitle:"切换流程",charIndex:173},{level:2,title:"damocles 切换为 lotus-miner 流程",slug:"damocles-切换为-lotus-miner-流程",normalizedTitle:"damocles 切换为 lotus-miner 流程",charIndex:872},{level:3,title:"切换流程",slug:"切换流程-2",normalizedTitle:"切换流程",charIndex:173}],headersStr:"lotus-miner 切换 damocles 流程 切换流程 damocles 切换为 lotus-miner 流程 切换流程",content:'# lotus-miner 切换 damocles 流程\n\n本节介绍从 lotus-miner 切换 damocles 的流程，通常此需求的场景为：\n\n * lotus-miner 已经封装了扇区；\n * 切换为 damocles 后要求：\n   * 可以继续封装新的扇区；\n   * 时空证明正常（wdPoSt 和 winPoSt）。\n\n\n# 切换流程\n\n根据上述需求，切换为 damocles 流程有：\n\n * 搭建 venus 链服务，或者接入已有的链服务，请参考 venus 相关文档，注意钱包私钥需要导入 venus-wallet；\n * 初始化 damocles repo，具体参考快速启动；\n * 扇区永久存储导入，参考06.导入已存在的扇区；\n * 扇区元数据导入；\n * 用 damocles 封装新的扇区，具体参考相关文档。\n\n先决条件：\n\n * damocles 和 lotus-miner 封装扇区的过程不兼容，故切换时应保证所有的扇区封装任务都已完成 (Proving)\n\n# 元数据导入\n\n导入扇区元数据是通过调用各自接口进行的，故过程中需要 lotus-miner 和 damocles-manager 服务都启动。\n\n./damocles-manager util sealer sectors import --api=<cat ~/.lotusminer/api> \\\n--token=<cat ~/.lotusminer/token>\n\n\n可选 flag：\n\n * override, bool类型，标识是否覆盖已有的数据，默认为 false；\n * numbers, 切片类型，指定要导入的扇区号，假设要导入扇区号为 200,300 的扇区元数据，使用参考为：--numbers=200 --numbers=300；\n\ndamocles 中通过下面命令查询导入的扇区信息：\n\n./damocles-manager util sealer sectors list --offline=true\n\n\n\n# damocles 切换为 lotus-miner 流程\n\n本节介绍从 damocles 切换为 lotus-miner 的流程，通常此需求的场景为：\n\n * 用 damocles 封装了扇区；\n * 切换为 lotus-miner 后要求：\n   * 可以继续封装新的扇区；\n   * 时空证明正常（wdPoSt 和 winPoSt）。\n\n\n# 切换流程\n\n根据上述需求，切换为 lotus-miner 需要进行的流程有：\n\n * 搭建 lotus 链服务，请参考 lotus 官方文档，需要注意的是 venus-wallet 中用到的钱包私钥需全部导出到 lotus；\n * 扇区持久化数据导出，保证 damocles 密封的扇区在 lotus-miner 中能够被正确读取，这是时空证明所必需的；\n * 扇区元数据导出，以支持在 lotus-miner 中查看历史扇区信息和重建扇区。如果需要重建已损坏的扇区文件，这一步是必须的；\n * 更新 lotus-miner 数据库中 /storage/nextid，以使得新封装扇区的编号不重复。\n\n先决条件：\n\n * damocles 和 lotus-miner 封装扇区的过程不兼容，故切换时应保证所有的扇区封装任务（包括重做的）都已完成，即状态 Finalized: true，未完成的扇区不会导出。\n * lotus-miner 没有提供导入扇区元数据的 API，故采用直接将数据写入数据库的方式，导出数据前需要停止 lotus-miner 进程。\n\n# 持久化数据导出\n\ndamocles 的持久化存储目录允许管理多个 miner 的扇区文件，而 lotus-miner 的多个持久化目录仅管理一个 miner 的扇区文件。\n\n> 如果 cluster 的永久存储管理了多个 miner 的扇区文件，退出一个 miner 时需要重新归档，则建议使用命令导出，否则只需将 cluster 永久存储配置给 lotus-miner 即可，见后文。\n\n在 lotus-miner 中通过命令配置持久化存储：\n\n./lotus-miner storage attach --init --store <store path>\n\n\n查看持久化存储\n\n./lotus-miner storage list\n\n\n更多关于 lotus-miner 的持久化存储请参考lotus-miner storage config\n\n导出扇区持久化数据\n\n./damocles-manager util sealer sectors export --miner=<miner address> files \\\n--dest-path=<lotus store path>\n\n\n可选 flag：\n\n * reserve, bool类型，标识是否保留 damocles 中的持久化数据，默认为 false；\n * numbers, 切片类型，指定要导出的扇区号，不设置则导出全部。假设要导出扇区号为 200,300 的持久化数据，使用参考为：--numbers=200 --numbers=300；\n\n命令执行参考：\n\n * 增加 --reserve flag 表示保留源文件，即拷贝扇区文件。这种方式比较慢，如果目标文件不存在，直接拷贝，存在则忽略。使用此方式可以避免导出时间段内可能的 wdPoSt 失败。\n\n> ./damocles-manager util sealer sectors export --miner=t04079 files --dest-path=/storage-nfs-4/dest/t04079 --numbers 160 --reserve\nmove sector 160 file...\ncopy file from /storage-nfs-4/src/t04079/update/s-t04079-160 to /storage-nfs-4/dest/t04079/update/s-t04079-160\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/p_aux to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/p_aux\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-0.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-0.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-1.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-1.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-2.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-2.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-3.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-3.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-4.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-4.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-5.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-5.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-6.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-6.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-7.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-7.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/t_aux to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/t_aux\nexport failure number: 0, total: 1\n\n\n * 不增加 --reserve flag 表示不保留源文件，即移动扇区文件。如果目标和源在磁盘同一个分区，这种方式速度很快，建议使用此方式。\n\n> ./damocles-manager util sealer sectors export --miner=t04079 files --dest-path=/storage-nfs-4/dest/t04079 --numbers 160\nmove sector 160 file...\nmove file from /storage-nfs-4/src/t04079/update/s-t04079-160 to /storage-nfs-4/dest/t04079/update/s-t04079-160\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/p_aux to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/p_aux\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-0.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-0.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-1.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-1.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-2.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-2.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-3.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-3.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-4.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-4.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-5.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-5.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-6.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-6.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-7.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-7.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/t_aux to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/t_aux\nexport failure number: 0, total: 1\n\n\nTIP\n\n什么时候需要迁移扇区持久化文件呢？当 damocles 和 lotus-miner 的持久化目录不一致时。在通常情况下，我们不建议对扇区文件进行迁移，因为迁移比较耗时，过程中可能导致窗口期的 wdPoSt 失败。 比较好的做法就是将 damocles 的持久化目录也配置到 lotus-miner 的存储列表中。\n\n假设 lotus-miner 原来的 store 列表有：\n\n$ ./lotus-miner storage list\n4e34aa51-e955-4542-bd3a-6da3befc15a4:\n        [############                                      ] 54.58 TiB/217.4 TiB 25%\n        Unsealed: 0; Sealed: 0; Caches: 0; Updated: 0; Update-caches: 0; Reserved: 0 B\n        Weight: 10; Use: Store\n        Local: /storage-nfs-4/thelt/t05114\n        URL: http://127.0.0.1:2345/remote\n\n\n切换到 damocles 后为此 miner 新增存储列表如下：\n\n[[Common.PersistStores]]\nName = "t05114_01"\nPath = "/storage-nfs-4/t05114_01"\n\n[[Common.PersistStores]]\nName = "t05114_02"\nPath = "/storage-nfs-4/t05114_02"\n\n\n我们只需将这两个目录 attach 到 lotus-miner 的存储列表中：\n\n$ ./lotus-miner storage attach --store --init /storage-nfs-4/t05114_02\n$ ./lotus-miner storage attach --store --init /storage-nfs-4/t05114_01\n\n\n添加后在列表中出现即表示成功。\n\n# 元数据导出\n\n仅修改/storage/nextid：\n\n./damocles-manager util sealer sectors export --miner=<miner address> metadata \\\n--dest-repo=<lotus-miner repo> \\\n--next-number=<next number> \\\n--only-next-number=true \n\n\n> next-number 是必填项，最好为已封装扇区中最大的扇区号，lotus-miner 中新分配的扇区号是这里设置的值 +1。\n\n> dest-repo 使用绝对路径，如 /root/.lotusminer。\n\n> 修改 /storage/nextid 需要在 lotus-miner 开始新的扇区封装之前完成。一旦用 lotus-miner 开始了封装，此修改将无效，这是 lotus-miner 自身的分配机制决定的，具体参考 lotus-miner next-sid 分配\n\n导出元数据，并设置新扇区号：\n\n./damocles-manager util sealer sectors export --miner=<miner address> metadata \\\n--dest-repo=<lotus-miner repo> \\\n--next-number=<next number>\n\n\n> next-number 为可选项，不设置时处理为已封装的最大扇区号。设置时，如果值小于已封装的最大扇区号，则处理为已封装的最大扇区号，否则处理为设置的值。\n\n启动 lotus-miner,通过下面命令能够查询到 damocles 封装的扇区即表示导出成功，后续就可以封装新的扇区了。\n\n./lotus-miner sector list\n\n\n查看新的扇区号：\n\n./lotus-miner sectors numbers info\n\n\n> damocles 中可能存在已经封装上链的扇区状态还是 Finalized: false 的情况，需要手动将其设置为完成状态才会被导出到 lotus-miner。',normalizedContent:'# lotus-miner 切换 damocles 流程\n\n本节介绍从 lotus-miner 切换 damocles 的流程，通常此需求的场景为：\n\n * lotus-miner 已经封装了扇区；\n * 切换为 damocles 后要求：\n   * 可以继续封装新的扇区；\n   * 时空证明正常（wdpost 和 winpost）。\n\n\n# 切换流程\n\n根据上述需求，切换为 damocles 流程有：\n\n * 搭建 venus 链服务，或者接入已有的链服务，请参考 venus 相关文档，注意钱包私钥需要导入 venus-wallet；\n * 初始化 damocles repo，具体参考快速启动；\n * 扇区永久存储导入，参考06.导入已存在的扇区；\n * 扇区元数据导入；\n * 用 damocles 封装新的扇区，具体参考相关文档。\n\n先决条件：\n\n * damocles 和 lotus-miner 封装扇区的过程不兼容，故切换时应保证所有的扇区封装任务都已完成 (proving)\n\n# 元数据导入\n\n导入扇区元数据是通过调用各自接口进行的，故过程中需要 lotus-miner 和 damocles-manager 服务都启动。\n\n./damocles-manager util sealer sectors import --api=<cat ~/.lotusminer/api> \\\n--token=<cat ~/.lotusminer/token>\n\n\n可选 flag：\n\n * override, bool类型，标识是否覆盖已有的数据，默认为 false；\n * numbers, 切片类型，指定要导入的扇区号，假设要导入扇区号为 200,300 的扇区元数据，使用参考为：--numbers=200 --numbers=300；\n\ndamocles 中通过下面命令查询导入的扇区信息：\n\n./damocles-manager util sealer sectors list --offline=true\n\n\n\n# damocles 切换为 lotus-miner 流程\n\n本节介绍从 damocles 切换为 lotus-miner 的流程，通常此需求的场景为：\n\n * 用 damocles 封装了扇区；\n * 切换为 lotus-miner 后要求：\n   * 可以继续封装新的扇区；\n   * 时空证明正常（wdpost 和 winpost）。\n\n\n# 切换流程\n\n根据上述需求，切换为 lotus-miner 需要进行的流程有：\n\n * 搭建 lotus 链服务，请参考 lotus 官方文档，需要注意的是 venus-wallet 中用到的钱包私钥需全部导出到 lotus；\n * 扇区持久化数据导出，保证 damocles 密封的扇区在 lotus-miner 中能够被正确读取，这是时空证明所必需的；\n * 扇区元数据导出，以支持在 lotus-miner 中查看历史扇区信息和重建扇区。如果需要重建已损坏的扇区文件，这一步是必须的；\n * 更新 lotus-miner 数据库中 /storage/nextid，以使得新封装扇区的编号不重复。\n\n先决条件：\n\n * damocles 和 lotus-miner 封装扇区的过程不兼容，故切换时应保证所有的扇区封装任务（包括重做的）都已完成，即状态 finalized: true，未完成的扇区不会导出。\n * lotus-miner 没有提供导入扇区元数据的 api，故采用直接将数据写入数据库的方式，导出数据前需要停止 lotus-miner 进程。\n\n# 持久化数据导出\n\ndamocles 的持久化存储目录允许管理多个 miner 的扇区文件，而 lotus-miner 的多个持久化目录仅管理一个 miner 的扇区文件。\n\n> 如果 cluster 的永久存储管理了多个 miner 的扇区文件，退出一个 miner 时需要重新归档，则建议使用命令导出，否则只需将 cluster 永久存储配置给 lotus-miner 即可，见后文。\n\n在 lotus-miner 中通过命令配置持久化存储：\n\n./lotus-miner storage attach --init --store <store path>\n\n\n查看持久化存储\n\n./lotus-miner storage list\n\n\n更多关于 lotus-miner 的持久化存储请参考lotus-miner storage config\n\n导出扇区持久化数据\n\n./damocles-manager util sealer sectors export --miner=<miner address> files \\\n--dest-path=<lotus store path>\n\n\n可选 flag：\n\n * reserve, bool类型，标识是否保留 damocles 中的持久化数据，默认为 false；\n * numbers, 切片类型，指定要导出的扇区号，不设置则导出全部。假设要导出扇区号为 200,300 的持久化数据，使用参考为：--numbers=200 --numbers=300；\n\n命令执行参考：\n\n * 增加 --reserve flag 表示保留源文件，即拷贝扇区文件。这种方式比较慢，如果目标文件不存在，直接拷贝，存在则忽略。使用此方式可以避免导出时间段内可能的 wdpost 失败。\n\n> ./damocles-manager util sealer sectors export --miner=t04079 files --dest-path=/storage-nfs-4/dest/t04079 --numbers 160 --reserve\nmove sector 160 file...\ncopy file from /storage-nfs-4/src/t04079/update/s-t04079-160 to /storage-nfs-4/dest/t04079/update/s-t04079-160\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/p_aux to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/p_aux\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-0.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-0.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-1.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-1.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-2.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-2.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-3.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-3.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-4.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-4.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-5.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-5.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-6.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-6.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-7.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-7.dat\ncopy file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/t_aux to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/t_aux\nexport failure number: 0, total: 1\n\n\n * 不增加 --reserve flag 表示不保留源文件，即移动扇区文件。如果目标和源在磁盘同一个分区，这种方式速度很快，建议使用此方式。\n\n> ./damocles-manager util sealer sectors export --miner=t04079 files --dest-path=/storage-nfs-4/dest/t04079 --numbers 160\nmove sector 160 file...\nmove file from /storage-nfs-4/src/t04079/update/s-t04079-160 to /storage-nfs-4/dest/t04079/update/s-t04079-160\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/p_aux to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/p_aux\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-0.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-0.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-1.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-1.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-2.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-2.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-3.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-3.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-4.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-4.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-5.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-5.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-6.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-6.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-7.dat to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/sc-02-data-tree-r-last-7.dat\nmove file from /storage-nfs-4/src/t04079/update-cache/s-t04079-160/t_aux to /storage-nfs-4/dest/t04079/update-cache/s-t04079-160/t_aux\nexport failure number: 0, total: 1\n\n\ntip\n\n什么时候需要迁移扇区持久化文件呢？当 damocles 和 lotus-miner 的持久化目录不一致时。在通常情况下，我们不建议对扇区文件进行迁移，因为迁移比较耗时，过程中可能导致窗口期的 wdpost 失败。 比较好的做法就是将 damocles 的持久化目录也配置到 lotus-miner 的存储列表中。\n\n假设 lotus-miner 原来的 store 列表有：\n\n$ ./lotus-miner storage list\n4e34aa51-e955-4542-bd3a-6da3befc15a4:\n        [############                                      ] 54.58 tib/217.4 tib 25%\n        unsealed: 0; sealed: 0; caches: 0; updated: 0; update-caches: 0; reserved: 0 b\n        weight: 10; use: store\n        local: /storage-nfs-4/thelt/t05114\n        url: http://127.0.0.1:2345/remote\n\n\n切换到 damocles 后为此 miner 新增存储列表如下：\n\n[[common.persiststores]]\nname = "t05114_01"\npath = "/storage-nfs-4/t05114_01"\n\n[[common.persiststores]]\nname = "t05114_02"\npath = "/storage-nfs-4/t05114_02"\n\n\n我们只需将这两个目录 attach 到 lotus-miner 的存储列表中：\n\n$ ./lotus-miner storage attach --store --init /storage-nfs-4/t05114_02\n$ ./lotus-miner storage attach --store --init /storage-nfs-4/t05114_01\n\n\n添加后在列表中出现即表示成功。\n\n# 元数据导出\n\n仅修改/storage/nextid：\n\n./damocles-manager util sealer sectors export --miner=<miner address> metadata \\\n--dest-repo=<lotus-miner repo> \\\n--next-number=<next number> \\\n--only-next-number=true \n\n\n> next-number 是必填项，最好为已封装扇区中最大的扇区号，lotus-miner 中新分配的扇区号是这里设置的值 +1。\n\n> dest-repo 使用绝对路径，如 /root/.lotusminer。\n\n> 修改 /storage/nextid 需要在 lotus-miner 开始新的扇区封装之前完成。一旦用 lotus-miner 开始了封装，此修改将无效，这是 lotus-miner 自身的分配机制决定的，具体参考 lotus-miner next-sid 分配\n\n导出元数据，并设置新扇区号：\n\n./damocles-manager util sealer sectors export --miner=<miner address> metadata \\\n--dest-repo=<lotus-miner repo> \\\n--next-number=<next number>\n\n\n> next-number 为可选项，不设置时处理为已封装的最大扇区号。设置时，如果值小于已封装的最大扇区号，则处理为已封装的最大扇区号，否则处理为设置的值。\n\n启动 lotus-miner,通过下面命令能够查询到 damocles 封装的扇区即表示导出成功，后续就可以封装新的扇区了。\n\n./lotus-miner sector list\n\n\n查看新的扇区号：\n\n./lotus-miner sectors numbers info\n\n\n> damocles 中可能存在已经封装上链的扇区状态还是 finalized: false 的情况，需要手动将其设置为完成状态才会被导出到 lotus-miner。',charsets:{cjk:!0}},{title:"导入已存在的扇区数据",frontmatter:{},regularPath:"/zh/operation/migrate-sectors.html",relativePath:"zh/operation/migrate-sectors.md",key:"v-2e3168b2",path:"/zh/operation/migrate-sectors.html",headers:[{level:2,title:"导入及校验",slug:"导入及校验",normalizedTitle:"导入及校验",charIndex:101},{level:3,title:"导入",slug:"导入",normalizedTitle:"导入",charIndex:2},{level:3,title:"校验",slug:"校验",normalizedTitle:"校验",charIndex:104}],headersStr:"导入及校验 导入 校验",content:'# 导入已存在的扇区数据\n\n当我们希望将已经通过其他算力组件方案完成的扇区存储目录迁移到 damocles 中时，只需要使用 damocles-manager 导入，并更新相应的配置文件即可。\n\n\n# 导入及校验\n\n注意:导入和校验都需要在未启动 damocles-manager daemon 的情况下进行。\n\n\n# 导入\n\ndamocles-manager 提供了名为 storage attach 的导入工具，其使用方式如下：\n\ndamocles-manager util storage attach --verbose --name={storage name} <path>\n\n\n其中：\n\n * name 是一个选填参数;\n * <path> 是存储路径，在导入过程中会被转换成绝对路径。\n\nname 和 <path> 的含义可以参考 Common.PersistStores。\n\n举例来说，我们使用\n\ndamocles-manager util storage attach --verbose --name=a ./mock-tmp/remote\n\n\n通常会产生类似下面的日志：\n\n2022-03-11T16:03:52.492+0800    DEBUG   policy  policy/const.go:18      NETWORK SETUP   {"name": "mainnet"}\n2022-03-11T16:03:52.493+0800    INFO    cmd     internal/util_storage.go:104    use match pattern "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/remote/sealed/*"     {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.493+0800    INFO    cmd     internal/util_storage.go:121    path "s-t010000-16" matched=true        {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.494+0800    INFO    cmd     internal/util_storage.go:121    path "s-t010000-17" matched=true        {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.494+0800    INFO    cmd     internal/util_storage.go:121    path "s-t010000-18" matched=true        {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.508+0800    INFO    cmd     internal/util_storage.go:148    sector indexer updated for s-t010000-16 {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.509+0800    INFO    cmd     internal/util_storage.go:148    sector indexer updated for s-t010000-17 {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.509+0800    INFO    cmd     internal/util_storage.go:148    sector indexer updated for s-t010000-18 {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.509+0800    INFO    cmd     internal/util_storage.go:152    3 sectors out of 3 files have been updated      {"name": "a", "strict": false, "read-only": false}\n2022-03-11T16:03:52.509+0800    WARN    cmd     internal/util_storage.go:153    add the section below into the config file:     {"name": "a", "strict": false, "read-only": false}\n\n[[Common.PersistStores]]\nName = "a"\nPath = "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/remote"\nStrict = false\nReadOnly = false\n\n\n这时目录导入就已经完成了，所有扇区的位置信息也被记录了下来。 我们将最后输出的范例配置复制并填写到 damocles-manager 的配置文件中即可完成导入工作。\n\n# 重新导入\n\n如果我们发现导入时填写的信息有误，例如 --name 出现了拼写错误，那么我们只需要重新使用正确的信息完成一次导入流程即可。 扇区的位置信息会被覆盖更新。\n\n# sealed_file 与 cache_dir 分离\n\n一些算力组件允许 sealed_file 与 cache_dir 位于不同的存储实例上，这种情况下，常规导入可能会无法正常定位扇区文件。 这种情况下，可以通过增加命令行参数 --allow-splitted 来启用分隔扫描模式，在这种模式下，会单独扫描 sealed 文件夹和 cache 文件夹中符合扇区命名规则的路径，并分别记录定位信息。\n\n此时，日志会类似：\n\n2022-04-19T19:11:55.137+0800    DEBUG   policy  policy/const.go:18      NETWORK SETUP   {"name": "mainnet"}\n2022-04-19T19:11:55.154+0800    INFO    cmd     internal/util_storage.go:120    scan for sectors(upgrade=false) {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.154+0800    INFO    cmd     internal/util_storage.go:211    0 sectors out of 0 files have been found        {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.154+0800    INFO    cmd     internal/util_storage.go:145    scan for splitted cache dirs(upgrade=false)     {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.155+0800    INFO    cmd     internal/util_storage.go:211    3 sectors out of 3 files have been found        {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.156+0800    INFO    cmd     internal/util_storage.go:120    scan for sectors(upgrade=true)  {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.156+0800    INFO    cmd     internal/util_storage.go:211    0 sectors out of 0 files have been found        {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.156+0800    INFO    cmd     internal/util_storage.go:145    scan for splitted cache dirs(upgrade=true)      {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.156+0800    INFO    cmd     internal/util_storage.go:211    0 sectors out of 0 files have been found        {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19T19:11:55.156+0800    WARN    cmd     internal/util_storage.go:166    add the section below into the config file:     {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n\n[[Common.PersistStores]]\nName = "p3"\nPath = "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/pstore3"\nStrict = false\nReadOnly = false\n\n\n注意，使用这种模式需要确认：\n\n * 目标目录中不存在由于存储异常导致的重复存储的文件\n * 仅有 cache_dir 而无与之对应的 sealed_file 的扇区仍然无法正常定位\n\n\n# 校验\n\ndamocles-manager 提供的 storage find 工具可以用来检查扇区导入的结果是否正确，其使用方式如下：\n\ndamocles-manager util storage find <miner actor id> <sector number>\n\n\n继续以上面示范的导入工作为例，我们希望检验扇区 s-t010000-17 是否已被正确记录，可以使用：\n\ndamocles-manager util storage find 10000 17\n\n\n通常会产生类似下面的日志：\n\n2022-04-19T19:13:15.235+0800    DEBUG   policy  policy/const.go:18      NETWORK SETUP   {"name": "mainnet"}\n2022-04-19T19:13:15.249+0800    INFO    cmd     internal/util_storage.go:279    sector s-t010000-17 located, sealed file in "a", cache dir in "a"\n2022-04-19T19:13:15.249+0800    INFO    cmd     internal/util_storage.go:285    store instance exists   {"instance": "a"}\n2022-04-19T19:13:15.249+0800    INFO    cmd     internal/util_storage.go:285    store instance exists   {"instance": "a"}\n\n\n这就表示我们的导入和配置工作都已经完成了。\n\n# 校验异常：扇区信息未记录成功\n\n如果校验过程中出现类似\n\n2022-03-11T16:45:59.120+0800    WARN    cmd     internal/util_storage.go:214    s-t010000-17 not found\n\n\n这样的日志，说明指定的扇区未导入成功，我们需要重新检查导入过程。\n\n# 校验异常：存储配置未更新\n\n如果校验过程中出现类似\n\n2022-03-11T16:22:34.044+0800    DEBUG   policy  policy/const.go:18      NETWORK SETUP   {"name": "mainnet"}\n2022-03-11T16:22:34.059+0800    INFO    cmd     internal/util_storage.go:218    found s-t010000-17 in "a"\n2022-03-11T16:22:34.059+0800    WARN    cmd     internal/util_storage.go:227    store instance not found, check your config file\n\n\n这样的日志，说明 damocles-manager 的配置文件没有更新成功，我们需要按之前所说的方法去更新配置。\n\n# 扇区检查\n\n使用如下命令全量检查 attach 的文件是否存在\n\nfor i in `seq 0 47`; do  damocles-manager util sealer proving --miner <miner_id> check $i ; done\n\n\n输出的信息如下：\n\ndeadline  partition  good  bad\n0         0          2349  0\ndeadline  partition  good  bad\n1         0          2349  0\ndeadline  partition  good  bad\n2         0          2349  0\ndeadline  partition  good  bad\n3         0          2349  0\ndeadline  partition  good  bad\n4         0          2349  0\ndeadline  partition  good  bad\n5         0          2349  0\ndeadline  partition  good  bad\n6         0          2349  0\ndeadline  partition  good  bad\n7         0          2349  0\ndeadline  partition  good  bad\n8         0          2349  0\ndeadline  partition  good  bad\n9         0          2349  0\ndeadline  partition  good  bad\n10        0          2349  0\ndeadline  partition  good  bad\n11        0          2349  0\ndeadline  partition  good  bad\n12        0          2349  0\ndeadline  partition  good  bad\n13        0          2349  0\ndeadline  partition  good  bad\n14        0          264   0\ndeadline  partition  good  bad\n15        0          2349  0\ndeadline  partition  good  bad\n16        0          2349    0\n...\ndeadline  partition  good  bad\n47        0          2349  0\n',normalizedContent:'# 导入已存在的扇区数据\n\n当我们希望将已经通过其他算力组件方案完成的扇区存储目录迁移到 damocles 中时，只需要使用 damocles-manager 导入，并更新相应的配置文件即可。\n\n\n# 导入及校验\n\n注意:导入和校验都需要在未启动 damocles-manager daemon 的情况下进行。\n\n\n# 导入\n\ndamocles-manager 提供了名为 storage attach 的导入工具，其使用方式如下：\n\ndamocles-manager util storage attach --verbose --name={storage name} <path>\n\n\n其中：\n\n * name 是一个选填参数;\n * <path> 是存储路径，在导入过程中会被转换成绝对路径。\n\nname 和 <path> 的含义可以参考 common.persiststores。\n\n举例来说，我们使用\n\ndamocles-manager util storage attach --verbose --name=a ./mock-tmp/remote\n\n\n通常会产生类似下面的日志：\n\n2022-03-11t16:03:52.492+0800    debug   policy  policy/const.go:18      network setup   {"name": "mainnet"}\n2022-03-11t16:03:52.493+0800    info    cmd     internal/util_storage.go:104    use match pattern "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/remote/sealed/*"     {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.493+0800    info    cmd     internal/util_storage.go:121    path "s-t010000-16" matched=true        {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.494+0800    info    cmd     internal/util_storage.go:121    path "s-t010000-17" matched=true        {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.494+0800    info    cmd     internal/util_storage.go:121    path "s-t010000-18" matched=true        {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.508+0800    info    cmd     internal/util_storage.go:148    sector indexer updated for s-t010000-16 {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.509+0800    info    cmd     internal/util_storage.go:148    sector indexer updated for s-t010000-17 {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.509+0800    info    cmd     internal/util_storage.go:148    sector indexer updated for s-t010000-18 {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.509+0800    info    cmd     internal/util_storage.go:152    3 sectors out of 3 files have been updated      {"name": "a", "strict": false, "read-only": false}\n2022-03-11t16:03:52.509+0800    warn    cmd     internal/util_storage.go:153    add the section below into the config file:     {"name": "a", "strict": false, "read-only": false}\n\n[[common.persiststores]]\nname = "a"\npath = "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/remote"\nstrict = false\nreadonly = false\n\n\n这时目录导入就已经完成了，所有扇区的位置信息也被记录了下来。 我们将最后输出的范例配置复制并填写到 damocles-manager 的配置文件中即可完成导入工作。\n\n# 重新导入\n\n如果我们发现导入时填写的信息有误，例如 --name 出现了拼写错误，那么我们只需要重新使用正确的信息完成一次导入流程即可。 扇区的位置信息会被覆盖更新。\n\n# sealed_file 与 cache_dir 分离\n\n一些算力组件允许 sealed_file 与 cache_dir 位于不同的存储实例上，这种情况下，常规导入可能会无法正常定位扇区文件。 这种情况下，可以通过增加命令行参数 --allow-splitted 来启用分隔扫描模式，在这种模式下，会单独扫描 sealed 文件夹和 cache 文件夹中符合扇区命名规则的路径，并分别记录定位信息。\n\n此时，日志会类似：\n\n2022-04-19t19:11:55.137+0800    debug   policy  policy/const.go:18      network setup   {"name": "mainnet"}\n2022-04-19t19:11:55.154+0800    info    cmd     internal/util_storage.go:120    scan for sectors(upgrade=false) {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.154+0800    info    cmd     internal/util_storage.go:211    0 sectors out of 0 files have been found        {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.154+0800    info    cmd     internal/util_storage.go:145    scan for splitted cache dirs(upgrade=false)     {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.155+0800    info    cmd     internal/util_storage.go:211    3 sectors out of 3 files have been found        {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.156+0800    info    cmd     internal/util_storage.go:120    scan for sectors(upgrade=true)  {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.156+0800    info    cmd     internal/util_storage.go:211    0 sectors out of 0 files have been found        {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.156+0800    info    cmd     internal/util_storage.go:145    scan for splitted cache dirs(upgrade=true)      {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.156+0800    info    cmd     internal/util_storage.go:211    0 sectors out of 0 files have been found        {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n2022-04-19t19:11:55.156+0800    warn    cmd     internal/util_storage.go:166    add the section below into the config file:     {"name": "p3", "strict": false, "read-only": false, "splitted": true}\n\n[[common.persiststores]]\nname = "p3"\npath = "/home/dtynn/proj/github.com/ipfs-force-community/venus-cluster/mock-tmp/pstore3"\nstrict = false\nreadonly = false\n\n\n注意，使用这种模式需要确认：\n\n * 目标目录中不存在由于存储异常导致的重复存储的文件\n * 仅有 cache_dir 而无与之对应的 sealed_file 的扇区仍然无法正常定位\n\n\n# 校验\n\ndamocles-manager 提供的 storage find 工具可以用来检查扇区导入的结果是否正确，其使用方式如下：\n\ndamocles-manager util storage find <miner actor id> <sector number>\n\n\n继续以上面示范的导入工作为例，我们希望检验扇区 s-t010000-17 是否已被正确记录，可以使用：\n\ndamocles-manager util storage find 10000 17\n\n\n通常会产生类似下面的日志：\n\n2022-04-19t19:13:15.235+0800    debug   policy  policy/const.go:18      network setup   {"name": "mainnet"}\n2022-04-19t19:13:15.249+0800    info    cmd     internal/util_storage.go:279    sector s-t010000-17 located, sealed file in "a", cache dir in "a"\n2022-04-19t19:13:15.249+0800    info    cmd     internal/util_storage.go:285    store instance exists   {"instance": "a"}\n2022-04-19t19:13:15.249+0800    info    cmd     internal/util_storage.go:285    store instance exists   {"instance": "a"}\n\n\n这就表示我们的导入和配置工作都已经完成了。\n\n# 校验异常：扇区信息未记录成功\n\n如果校验过程中出现类似\n\n2022-03-11t16:45:59.120+0800    warn    cmd     internal/util_storage.go:214    s-t010000-17 not found\n\n\n这样的日志，说明指定的扇区未导入成功，我们需要重新检查导入过程。\n\n# 校验异常：存储配置未更新\n\n如果校验过程中出现类似\n\n2022-03-11t16:22:34.044+0800    debug   policy  policy/const.go:18      network setup   {"name": "mainnet"}\n2022-03-11t16:22:34.059+0800    info    cmd     internal/util_storage.go:218    found s-t010000-17 in "a"\n2022-03-11t16:22:34.059+0800    warn    cmd     internal/util_storage.go:227    store instance not found, check your config file\n\n\n这样的日志，说明 damocles-manager 的配置文件没有更新成功，我们需要按之前所说的方法去更新配置。\n\n# 扇区检查\n\n使用如下命令全量检查 attach 的文件是否存在\n\nfor i in `seq 0 47`; do  damocles-manager util sealer proving --miner <miner_id> check $i ; done\n\n\n输出的信息如下：\n\ndeadline  partition  good  bad\n0         0          2349  0\ndeadline  partition  good  bad\n1         0          2349  0\ndeadline  partition  good  bad\n2         0          2349  0\ndeadline  partition  good  bad\n3         0          2349  0\ndeadline  partition  good  bad\n4         0          2349  0\ndeadline  partition  good  bad\n5         0          2349  0\ndeadline  partition  good  bad\n6         0          2349  0\ndeadline  partition  good  bad\n7         0          2349  0\ndeadline  partition  good  bad\n8         0          2349  0\ndeadline  partition  good  bad\n9         0          2349  0\ndeadline  partition  good  bad\n10        0          2349  0\ndeadline  partition  good  bad\n11        0          2349  0\ndeadline  partition  good  bad\n12        0          2349  0\ndeadline  partition  good  bad\n13        0          2349  0\ndeadline  partition  good  bad\n14        0          264   0\ndeadline  partition  good  bad\n15        0          2349  0\ndeadline  partition  good  bad\n16        0          2349    0\n...\ndeadline  partition  good  bad\n47        0          2349  0\n',charsets:{cjk:!0}},{title:"独立运行的 PoSter 节点",frontmatter:{},regularPath:"/zh/operation/poster.html",relativePath:"zh/operation/poster.md",key:"v-4578197f",path:"/zh/operation/poster.html",headers:[{level:2,title:"worker-prover 模式",slug:"worker-prover-模式",normalizedTitle:"worker-prover 模式",charIndex:415},{level:3,title:"基本原理",slug:"基本原理",normalizedTitle:"基本原理",charIndex:547},{level:3,title:"damocles-manager 配置与启动",slug:"damocles-manager-配置与启动",normalizedTitle:"damocles-manager 配置与启动",charIndex:2406},{level:3,title:"damocles-worker 配置",slug:"damocles-worker-配置",normalizedTitle:"damocles-worker 配置",charIndex:3079},{level:3,title:"管理 window post 任务",slug:"管理-window-post-任务",normalizedTitle:"管理 window post 任务",charIndex:6676},{level:3,title:"禁止 damocles-manager 使用 GPU",slug:"禁止-damocles-manager-使用-gpu",normalizedTitle:"禁止 damocles-manager 使用 gpu",charIndex:7914},{level:2,title:"代理节点模式",slug:"代理节点模式",normalizedTitle:"代理节点模式",charIndex:432},{level:3,title:"代理节点的使用方式",slug:"代理节点的使用方式",normalizedTitle:"代理节点的使用方式",charIndex:8333},{level:3,title:"代理节点使用已有配置文件",slug:"代理节点使用已有配置文件",normalizedTitle:"代理节点使用已有配置文件",charIndex:9198},{level:2,title:"ext-prover 执行器",slug:"ext-prover-执行器",normalizedTitle:"ext-prover 执行器",charIndex:9674},{level:2,title:"部署实践",slug:"部署实践",normalizedTitle:"部署实践",charIndex:11146},{level:2,title:"局限性",slug:"局限性",normalizedTitle:"局限性",charIndex:133}],headersStr:"worker-prover 模式 基本原理 damocles-manager 配置与启动 damocles-worker 配置 管理 window post 任务 禁止 damocles-manager 使用 GPU 代理节点模式 代理节点的使用方式 代理节点使用已有配置文件 ext-prover 执行器 部署实践 局限性",content:'# 独立运行的 PoSter 节点\n\n在早期版本中，虽然 damocles-manager 已经支持通过 daemon run 命令的 --poster、--miner 参数来选择是否启用相应的模块，但由于 post 证明过程与扇区定位信息的强关联，使得真正使用时，局限性比较大，且难以扩展。\n\n从 v0.2.0 版本起，我们提供了一系列的功能组合，使得易用、可扩展的独立 PoSter 节点成为大体量， 多矿工号 SP 的一种可选方案。\n\n以下，我们会介绍这些新的功能点，并提供一种通过这些功能完成独立 PoSter 节点部署的实践。后续文档都以开启 --poster 的节点作为示例，独立的 --miner 节点运作方式与之类似，不再单独阐述。\n\n----------------------------------------\n\n在 v0.8.0 版本中，damocles 支持三种方式独立运行 PoSter 节点，分别是 worker-prover 模式、代理节点模式、ext-prover 模式 (外部执行器模式)。\n\n\n# worker-prover 模式\n\nworker-prover 模式是 v0.8.0 新增的功能，特点是简单，可以非常轻松的支持多机 wdpost。\n\n\n# 基本原理\n\nworker-prover 模式利用 damocles-worker 计算 window post 证明，通过 RPC 的方式从 damocles-manager 获取 window post 任务和返回计算的结果。\n\ndamocles-worker 新增 wdpost planner 用于执行 window post 任务。\n\n# Architecture\n\n                +-----------------------------------+\n                |     damocles-manager daemon       |\n                |     with --worker-prover flag     |\n                |                                   |\n                |        +-----------------+        |\n                |        |damocles-manager |        |\n                |        |  poster module  |        |\n                |        +-------+-^-------+        |\n                |           send | |recv            |\n                |                | |                |\n                |        +-------v-+-------+        |\n                |        |  worker-prover  |        |\n       +--------+--------\x3e      module     <--------+--------+\n       |        |        +--------^--------+        |        |\n       |        |                 |                 |        |\n       |        +-----------------+-----------------+        |\n       |                          |                          |\n-------+--------------------------+--------------------------+------------\n       |                          |                          |\n       | pull job                 | pull job                 | pull job\n       | push res                 | push res                 | push res\n       | by rpc                   | by rpc                   | by rpc\n       |                          |                          |\n+------+--------+         +-------+-------+           +------+--------+\n|damocles-worker|         |damocles-worker|           |damocles-worker|\n|wdpost planner |         |wdpost planner |  ...      |wdpost planner |\n+---------------+         +---------------+           +---------------+\n\n\n\n# damocles-manager 配置与启动\n\n新增配置：\n\n# ~/.damocles-manager/sector-manager.cfg\n\n# ...\n\n[Common.Proving.WorkerProver]\n# WindowPoSt 任务的最大尝试次数，可选项，数字类型\n# 默认值为 2\n# 尝试次数超过 JobMaxTry 的 WindowPoSt 任务只能通过手动 reset 的方式被重新执行\nJobMaxTry = 2\n# WindowPoSt 任务的心跳超时时间，可选项，时间字符串类型\n# 默认值为 15s\n# 超过此时间没有发送心跳的任务将会被设置为失败并重试\nHeartbeatTimeout = "15s"\n# WindowPoSt 任务的过期时间，可选项，时间字符串类型\n# 默认值为 25h\n# 创建时间超过此时间的 WindowPoSt 任务将会被删除\nJobLifetime = "25h0m0s"\n\n# ...\n\n\n启动 damocles-manager 进程：\n\n# --miner flag 可选添加，表示启动 miner 模块用于执行 WinningPoSt 并出块\n# --poster flag 必须添加，表示启动 WindowPoSt 模块\n# --worker-prover 必须添加，表示使用 WorkerProver 模块执行 WindowPoSt\n./damocles-manager daemon run --miner --poster --worker-prover\n\n\n\n# damocles-worker 配置\n\n配置解释：\n\n[[sealing_thread]]\n# 配置使用 wdpost plan\nplan = "wdpost"\n# 配置只允许执行指定矿工号的任务，为空则表示不限制\n# sealing.allowed_miners = [6666, 7777]\n# 配置只允许运行指定 size 的扇区的任务\n# allowed_sizes = ["32GiB", "64GiB"]\n\n[[attached]]\n# 配置此 worker 执行 window post 任务过程中会用到的永久存储\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n\n# 控制 window_post 任务并发 (可选)，不配置则不限制\n[processors.limitation.concurrent]\nwindow_post = 2\n\n[[processors.window_post]]\n# 使用自定义 wdpost 算法 (可选)，如果不配置 bin，则默认使用内置算法\nbin="~/my_algorithm"\nargs = ["window_post"]\n# 限制子进程可使用的 cpu\ncgroup.cpuset = "10-19"\n# 配置自定义算法的环境变量 (可选)\nenvs = { BELLMAN_GPU_INDEXS="0", CUDA_VISIBLE_DEVICES="0", TMPDIR = "/tmp/worker-prover1/", ... }\n# 配置本进程最大并发数量 (可选)，不配置则不限制\nconcurrent = 1\n\n\n# 一份最简的只启动一个 wdpost sealing_thread 的配置如下：\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-USA-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\n# 尝试领取任务的时间间隔，默认为 60s，\n# 针对 wdpost plan 我们可以调小此值便于更快的领取到新的 wdpost 任务\nsealing.recover_interval = "15s"\n# sealing.allowed_miners = [6666]\n# sealing.allowed_sizes = ["32GiB"]\n# ...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n\n# 单机单卡同时做 2 个 wdpost 的配置示例\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-USA-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [6666]\n# ...\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [7777]\n# ...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n[[attached]]\nname = "miner-7777-store"\nlocation = "/mnt/miner-7777-store"\n\n# -------------------------\n\n[[processors.window_post]]\nbin="~/my_algorithm"\n# args = ["window_post", ...]\ncgroup.cpuset = "10-19"\nenvs = { CUDA_VISIBLE_DEVICES="0", TMPDIR = "/tmp/worker-prover1/", ... }\nconcurrent = 1\n\n[[processors.window_post]]\nbin="~/my_algorithm"\n# args = ["window_post", ...]\ncgroup.cpuset = "20-29"\nenvs = { CUDA_VISIBLE_DEVICES="0", TMPDIR = "/tmp/worker-prover1/", ... }\nconcurrent = 1\n\n\n单卡同时做 2 个 wdpost 的意义在于因为计算 vanilla_proofs 时不使用显卡，两个 wdpost 任务的 vanilla_proofs 可以并行计算，而计算 snark_proof 时利用 TMPDIR 中的 gpu 锁文件串行计算。可以大幅度提高 GPU 的利用率。\n\n# 单机两张显卡同时做 wdpost 的配置示例\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-USA-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [6666]\n# ...\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [7777]\n# ...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n[[attached]]\nname = "miner-7777-store"\nlocation = "/mnt/miner-7777-store"\n\n# -------------------------\n\n[[processors.window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post", ...]\nenvs = { ... }\nconcurrent = 2\n\n# ----------- 或者 ---------\n\n#[[processors.window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post", ...]\n# cgroup.cpuset = "10-19"\n# envs = { CUDA_VISIBLE_DEVICES="0", TMPDIR = "/tmp/worker-prover1/", ... }\n# concurrent = 1\n\n# [[processors.window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post"]\n# cgroup.cpuset = "20-29"\n# envs = { CUDA_VISIBLE_DEVICES="1", TMPDIR = "/tmp/worker-prover2/", ... }\n# concurrent = 1\n\n\ndamocles-worker 在运行 wdpost plan 时不需要使用 damocles-worker store sealing-init -l 命令初始化封装过程中数据的本地存储目录。\n\n\n# 管理 window post 任务\n\n * # 显示 window post 任务列表\n\n# 默认显示未完成的任务和失败的任务， 其中 DDL 字段表示任务的 deadline Index, Try 字段是任务的尝试次数\n./damocles-manager util worker wdpost list\n\nJobID           MinerID  DDL Partitions  Worker        State       Try  CreateAt        Elapsed      Heartbeat  Error\n3FgfEnvrub1     1037     3   1,2         10.122.63.30  ReadyToRun  1    07-27 16:37:31  -            -\ngbCVH4TUgEf     1037     2   1,2                       ReadyToRun  0    07-27 16:35:56  -            -\nCrotWCLaXLa     1037     1   1,2         10.122.63.30  Succeed     1    07-27 17:19:04  6m38s(done)  -\n\n# 显示全部任务\n./damocles-manager util worker wdpost list --all\n# ...\n\n# 显示 window post 任务详细信息\n./damocles-manager util worker wdpost list --detail\n# ...\n\n\n * # 重置任务\n\n当 window post 任务执行失败且自动重试次数达到上限时，可以手动重置任务状态，使其可以继续被 damocles-worker 领取并执行。\n\n./damocles-manager util worker wdpost reset gbCVH4TUgEf 3FgfEnvrub1\n\n\n * # 删除任务\n\n删除任务和重置任务能达到的效果类似。当执行了删除任务的命令后，damocles-manager 的重试机制会检测当前 deadline 的 window post 任务是否存在于数据库中，如果不存在则会重新下发一遍任务，并记录到数据库中。\n\n另外 worker-prover 会自动的定时删除创建时间超过一定时间的任务 (默认为 25 小时，时间可配置)。\n\n# 删除指定的任务\n./damocles-manager util worker wdpost remove gbCVH4TUgEf 3FgfEnvrub1\n\n# 删除全部任务\n./damocles-manager util worker wdpost remove-all --really-do-it\n\n\n\n# 禁止 damocles-manager 使用 GPU\n\n启用 worker-prover 功能后，winning_post 由 damocles-manager 执行。如果不想让 winning_post 使用 GPU，可以使用如下命令编译 damocles-manager 禁止其使用 GPU。\n\nmake dist-clean\nFFI_BUILD_FROM_SOURCE=1 FFI_USE_GPU=0 make build-manager\n\n\n\n# 代理节点模式\n\n我们知道，对于 PoSter 节点来说，最重要的能力是获取实时、准确的扇区定位信息。在当前 damocles-manager 版本中，我们暂时仅提供基于本地内嵌式 kv 数据库的元数据管理方式。\n\n这使得数据仅能被一个进程管理，无法进行跨进程的直接数据共享。\n\n因此，我们设计了代理节点模式，将部分元数据通过网络接口提供给其他需要的节点，以此实现数据共享。\n\n\n# 代理节点的使用方式\n\n我们在 daemon run 命令中增加了 --proxy 参数。它的格式是 {ip}:{port} 这样的地址格式。当启动命令包含有效的 --proxy 参数时，节点将会以其指向的另一个 damocles-manager 节点作为数据源，并构造出必要的元数据（只读）管理模块。\n\n除了 --proxy 外，我们还提供了控制具体数据管理模块是否启用代理模式的开关。\n\n目前，我们暂时仅提供 --proxy-sector-indexer-off 这一个开关。当启用 --proxy-sector-indexer-off 时，节点会使用自己的数据目录下的 SectorIndexer 数据库。\n\n> 如果代理节点采用本地数据库且集群还在封装扇区时，不要启用 --proxy-sector-indexer-off，新封装的扇区元数据无法同步到代理节点的数据库，一旦 winningPoSt 选中了新封装扇区，无法正确计算证明，出块会失败！\n\n举例来说，如果按 damocles-manager daemon run --miner 命令启动，那么将会存在一个使用 ~/.damocles-manager 作为数据目录，监听 1789 端口的 damocles-manager 实例，且启用挖矿模块。\n\n这时，我们可以通过以下命令，在同一台机器上初始化并启动一个以上述实例作为数据源的代理节点，这个代理节点将会使用 ~/.damocles-manager2 作为数据目录，并监听 2789 端口。\n\ndamocles-manager --home ~/.damocles-manager2 daemon init\n// 维护配置文件\ndamocles-manager --home ~/.damocles-manager2 daemon run --proxy="127.0.0.1:1789" --listen=":2789" --poster\n\n\n代理节点能够提供与源节点完全一致且实时的扇区定位信息。\n\n\n# 代理节点使用已有配置文件\n\n按照上一节所述的方法，我们已经可以启动一个代理节点，但这种启动方式还存在一个问题：代理节点的配置文件需要再次编写一遍，或从源节点的数据目录中拷贝过来。这会带来额外的维护工作，尤其是在配置文件可能频繁发生变化的时候。\n\n为此，我们还提供了一个 --conf-dir 参数，它的格式是一个可用的目录路径。当启动命令包含有效的 --conf-dir 参数时，节点将会使用指定目录中已存在的配置文件作为自己的配置文件。\n\n这样，就可以省去编写、维护在同一台机器上的、为同一组集群提供服务的、不同源节点和代理节点的配置文件的工作。\n\n基于这个功能，上一节中所提到的代理节点启动方式可以变成：\n\ndamocles-manager --home ~/.damocles-manager2 daemon run --proxy="127.0.0.1:1789" --listen=":2789" --conf-dir="~/.damocles-manager" --poster\n\n\n此时，源节点和代理节点将会使用同一批配置文件。\n\n\n# ext-prover 执行器\n\n除了共享扇区信息之外，独立的 PoSter 节点面临的另一个挑战则是硬件资源的利用。\n\n受限于底层算法库，计算节点对于 GPU 的使用只能以进程为单位。这使得 PoSter 节点难以有效发挥多块 GPU 的计算能力，也难以在多个 SP 存在 WindostPoSt 证明窗口期冲突的情况下，安全地避免证明超时。\n\n为此，我们提供了类似 damocles-worker 中 ext processor 的 ext-prover 机制。\n\next-prover 机制包含两个组成部分：\n\n 1. daemon run 命令的 --ext-prover 参数\n 2. 节点数据目录中的 ext-prover.cfg 配置文件\n\n一个默认的 ext-prover.cfg 文件形如：\n\n# Default config:\n#[[WdPost]]\n#Bin = "/path/to/custom/bin"\n#Args = ["args1", "args2", "args3"]\n#Concurrent = 1\n#Weight = 1\n#ReadyTimeoutSecs = 5\n#[WdPost.Envs]\n#ENV_KEY = "ENV_VAL"\n#\n#[[WinPost]]\n#Bin = "/path/to/custom/bin"\n#Args = ["args1", "args2", "args3"]\n#Concurrent = 1\n#Weight = 1\n#ReadyTimeoutSecs = 5\n#[WinPost.Envs]\n#ENV_KEY = "ENV_VAL"\n#\n\n\n在最新版本中，daemon init 会初始化 ext-prover.cfg 文件。\n\n使用者可以自行编写，或从一个由最新版本初始化的数据目录中拷贝相应文件到已存在的数据目中。\n\next-prover.cfg 中各配置项的作用方式与 damocles-worker 中的配置块极为类似，使用者可以查阅相应文档进行参考。\n\n当 damocles-manager 的启动命令中包含 --ext-prover 参数时，节点将使用配置目录中的 ext-prover.cfg 配置文件作为启动子进程的依据。对于这个配置文件，设置 --conf-dir 参数也会带来效果。\n\n使用者看到类似这样的日志时，就表明 ext-prover 已就绪。\n\n2022-04-27T19:15:00.441+0800    INFO    porver-ext      ext/prover.go:122       response loop start     {"pid": 24764, "ppid": 24732, "loop": "resp"}\n2022-04-27T19:15:00.441+0800    INFO    porver-ext      ext/prover.go:155       request loop start      {"pid": 24764, "ppid": 24732, "loop": "req"}\n2022-04-27T19:15:00.468+0800    INFO    processor-cmd   processor/processor.go:35       ready   {"pid": 24764, "ppid": 24732, "proc": "wdpost"}\n\n\n\n# 部署实践\n\n假设我们存在一台配置了 8 GPU 的节点机器，那么我们可以通过以下配置方式来提供更强的时空证明处理能力。\n\n 1. 配置并启动源节点\n    \n    damocles-manager daemon run --miner\n    \n    \n    此时，源节点只提供封装相关的功能和能力；\n\n 2. 配置 ext-prover.cfg 文件：\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "0"\n    TMPDIR = "/tmp/ext-prover0/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "1"\n    TMPDIR = "/tmp/ext-prover1/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "2"\n    TMPDIR = "/tmp/ext-prover2/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "3"\n    TMPDIR = "/tmp/ext-prover3/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "4"\n    TMPDIR = "/tmp/ext-prover4/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "5"\n    TMPDIR = "/tmp/ext-prover5/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "6"\n    TMPDIR = "/tmp/ext-prover6/"\n    \n    [[WdPost]]\n    [WdPost.Envs]\n    CUDA_VISIBLE_DEVICES = "7"\n    TMPDIR = "/tmp/ext-prover7/"\n    \n    \n\n 3. 初始化并启动独立 PoSter 节点\n    \n    damocles-manager --home=~/.damocles-individual-poster daemon init\n    damocles-manager --home=~/.damocles-individual-poster daemon run --proxy="127.0.0.1:1789" --poster --listen=":2789" --conf-dir="~/.damocles-manager" --ext-prover\n    \n\n这种部署方式下，\n\n * 源节点同时提供封装和挖矿的支持\n * 代理节点提供 WindowPoSt 的支持\n   * 代理节点启用 ext-prover，且每个子进程独立使用一块 GPU、一个计算锁目录\n\nwinning post 和 window post 之间不会因设备使用而形成冲突\n\n\n# 局限性\n\n当目前为止，我们已经讲解了独立 PoSter 节点依托的功能、原理和简单的使用范例。\n\n但是，这种模式对于超大规模的 SP 集群仍然有一些局限性，具体体现在：\n\n * 时空证明的调度、证明窗口期的严重冲突，仍然需要在一定程度依赖运维层面的调配；',normalizedContent:'# 独立运行的 poster 节点\n\n在早期版本中，虽然 damocles-manager 已经支持通过 daemon run 命令的 --poster、--miner 参数来选择是否启用相应的模块，但由于 post 证明过程与扇区定位信息的强关联，使得真正使用时，局限性比较大，且难以扩展。\n\n从 v0.2.0 版本起，我们提供了一系列的功能组合，使得易用、可扩展的独立 poster 节点成为大体量， 多矿工号 sp 的一种可选方案。\n\n以下，我们会介绍这些新的功能点，并提供一种通过这些功能完成独立 poster 节点部署的实践。后续文档都以开启 --poster 的节点作为示例，独立的 --miner 节点运作方式与之类似，不再单独阐述。\n\n----------------------------------------\n\n在 v0.8.0 版本中，damocles 支持三种方式独立运行 poster 节点，分别是 worker-prover 模式、代理节点模式、ext-prover 模式 (外部执行器模式)。\n\n\n# worker-prover 模式\n\nworker-prover 模式是 v0.8.0 新增的功能，特点是简单，可以非常轻松的支持多机 wdpost。\n\n\n# 基本原理\n\nworker-prover 模式利用 damocles-worker 计算 window post 证明，通过 rpc 的方式从 damocles-manager 获取 window post 任务和返回计算的结果。\n\ndamocles-worker 新增 wdpost planner 用于执行 window post 任务。\n\n# architecture\n\n                +-----------------------------------+\n                |     damocles-manager daemon       |\n                |     with --worker-prover flag     |\n                |                                   |\n                |        +-----------------+        |\n                |        |damocles-manager |        |\n                |        |  poster module  |        |\n                |        +-------+-^-------+        |\n                |           send | |recv            |\n                |                | |                |\n                |        +-------v-+-------+        |\n                |        |  worker-prover  |        |\n       +--------+--------\x3e      module     <--------+--------+\n       |        |        +--------^--------+        |        |\n       |        |                 |                 |        |\n       |        +-----------------+-----------------+        |\n       |                          |                          |\n-------+--------------------------+--------------------------+------------\n       |                          |                          |\n       | pull job                 | pull job                 | pull job\n       | push res                 | push res                 | push res\n       | by rpc                   | by rpc                   | by rpc\n       |                          |                          |\n+------+--------+         +-------+-------+           +------+--------+\n|damocles-worker|         |damocles-worker|           |damocles-worker|\n|wdpost planner |         |wdpost planner |  ...      |wdpost planner |\n+---------------+         +---------------+           +---------------+\n\n\n\n# damocles-manager 配置与启动\n\n新增配置：\n\n# ~/.damocles-manager/sector-manager.cfg\n\n# ...\n\n[common.proving.workerprover]\n# windowpost 任务的最大尝试次数，可选项，数字类型\n# 默认值为 2\n# 尝试次数超过 jobmaxtry 的 windowpost 任务只能通过手动 reset 的方式被重新执行\njobmaxtry = 2\n# windowpost 任务的心跳超时时间，可选项，时间字符串类型\n# 默认值为 15s\n# 超过此时间没有发送心跳的任务将会被设置为失败并重试\nheartbeattimeout = "15s"\n# windowpost 任务的过期时间，可选项，时间字符串类型\n# 默认值为 25h\n# 创建时间超过此时间的 windowpost 任务将会被删除\njoblifetime = "25h0m0s"\n\n# ...\n\n\n启动 damocles-manager 进程：\n\n# --miner flag 可选添加，表示启动 miner 模块用于执行 winningpost 并出块\n# --poster flag 必须添加，表示启动 windowpost 模块\n# --worker-prover 必须添加，表示使用 workerprover 模块执行 windowpost\n./damocles-manager daemon run --miner --poster --worker-prover\n\n\n\n# damocles-worker 配置\n\n配置解释：\n\n[[sealing_thread]]\n# 配置使用 wdpost plan\nplan = "wdpost"\n# 配置只允许执行指定矿工号的任务，为空则表示不限制\n# sealing.allowed_miners = [6666, 7777]\n# 配置只允许运行指定 size 的扇区的任务\n# allowed_sizes = ["32gib", "64gib"]\n\n[[attached]]\n# 配置此 worker 执行 window post 任务过程中会用到的永久存储\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n\n# 控制 window_post 任务并发 (可选)，不配置则不限制\n[processors.limitation.concurrent]\nwindow_post = 2\n\n[[processors.window_post]]\n# 使用自定义 wdpost 算法 (可选)，如果不配置 bin，则默认使用内置算法\nbin="~/my_algorithm"\nargs = ["window_post"]\n# 限制子进程可使用的 cpu\ncgroup.cpuset = "10-19"\n# 配置自定义算法的环境变量 (可选)\nenvs = { bellman_gpu_indexs="0", cuda_visible_devices="0", tmpdir = "/tmp/worker-prover1/", ... }\n# 配置本进程最大并发数量 (可选)，不配置则不限制\nconcurrent = 1\n\n\n# 一份最简的只启动一个 wdpost sealing_thread 的配置如下：\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-usa-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\n# 尝试领取任务的时间间隔，默认为 60s，\n# 针对 wdpost plan 我们可以调小此值便于更快的领取到新的 wdpost 任务\nsealing.recover_interval = "15s"\n# sealing.allowed_miners = [6666]\n# sealing.allowed_sizes = ["32gib"]\n# ...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n\n# 单机单卡同时做 2 个 wdpost 的配置示例\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-usa-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [6666]\n# ...\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [7777]\n# ...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n[[attached]]\nname = "miner-7777-store"\nlocation = "/mnt/miner-7777-store"\n\n# -------------------------\n\n[[processors.window_post]]\nbin="~/my_algorithm"\n# args = ["window_post", ...]\ncgroup.cpuset = "10-19"\nenvs = { cuda_visible_devices="0", tmpdir = "/tmp/worker-prover1/", ... }\nconcurrent = 1\n\n[[processors.window_post]]\nbin="~/my_algorithm"\n# args = ["window_post", ...]\ncgroup.cpuset = "20-29"\nenvs = { cuda_visible_devices="0", tmpdir = "/tmp/worker-prover1/", ... }\nconcurrent = 1\n\n\n单卡同时做 2 个 wdpost 的意义在于因为计算 vanilla_proofs 时不使用显卡，两个 wdpost 任务的 vanilla_proofs 可以并行计算，而计算 snark_proof 时利用 tmpdir 中的 gpu 锁文件串行计算。可以大幅度提高 gpu 的利用率。\n\n# 单机两张显卡同时做 wdpost 的配置示例\n\n# /path/to/your-damocles-worker-config.toml\n\n[worker]\nname = "damocles-worker-usa-01"\n\n[sector_manager]\nrpc_client.addr = "/ip4/your-damocles-manager-address-here/tcp/1789"\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [6666]\n# ...\n\n[[sealing_thread]]\nplan = "wdpost"\nsealing.recover_interval = "15s"\nsealing.allowed_miners = [7777]\n# ...\n\n[[attached]]\nname = "miner-6666-store"\nlocation = "/mnt/miner-6666-store"\n\n[[attached]]\nname = "miner-7777-store"\nlocation = "/mnt/miner-7777-store"\n\n# -------------------------\n\n[[processors.window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post", ...]\nenvs = { ... }\nconcurrent = 2\n\n# ----------- 或者 ---------\n\n#[[processors.window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post", ...]\n# cgroup.cpuset = "10-19"\n# envs = { cuda_visible_devices="0", tmpdir = "/tmp/worker-prover1/", ... }\n# concurrent = 1\n\n# [[processors.window_post]]\n# bin="~/my_algorithm"\n# args = ["window_post"]\n# cgroup.cpuset = "20-29"\n# envs = { cuda_visible_devices="1", tmpdir = "/tmp/worker-prover2/", ... }\n# concurrent = 1\n\n\ndamocles-worker 在运行 wdpost plan 时不需要使用 damocles-worker store sealing-init -l 命令初始化封装过程中数据的本地存储目录。\n\n\n# 管理 window post 任务\n\n * # 显示 window post 任务列表\n\n# 默认显示未完成的任务和失败的任务， 其中 ddl 字段表示任务的 deadline index, try 字段是任务的尝试次数\n./damocles-manager util worker wdpost list\n\njobid           minerid  ddl partitions  worker        state       try  createat        elapsed      heartbeat  error\n3fgfenvrub1     1037     3   1,2         10.122.63.30  readytorun  1    07-27 16:37:31  -            -\ngbcvh4tugef     1037     2   1,2                       readytorun  0    07-27 16:35:56  -            -\ncrotwclaxla     1037     1   1,2         10.122.63.30  succeed     1    07-27 17:19:04  6m38s(done)  -\n\n# 显示全部任务\n./damocles-manager util worker wdpost list --all\n# ...\n\n# 显示 window post 任务详细信息\n./damocles-manager util worker wdpost list --detail\n# ...\n\n\n * # 重置任务\n\n当 window post 任务执行失败且自动重试次数达到上限时，可以手动重置任务状态，使其可以继续被 damocles-worker 领取并执行。\n\n./damocles-manager util worker wdpost reset gbcvh4tugef 3fgfenvrub1\n\n\n * # 删除任务\n\n删除任务和重置任务能达到的效果类似。当执行了删除任务的命令后，damocles-manager 的重试机制会检测当前 deadline 的 window post 任务是否存在于数据库中，如果不存在则会重新下发一遍任务，并记录到数据库中。\n\n另外 worker-prover 会自动的定时删除创建时间超过一定时间的任务 (默认为 25 小时，时间可配置)。\n\n# 删除指定的任务\n./damocles-manager util worker wdpost remove gbcvh4tugef 3fgfenvrub1\n\n# 删除全部任务\n./damocles-manager util worker wdpost remove-all --really-do-it\n\n\n\n# 禁止 damocles-manager 使用 gpu\n\n启用 worker-prover 功能后，winning_post 由 damocles-manager 执行。如果不想让 winning_post 使用 gpu，可以使用如下命令编译 damocles-manager 禁止其使用 gpu。\n\nmake dist-clean\nffi_build_from_source=1 ffi_use_gpu=0 make build-manager\n\n\n\n# 代理节点模式\n\n我们知道，对于 poster 节点来说，最重要的能力是获取实时、准确的扇区定位信息。在当前 damocles-manager 版本中，我们暂时仅提供基于本地内嵌式 kv 数据库的元数据管理方式。\n\n这使得数据仅能被一个进程管理，无法进行跨进程的直接数据共享。\n\n因此，我们设计了代理节点模式，将部分元数据通过网络接口提供给其他需要的节点，以此实现数据共享。\n\n\n# 代理节点的使用方式\n\n我们在 daemon run 命令中增加了 --proxy 参数。它的格式是 {ip}:{port} 这样的地址格式。当启动命令包含有效的 --proxy 参数时，节点将会以其指向的另一个 damocles-manager 节点作为数据源，并构造出必要的元数据（只读）管理模块。\n\n除了 --proxy 外，我们还提供了控制具体数据管理模块是否启用代理模式的开关。\n\n目前，我们暂时仅提供 --proxy-sector-indexer-off 这一个开关。当启用 --proxy-sector-indexer-off 时，节点会使用自己的数据目录下的 sectorindexer 数据库。\n\n> 如果代理节点采用本地数据库且集群还在封装扇区时，不要启用 --proxy-sector-indexer-off，新封装的扇区元数据无法同步到代理节点的数据库，一旦 winningpost 选中了新封装扇区，无法正确计算证明，出块会失败！\n\n举例来说，如果按 damocles-manager daemon run --miner 命令启动，那么将会存在一个使用 ~/.damocles-manager 作为数据目录，监听 1789 端口的 damocles-manager 实例，且启用挖矿模块。\n\n这时，我们可以通过以下命令，在同一台机器上初始化并启动一个以上述实例作为数据源的代理节点，这个代理节点将会使用 ~/.damocles-manager2 作为数据目录，并监听 2789 端口。\n\ndamocles-manager --home ~/.damocles-manager2 daemon init\n// 维护配置文件\ndamocles-manager --home ~/.damocles-manager2 daemon run --proxy="127.0.0.1:1789" --listen=":2789" --poster\n\n\n代理节点能够提供与源节点完全一致且实时的扇区定位信息。\n\n\n# 代理节点使用已有配置文件\n\n按照上一节所述的方法，我们已经可以启动一个代理节点，但这种启动方式还存在一个问题：代理节点的配置文件需要再次编写一遍，或从源节点的数据目录中拷贝过来。这会带来额外的维护工作，尤其是在配置文件可能频繁发生变化的时候。\n\n为此，我们还提供了一个 --conf-dir 参数，它的格式是一个可用的目录路径。当启动命令包含有效的 --conf-dir 参数时，节点将会使用指定目录中已存在的配置文件作为自己的配置文件。\n\n这样，就可以省去编写、维护在同一台机器上的、为同一组集群提供服务的、不同源节点和代理节点的配置文件的工作。\n\n基于这个功能，上一节中所提到的代理节点启动方式可以变成：\n\ndamocles-manager --home ~/.damocles-manager2 daemon run --proxy="127.0.0.1:1789" --listen=":2789" --conf-dir="~/.damocles-manager" --poster\n\n\n此时，源节点和代理节点将会使用同一批配置文件。\n\n\n# ext-prover 执行器\n\n除了共享扇区信息之外，独立的 poster 节点面临的另一个挑战则是硬件资源的利用。\n\n受限于底层算法库，计算节点对于 gpu 的使用只能以进程为单位。这使得 poster 节点难以有效发挥多块 gpu 的计算能力，也难以在多个 sp 存在 windostpost 证明窗口期冲突的情况下，安全地避免证明超时。\n\n为此，我们提供了类似 damocles-worker 中 ext processor 的 ext-prover 机制。\n\next-prover 机制包含两个组成部分：\n\n 1. daemon run 命令的 --ext-prover 参数\n 2. 节点数据目录中的 ext-prover.cfg 配置文件\n\n一个默认的 ext-prover.cfg 文件形如：\n\n# default config:\n#[[wdpost]]\n#bin = "/path/to/custom/bin"\n#args = ["args1", "args2", "args3"]\n#concurrent = 1\n#weight = 1\n#readytimeoutsecs = 5\n#[wdpost.envs]\n#env_key = "env_val"\n#\n#[[winpost]]\n#bin = "/path/to/custom/bin"\n#args = ["args1", "args2", "args3"]\n#concurrent = 1\n#weight = 1\n#readytimeoutsecs = 5\n#[winpost.envs]\n#env_key = "env_val"\n#\n\n\n在最新版本中，daemon init 会初始化 ext-prover.cfg 文件。\n\n使用者可以自行编写，或从一个由最新版本初始化的数据目录中拷贝相应文件到已存在的数据目中。\n\next-prover.cfg 中各配置项的作用方式与 damocles-worker 中的配置块极为类似，使用者可以查阅相应文档进行参考。\n\n当 damocles-manager 的启动命令中包含 --ext-prover 参数时，节点将使用配置目录中的 ext-prover.cfg 配置文件作为启动子进程的依据。对于这个配置文件，设置 --conf-dir 参数也会带来效果。\n\n使用者看到类似这样的日志时，就表明 ext-prover 已就绪。\n\n2022-04-27t19:15:00.441+0800    info    porver-ext      ext/prover.go:122       response loop start     {"pid": 24764, "ppid": 24732, "loop": "resp"}\n2022-04-27t19:15:00.441+0800    info    porver-ext      ext/prover.go:155       request loop start      {"pid": 24764, "ppid": 24732, "loop": "req"}\n2022-04-27t19:15:00.468+0800    info    processor-cmd   processor/processor.go:35       ready   {"pid": 24764, "ppid": 24732, "proc": "wdpost"}\n\n\n\n# 部署实践\n\n假设我们存在一台配置了 8 gpu 的节点机器，那么我们可以通过以下配置方式来提供更强的时空证明处理能力。\n\n 1. 配置并启动源节点\n    \n    damocles-manager daemon run --miner\n    \n    \n    此时，源节点只提供封装相关的功能和能力；\n\n 2. 配置 ext-prover.cfg 文件：\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "0"\n    tmpdir = "/tmp/ext-prover0/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "1"\n    tmpdir = "/tmp/ext-prover1/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "2"\n    tmpdir = "/tmp/ext-prover2/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "3"\n    tmpdir = "/tmp/ext-prover3/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "4"\n    tmpdir = "/tmp/ext-prover4/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "5"\n    tmpdir = "/tmp/ext-prover5/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "6"\n    tmpdir = "/tmp/ext-prover6/"\n    \n    [[wdpost]]\n    [wdpost.envs]\n    cuda_visible_devices = "7"\n    tmpdir = "/tmp/ext-prover7/"\n    \n    \n\n 3. 初始化并启动独立 poster 节点\n    \n    damocles-manager --home=~/.damocles-individual-poster daemon init\n    damocles-manager --home=~/.damocles-individual-poster daemon run --proxy="127.0.0.1:1789" --poster --listen=":2789" --conf-dir="~/.damocles-manager" --ext-prover\n    \n\n这种部署方式下，\n\n * 源节点同时提供封装和挖矿的支持\n * 代理节点提供 windowpost 的支持\n   * 代理节点启用 ext-prover，且每个子进程独立使用一块 gpu、一个计算锁目录\n\nwinning post 和 window post 之间不会因设备使用而形成冲突\n\n\n# 局限性\n\n当目前为止，我们已经讲解了独立 poster 节点依托的功能、原理和简单的使用范例。\n\n但是，这种模式对于超大规模的 sp 集群仍然有一些局限性，具体体现在：\n\n * 时空证明的调度、证明窗口期的严重冲突，仍然需要在一定程度依赖运维层面的调配；',charsets:{cjk:!0}},{title:"damocles-worker 外部执行器的配置范例",frontmatter:{},regularPath:"/zh/operation/processors-config-example.html",relativePath:"zh/operation/processors-config-example.md",key:"v-08acd132",path:"/zh/operation/processors-config-example.html",headers:[{level:2,title:"实例分析",slug:"实例分析",normalizedTitle:"实例分析",charIndex:136},{level:3,title:"硬件配置",slug:"硬件配置",normalizedTitle:"硬件配置",charIndex:145},{level:3,title:"配比方案",slug:"配比方案",normalizedTitle:"配比方案",charIndex:375},{level:2,title:"总结",slug:"总结",normalizedTitle:"总结",charIndex:4108}],headersStr:"实例分析 硬件配置 配比方案 总结",content:'# damocles-worker 外部执行器的配置范例\n\n在其他文档中，我们介绍了 processor 的基本概念，也分析了 damocles-worker 中关于 processor 的配置项。 接下来，我们以一个实际场景为例，来看看具体如何编排外部处理器。\n\n\n# 实例分析\n\n\n# 硬件配置\n\n我们针对的配置为：\n\n * 双路 AMD EPYC 7642 48-Core Processor\n * 2T 内存\n * GeForce RTX 3080 x 2\n * 足量的本地 NVME 数据盘\n\n这样，我们可使用的计算资源约为：\n\n * 96 个 CPU 物理核，以 3c/DIE 计，共 32 个可用的 pc1 multicore 组\n * 约 1.96TiB 可用物理内存\n * 2 块 GeForce RTX 3080 GPU\n\n\n# 配比方案\n\n考虑 32GiB CC 扇区的封装，我们存在两种可能的配比方案，以下分别阐述。 注意：以下的设计可以作为方案参考，但不宜直接用于生产环境。 使用者需要根据实际情况，以及试验阶段的产量情况进行调整。\n\n# 方案 A: 28 pc1，1 pc2, 1 c2，pc2 与 c2 各组独占 GPU\n\n这种方案的思路，是在满足消费能力的前提下，共用各一个的独立 pc2、c2 外部执行器。 其 processors 部分的配置如下：\n\n[processors.limitation.concurrent]\npc1 = 28\npc2 = 1\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-41"\nconcurrent = 14\nenvs = { FIL_PROOFS_MAXIMIZE_CACHING="1", FIL_PROOFS_USE_MULTICORE_SDR = "1", FIL_PROOFS_MULTICORE_SDR_PRODUCERS = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-89"\nconcurrent = 14\nenvs = { FIL_PROOFS_MAXIMIZE_CACHING="1", FIL_PROOFS_USE_MULTICORE_SDR = "1", FIL_PROOFS_MULTICORE_SDR_PRODUCERS = "1" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "2,5,8,11,50,53,56,59"\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.c2]]\ncgroup.cpuset =  "14,17,20,23,26,29,32,35,42-47,62,65,68,71,74,77,80,83,90-95"\nenvs = { CUDA_VISIBLE_DEVICES = "1" }\n\n\n在此配置下，共启动 2 个外部 pc1 执行器，1 个外部 pc2 执行器，1 个外部 c2 执行器：\n\n * 1 个 pc1 外部处理器，指定内存亲和 numa 0 区，使用 1 个主核 + 1 个 Producer 核的配置，分配位于 numa 0 区的 CPU 核 0-41；\n * 1 个 pc1 外部处理器，指定内存亲和 numa 1 区，使用 1 个主核 + 1 个 Producer 核的配置，分配位于 numa 1 区的 CPU 核 48-89；\n * 1 个 pc2 外部处理器，使用 CPU 核 2,5,8,11,50,53,56,59，指定可见序号为 0 的 GPU； 这种方法可行的原因是在当前的 pc1 配置下，每个 DIE 会空出一个核，可以用来执行轻量的计算任务。而 pc2 在使用 GPU 的情况下，CPU 基本只用于数据搬运；\n * 1 个 c2 外部处理器，使用 CPU 核 14,17,20,23,26,29,32,35,42-47,62,65,68,71,74,77,80,83,90-95，指定可见序号为 1 的 GPU；\n * 空余 CPU 核 38,41,86,89，可以用于其他轻量任务，如运维管理等。\n\n这种方案的优势是，pc2 和 c2 都各自独占资源，不会出现调度上的问题。\n\n# 方案 B: 28 pc1，2 pc2, 2 c2，每个 pc2 与 一个 c2 形成一组，每组占用 1 GPU，组内不同阶段的任务形成竞争关系\n\n这种方案的思路，实际上是将双路的硬件配置视作 2 组单路硬件的组合，对于每一个单路组执行一样的配置策略。 其 processors 部分的配置如下：\n\n\n[processors.limitation.concurrent]\npc1 = 28\npc2 = 2\nc2 = 2\n\n[processors.ext_locks]\ngpu1 = 1\ngpu2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-41"\nconcurrent = 14\nenvs = { FIL_PROOFS_MAXIMIZE_CACHING="1", FIL_PROOFS_USE_MULTICORE_SDR = "1", FIL_PROOFS_MULTICORE_SDR_PRODUCERS = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-89"\nconcurrent = 14\nenvs = { FIL_PROOFS_MAXIMIZE_CACHING="1", FIL_PROOFS_USE_MULTICORE_SDR = "1", FIL_PROOFS_MULTICORE_SDR_PRODUCERS = "1" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "2,5,8,11,14,17,20,23"\nlocks = ["gpu1"]\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "50,53,56,59,62,65,68,71"\nlocks = ["gpu2"]\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "1" }\n\n[[processors.c2]]\ncgroup.cpuset =  "2,5,8,11,14,17,20,23,26,29,32,35,42-47"\nlocks = ["gpu1"]\nenvs = { CUDA_VISIBLE_DEVICES = "0" }\n\n[[processors.c2]]\ncgroup.cpuset =  "50,53,56,59,62,65,68,71,74,77,80,83,90-95"\nlocks = ["gpu2"]\nenvs = { CUDA_VISIBLE_DEVICES = "1" }\n\n\n在此配置下，共启动 2 个外部 pc1 执行器，2 个外部 pc2 执行器，2 个外部 c2 执行器：\n\n * 1 个 pc1 外部处理器，指定内存亲和 numa 0 区，使用 1 个主核 + 1 个 Producer 核的配置，分配位于 numa 0 区的 CPU 核 0-41；\n * 1 个 pc1 外部处理器，指定内存亲和 numa 1 区，使用 1 个主核 + 1 个 Producer 核的配置，分配位于 numa 1 区的 CPU 核 48-89；\n * 1 个 pc2 与 1 个 c2 围绕自定义控制锁 gpu1 形成竞争关系，继而形成一个组，其中：\n   * 1 个 pc2，使用 CPU 核 2,5,8,11,14,17,20,23, 指定可见序号为 0 的 GPU；\n   * 1 个 c2，使用 CPU 核 2,5,8,11,14,17,20,23,26,29,32,35,42-47，指定可见序号为 0 的 GPU； 可以这样做的原因是，受限于自定义控制锁，本组内的 pc2 和 c2 处理器不会同时执行任务，因而可以共用部分 CPU 和 GPU 资源；\n * 1 个 pc2 与 1 个 c2 围绕自定义控制锁 gpu2 形成竞争关系，继而形成一个组，其中：\n   * 1 个 pc2，使用 CPU 核 50,53,56,59,62,65,68,71, 指定可见序号为 1 的 GPU；\n   * 1 个 c2，使用 CPU 核 50,53,56,59,62,65,68,71,74,77,80,83,90-95，指定可见序号为 1 的 GPU；\n * 空余 CPU 核 38,41,86,89，可以用于其他轻量任务，如运维管理等。\n\n相比方案 A，本方案可能存在一种极端情况：即控制锁始终被一种任务持续持有，而导致另一种任务长时间无法执行任务，导致扇区消费不通畅。 用通俗的话来说，可以类比为：GPU 长时间被用于执行 pc2，无法释放给 c2，导致等待 c2 资源的扇区堆积。\n\n\n# 总结\n\n本文档提供的，是 如何设计一个适合自身的配比方案，而非 一套适用一切场景的配比方案。 后续我们希望能提供更多的自动配置工具、计算器来简化使用者设计方案的过程，但同时，仍然建议使用者对方案中的关键环节有基本的理解。',normalizedContent:'# damocles-worker 外部执行器的配置范例\n\n在其他文档中，我们介绍了 processor 的基本概念，也分析了 damocles-worker 中关于 processor 的配置项。 接下来，我们以一个实际场景为例，来看看具体如何编排外部处理器。\n\n\n# 实例分析\n\n\n# 硬件配置\n\n我们针对的配置为：\n\n * 双路 amd epyc 7642 48-core processor\n * 2t 内存\n * geforce rtx 3080 x 2\n * 足量的本地 nvme 数据盘\n\n这样，我们可使用的计算资源约为：\n\n * 96 个 cpu 物理核，以 3c/die 计，共 32 个可用的 pc1 multicore 组\n * 约 1.96tib 可用物理内存\n * 2 块 geforce rtx 3080 gpu\n\n\n# 配比方案\n\n考虑 32gib cc 扇区的封装，我们存在两种可能的配比方案，以下分别阐述。 注意：以下的设计可以作为方案参考，但不宜直接用于生产环境。 使用者需要根据实际情况，以及试验阶段的产量情况进行调整。\n\n# 方案 a: 28 pc1，1 pc2, 1 c2，pc2 与 c2 各组独占 gpu\n\n这种方案的思路，是在满足消费能力的前提下，共用各一个的独立 pc2、c2 外部执行器。 其 processors 部分的配置如下：\n\n[processors.limitation.concurrent]\npc1 = 28\npc2 = 1\nc2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-41"\nconcurrent = 14\nenvs = { fil_proofs_maximize_caching="1", fil_proofs_use_multicore_sdr = "1", fil_proofs_multicore_sdr_producers = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-89"\nconcurrent = 14\nenvs = { fil_proofs_maximize_caching="1", fil_proofs_use_multicore_sdr = "1", fil_proofs_multicore_sdr_producers = "1" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "2,5,8,11,50,53,56,59"\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0" }\n\n[[processors.c2]]\ncgroup.cpuset =  "14,17,20,23,26,29,32,35,42-47,62,65,68,71,74,77,80,83,90-95"\nenvs = { cuda_visible_devices = "1" }\n\n\n在此配置下，共启动 2 个外部 pc1 执行器，1 个外部 pc2 执行器，1 个外部 c2 执行器：\n\n * 1 个 pc1 外部处理器，指定内存亲和 numa 0 区，使用 1 个主核 + 1 个 producer 核的配置，分配位于 numa 0 区的 cpu 核 0-41；\n * 1 个 pc1 外部处理器，指定内存亲和 numa 1 区，使用 1 个主核 + 1 个 producer 核的配置，分配位于 numa 1 区的 cpu 核 48-89；\n * 1 个 pc2 外部处理器，使用 cpu 核 2,5,8,11,50,53,56,59，指定可见序号为 0 的 gpu； 这种方法可行的原因是在当前的 pc1 配置下，每个 die 会空出一个核，可以用来执行轻量的计算任务。而 pc2 在使用 gpu 的情况下，cpu 基本只用于数据搬运；\n * 1 个 c2 外部处理器，使用 cpu 核 14,17,20,23,26,29,32,35,42-47,62,65,68,71,74,77,80,83,90-95，指定可见序号为 1 的 gpu；\n * 空余 cpu 核 38,41,86,89，可以用于其他轻量任务，如运维管理等。\n\n这种方案的优势是，pc2 和 c2 都各自独占资源，不会出现调度上的问题。\n\n# 方案 b: 28 pc1，2 pc2, 2 c2，每个 pc2 与 一个 c2 形成一组，每组占用 1 gpu，组内不同阶段的任务形成竞争关系\n\n这种方案的思路，实际上是将双路的硬件配置视作 2 组单路硬件的组合，对于每一个单路组执行一样的配置策略。 其 processors 部分的配置如下：\n\n\n[processors.limitation.concurrent]\npc1 = 28\npc2 = 2\nc2 = 2\n\n[processors.ext_locks]\ngpu1 = 1\ngpu2 = 1\n\n[[processors.pc1]]\nnuma_preferred = 0\ncgroup.cpuset = "0-41"\nconcurrent = 14\nenvs = { fil_proofs_maximize_caching="1", fil_proofs_use_multicore_sdr = "1", fil_proofs_multicore_sdr_producers = "1" }\n\n[[processors.pc1]]\nnuma_preferred = 1\ncgroup.cpuset = "48-89"\nconcurrent = 14\nenvs = { fil_proofs_maximize_caching="1", fil_proofs_use_multicore_sdr = "1", fil_proofs_multicore_sdr_producers = "1" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "2,5,8,11,14,17,20,23"\nlocks = ["gpu1"]\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0" }\n\n[[processors.pc2]]\ncgroup.cpuset =  "50,53,56,59,62,65,68,71"\nlocks = ["gpu2"]\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "1" }\n\n[[processors.c2]]\ncgroup.cpuset =  "2,5,8,11,14,17,20,23,26,29,32,35,42-47"\nlocks = ["gpu1"]\nenvs = { cuda_visible_devices = "0" }\n\n[[processors.c2]]\ncgroup.cpuset =  "50,53,56,59,62,65,68,71,74,77,80,83,90-95"\nlocks = ["gpu2"]\nenvs = { cuda_visible_devices = "1" }\n\n\n在此配置下，共启动 2 个外部 pc1 执行器，2 个外部 pc2 执行器，2 个外部 c2 执行器：\n\n * 1 个 pc1 外部处理器，指定内存亲和 numa 0 区，使用 1 个主核 + 1 个 producer 核的配置，分配位于 numa 0 区的 cpu 核 0-41；\n * 1 个 pc1 外部处理器，指定内存亲和 numa 1 区，使用 1 个主核 + 1 个 producer 核的配置，分配位于 numa 1 区的 cpu 核 48-89；\n * 1 个 pc2 与 1 个 c2 围绕自定义控制锁 gpu1 形成竞争关系，继而形成一个组，其中：\n   * 1 个 pc2，使用 cpu 核 2,5,8,11,14,17,20,23, 指定可见序号为 0 的 gpu；\n   * 1 个 c2，使用 cpu 核 2,5,8,11,14,17,20,23,26,29,32,35,42-47，指定可见序号为 0 的 gpu； 可以这样做的原因是，受限于自定义控制锁，本组内的 pc2 和 c2 处理器不会同时执行任务，因而可以共用部分 cpu 和 gpu 资源；\n * 1 个 pc2 与 1 个 c2 围绕自定义控制锁 gpu2 形成竞争关系，继而形成一个组，其中：\n   * 1 个 pc2，使用 cpu 核 50,53,56,59,62,65,68,71, 指定可见序号为 1 的 gpu；\n   * 1 个 c2，使用 cpu 核 50,53,56,59,62,65,68,71,74,77,80,83,90-95，指定可见序号为 1 的 gpu；\n * 空余 cpu 核 38,41,86,89，可以用于其他轻量任务，如运维管理等。\n\n相比方案 a，本方案可能存在一种极端情况：即控制锁始终被一种任务持续持有，而导致另一种任务长时间无法执行任务，导致扇区消费不通畅。 用通俗的话来说，可以类比为：gpu 长时间被用于执行 pc2，无法释放给 c2，导致等待 c2 资源的扇区堆积。\n\n\n# 总结\n\n本文档提供的，是 如何设计一个适合自身的配比方案，而非 一套适用一切场景的配比方案。 后续我们希望能提供更多的自动配置工具、计算器来简化使用者设计方案的过程，但同时，仍然建议使用者对方案中的关键环节有基本的理解。',charsets:{cjk:!0}},{title:"快速启用",frontmatter:{},regularPath:"/zh/operation/quick-start.html",relativePath:"zh/operation/quick-start.md",key:"v-dd9c338e",path:"/zh/operation/quick-start.html",headers:[{level:2,title:"准备工作",slug:"准备工作",normalizedTitle:"准备工作",charIndex:11},{level:2,title:"生产模式",slug:"生产模式",normalizedTitle:"生产模式",charIndex:507},{level:3,title:"damocles-manager",slug:"damocles-manager",normalizedTitle:"damocles-manager",charIndex:286},{level:3,title:"damocles-worker",slug:"damocles-worker",normalizedTitle:"damocles-worker",charIndex:268},{level:2,title:"Mock 模式 (开发人员使用)",slug:"mock-模式-开发人员使用",normalizedTitle:"mock 模式 (开发人员使用)",charIndex:1765},{level:3,title:"damocles-manager",slug:"damocles-manager-2",normalizedTitle:"damocles-manager",charIndex:286},{level:3,title:"damocles-worker",slug:"damocles-worker-2",normalizedTitle:"damocles-worker",charIndex:268}],headersStr:"准备工作 生产模式 damocles-manager damocles-worker Mock 模式 (开发人员使用) damocles-manager damocles-worker",content:"# 快速启用\n\n\n# 准备工作\n\n 1. 安装必要的第三方库。\n    \n    这一部分可以参考 lotus 文档中的相应部分 Software dependencies。\n\n 2. 下载代码库\n    \n    git clone https://github.com/ipfs-force-community/damocles.git\n    \n\n 3. 编译 damocles 的组件\n    \n    cd damocles\n    make all\n    \n    \n    完成后，在 ./dist/bin 目录下会有 damocles-worker 和 damocles-manager 两个可执行文件。\n\n 4. 分发可执行文件到需要的机器上。\n\n 5. 将 ./damocles-worker/create-cgroup.sh 分发到 damocles-worker 所在的机器上，并以准备运行 damocles-worker 的系统用户身份执行。\n    \n    这会为这样的用户生成相应的 cgroup 组，以便damocles-worker 为其外部执行器进程分配硬件资源。\n\n\n# 生产模式\n\n\n# damocles-manager\n\n 1. 初始化工作目录\n    \n    ./dist/bin/damocles-manager daemon init\n    \n\n 2. 按需配置默认配置文件 ~/.damocles-manager/sector-manager.cfg\n    \n    配置项、作用、配置方法可以参考文档 damocles-manager 的配置解析。\n\n 3. 创建矿工号（可选；如果已有，可略过此步骤）\n    \n    $ ./damocles-manager util miner create \\\n    --from=<OWNER_ADDRESS> \\\n    --owner=<OWNER_ADDRESS> \\\n    --worker=<WORKER_ADDRESS> \\\n    --sector-size=32GiB\n    \n    \n    会得到如下返回值。\n    \n    miner actor: f0xxx9 (f2drcv6746m5ehwxxxxxy)\t   \n    \n    \n    结果中的 miner actor f0xxx9 就是创建的 miner id。\n    \n    > 注意⚠️：--from 地址要保证有足够的余额，保证上链成功。\n\n 4. 启动 damocles-manager\n    \n    ./dist/bin/damocles-manager daemon run\n    \n    \n    > 注意⚠️：如果要开启 winningPost，配置文件的Miners.Proof.Enabled字段和启动的 flag --miner需要同时设置为 true。要开启 windowPost 的话，配置文件的Miners.PoSt.Enabled和启动 flag --poster也需要同时设置为 true。\n\n\n# damocles-worker\n\n 1. 创建 sealing_thread.location 父级目录，damocles-worker 在启动时会自动初始化每个 sealing_thread.location 目录。\n\n 2. (可选) 下载计算参数文件\n    \n    ./dist/bin/damocles-manager util fetch-params 512MiB\n    \n\n 3. (可选) 创建 NUMA 亲和的 hugepage 内存文件\n\n 4. 规划用于各阶段的 CPU 核、numa 区域等配置。\n    \n    按需完成配置文件。\n    \n    配置项、作用、配置方法可以参考文档 damocles-worker 的配置解析。\n\n 5. 启动 damocles-worker\n    \n    ./dist/bin/damocles-worker daemon -c /path/to/damocles-worker.toml\n    \n\n\n# Mock 模式 (开发人员使用)\n\n默认情况下，可以通过一系列命令在单机上启动一组 mock 实例。\n\n\n# damocles-manager\n\n通过\n\n./dist/bin/damocles-manager mock --miner=10000 --sector-size=2KiB\n\n\n命令启动一个模拟为 Actor 为 t010000 的 SP 分配 2KiB 扇区的 damocles-manager 服务。\n\n这一步骤也可以通过代码目录中的 ./mock/start_manager.sh 脚本完成。\n\n\n# damocles-worker\n\n 1. 创建并初始化本地存储，初始化远程存储\n    \n    ./dist/bin/damocles-worker store sealing-init -l ./mock-tmp/store1 ./mock-tmp/store2 ./mock-tmp/store3\n    ./dist/bin/damocles-worker store file-init -l ./mock-tmp/remote\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/cleanup_store.sh 脚本完成。\n\n 2. 以 mock 配置启动 damocles-worker\n    \n    ./dist/bin/damocles-worker daemon -c ./damocles-worker/assets/damocles-worker.mock.toml\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/start_worker.sh 脚本完成。",normalizedContent:"# 快速启用\n\n\n# 准备工作\n\n 1. 安装必要的第三方库。\n    \n    这一部分可以参考 lotus 文档中的相应部分 software dependencies。\n\n 2. 下载代码库\n    \n    git clone https://github.com/ipfs-force-community/damocles.git\n    \n\n 3. 编译 damocles 的组件\n    \n    cd damocles\n    make all\n    \n    \n    完成后，在 ./dist/bin 目录下会有 damocles-worker 和 damocles-manager 两个可执行文件。\n\n 4. 分发可执行文件到需要的机器上。\n\n 5. 将 ./damocles-worker/create-cgroup.sh 分发到 damocles-worker 所在的机器上，并以准备运行 damocles-worker 的系统用户身份执行。\n    \n    这会为这样的用户生成相应的 cgroup 组，以便damocles-worker 为其外部执行器进程分配硬件资源。\n\n\n# 生产模式\n\n\n# damocles-manager\n\n 1. 初始化工作目录\n    \n    ./dist/bin/damocles-manager daemon init\n    \n\n 2. 按需配置默认配置文件 ~/.damocles-manager/sector-manager.cfg\n    \n    配置项、作用、配置方法可以参考文档 damocles-manager 的配置解析。\n\n 3. 创建矿工号（可选；如果已有，可略过此步骤）\n    \n    $ ./damocles-manager util miner create \\\n    --from=<owner_address> \\\n    --owner=<owner_address> \\\n    --worker=<worker_address> \\\n    --sector-size=32gib\n    \n    \n    会得到如下返回值。\n    \n    miner actor: f0xxx9 (f2drcv6746m5ehwxxxxxy)\t   \n    \n    \n    结果中的 miner actor f0xxx9 就是创建的 miner id。\n    \n    > 注意⚠️：--from 地址要保证有足够的余额，保证上链成功。\n\n 4. 启动 damocles-manager\n    \n    ./dist/bin/damocles-manager daemon run\n    \n    \n    > 注意⚠️：如果要开启 winningpost，配置文件的miners.proof.enabled字段和启动的 flag --miner需要同时设置为 true。要开启 windowpost 的话，配置文件的miners.post.enabled和启动 flag --poster也需要同时设置为 true。\n\n\n# damocles-worker\n\n 1. 创建 sealing_thread.location 父级目录，damocles-worker 在启动时会自动初始化每个 sealing_thread.location 目录。\n\n 2. (可选) 下载计算参数文件\n    \n    ./dist/bin/damocles-manager util fetch-params 512mib\n    \n\n 3. (可选) 创建 numa 亲和的 hugepage 内存文件\n\n 4. 规划用于各阶段的 cpu 核、numa 区域等配置。\n    \n    按需完成配置文件。\n    \n    配置项、作用、配置方法可以参考文档 damocles-worker 的配置解析。\n\n 5. 启动 damocles-worker\n    \n    ./dist/bin/damocles-worker daemon -c /path/to/damocles-worker.toml\n    \n\n\n# mock 模式 (开发人员使用)\n\n默认情况下，可以通过一系列命令在单机上启动一组 mock 实例。\n\n\n# damocles-manager\n\n通过\n\n./dist/bin/damocles-manager mock --miner=10000 --sector-size=2kib\n\n\n命令启动一个模拟为 actor 为 t010000 的 sp 分配 2kib 扇区的 damocles-manager 服务。\n\n这一步骤也可以通过代码目录中的 ./mock/start_manager.sh 脚本完成。\n\n\n# damocles-worker\n\n 1. 创建并初始化本地存储，初始化远程存储\n    \n    ./dist/bin/damocles-worker store sealing-init -l ./mock-tmp/store1 ./mock-tmp/store2 ./mock-tmp/store3\n    ./dist/bin/damocles-worker store file-init -l ./mock-tmp/remote\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/cleanup_store.sh 脚本完成。\n\n 2. 以 mock 配置启动 damocles-worker\n    \n    ./dist/bin/damocles-worker daemon -c ./damocles-worker/assets/damocles-worker.mock.toml\n    \n    \n    这一步骤也可以通过代码目录中的 ./mock/start_worker.sh 脚本完成。",charsets:{cjk:!0}},{title:"扇区重建的支持",frontmatter:{},regularPath:"/zh/operation/sector-rebuild.html",relativePath:"zh/operation/sector-rebuild.md",key:"v-e5c0bb42",path:"/zh/operation/sector-rebuild.html",headers:[{level:2,title:"扇区重建的支持",slug:"扇区重建的支持",normalizedTitle:"扇区重建的支持",charIndex:2},{level:3,title:"基本原理",slug:"基本原理",normalizedTitle:"基本原理",charIndex:63},{level:3,title:"准备工作",slug:"准备工作",normalizedTitle:"准备工作",charIndex:223},{level:3,title:"开始重建",slug:"开始重建",normalizedTitle:"开始重建",charIndex:1411},{level:3,title:"其他相关命令",slug:"其他相关命令",normalizedTitle:"其他相关命令",charIndex:2036}],headersStr:"扇区重建的支持 基本原理 准备工作 开始重建 其他相关命令",content:'# 扇区重建的支持\n\n当扇区数据所在的磁盘发生损坏或者其他原因导致扇区数据丢失时，可以通过扇区重建功能恢复扇区数据。\n\n\n# 基本原理\n\ndamocles-manager 中保存了扇区的 piece 文件信息和申请的随机数等信息。扇区重建时 damocles-manager 会下发这些信息给 damocles-worker, 再由 damocles-worker 执行封装流程。\n\n大致流程参考：重建（rebuild）任务的状态流转。\n\n\n# 准备工作\n\n# 1. 准备需要重建的扇区号和矿工号\n\n# 2. 更改 damocles-worker 配置\n\n有两种更改配置的方法：\n\n第一种：更改 damocles-worker 主配置文件，增加一个 plan 为 rebuild 的 sealing_thread, 或将原有的一个 sealing_thread 的 plan 更改为 rebuild。这种方式需要重启 damocles-worker。示例如下：\n\n# /path/to/your-damocles-worker-config.toml\n\n# ...\n[[sealing_thread]]\nlocation = "/path/to/your_sealing_thread_location/"\nplan = "rebuild"\n# ...\n\n\n第二种：创建 sealing_thread 热更新配置文件。在 sealing_thread 的 location 目录下创建名为 config.toml 的文件。文件内容如下：\n\n# /path/to/your_sealing_thread_location/config.toml\n\nplan = "rebuild"\n\n\ndamocles-worker 会在扇区任务开始前加载热更新配置文件。具体参考：sealing_thread 配置热更新\n\n# 3. 检查 sealing_thread 状态\n\n执行：\n\ndamocles-worker worker list\n\n\noutput:\n\n#0: "/path/to/your_sealing_thread_location"; plan=rebuild, sector_id=None, paused=false, paused_elapsed=None, state=Empty, last_err=None\n// ...\n\n\n或者执行：\n\ndamocles-manager util worker info  <worker instance name or address>\n\n\noutput:\n\nIndex  Loc                                    Plan     SectorID       Paused  PausedElapsed  State      LastErr\n0      /path/to/your_sealing_thread_location  rebuild  NULL           false   NULL           Empty      NULL\n// ...\n\n\n如果显示 plan 为 rebuild 则说明配置改更成功。(注意：热更新配置文件可能不会立刻生效，需要等待扇区任务重新开始才会加载它)\n\n\n# 开始重建\n\n# 1. 创建扇区重建任务\n\n命令：\n\ndamocles-manager util sealer sectors rebuild <miner actor> <sector number>\n\n\n其中 <miner actor> 为矿工号， <sector number> 为需要重建的扇区号。\n\n我们以矿工号 1001, 需要重建的扇区号 123 为例。执行：\n\ndamocles-manager util sealer sectors rebuild 1001 123\n\n\n# 2. 观察 rebuild 的 sealing_thread 封装状态，等待重建完成\n\n执行：\n\ndamocles-worker worker list\n\n\n或者：\n\ndamocles-manager util worker info  <worker instance name or address>\n\n\n观察 state 和 last_err 字段信息。state 字段对应 重建（rebuild）任务的状态流转 中的 state。\n\n或者直接查看扇区状态信息：\n\n// 查询未完成的扇区\ndamocles-manager util sealer sectors state 1001 123\n\n// 查询完成的扇区\ndamocles-manager util sealer sectors state --offline 1001 123\n\n\n\n# 其他相关命令\n\n# 查询所有进行中的重建扇区的信息\n\ndamocles-manager util sealer sectors list --rebuild --sealing=false\n\n\n# 查询所有已完成的重建扇区的信息\n\ndamocles-manager util sealer sectors list --offline --rebuild --sealing=false\n',normalizedContent:'# 扇区重建的支持\n\n当扇区数据所在的磁盘发生损坏或者其他原因导致扇区数据丢失时，可以通过扇区重建功能恢复扇区数据。\n\n\n# 基本原理\n\ndamocles-manager 中保存了扇区的 piece 文件信息和申请的随机数等信息。扇区重建时 damocles-manager 会下发这些信息给 damocles-worker, 再由 damocles-worker 执行封装流程。\n\n大致流程参考：重建（rebuild）任务的状态流转。\n\n\n# 准备工作\n\n# 1. 准备需要重建的扇区号和矿工号\n\n# 2. 更改 damocles-worker 配置\n\n有两种更改配置的方法：\n\n第一种：更改 damocles-worker 主配置文件，增加一个 plan 为 rebuild 的 sealing_thread, 或将原有的一个 sealing_thread 的 plan 更改为 rebuild。这种方式需要重启 damocles-worker。示例如下：\n\n# /path/to/your-damocles-worker-config.toml\n\n# ...\n[[sealing_thread]]\nlocation = "/path/to/your_sealing_thread_location/"\nplan = "rebuild"\n# ...\n\n\n第二种：创建 sealing_thread 热更新配置文件。在 sealing_thread 的 location 目录下创建名为 config.toml 的文件。文件内容如下：\n\n# /path/to/your_sealing_thread_location/config.toml\n\nplan = "rebuild"\n\n\ndamocles-worker 会在扇区任务开始前加载热更新配置文件。具体参考：sealing_thread 配置热更新\n\n# 3. 检查 sealing_thread 状态\n\n执行：\n\ndamocles-worker worker list\n\n\noutput:\n\n#0: "/path/to/your_sealing_thread_location"; plan=rebuild, sector_id=none, paused=false, paused_elapsed=none, state=empty, last_err=none\n// ...\n\n\n或者执行：\n\ndamocles-manager util worker info  <worker instance name or address>\n\n\noutput:\n\nindex  loc                                    plan     sectorid       paused  pausedelapsed  state      lasterr\n0      /path/to/your_sealing_thread_location  rebuild  null           false   null           empty      null\n// ...\n\n\n如果显示 plan 为 rebuild 则说明配置改更成功。(注意：热更新配置文件可能不会立刻生效，需要等待扇区任务重新开始才会加载它)\n\n\n# 开始重建\n\n# 1. 创建扇区重建任务\n\n命令：\n\ndamocles-manager util sealer sectors rebuild <miner actor> <sector number>\n\n\n其中 <miner actor> 为矿工号， <sector number> 为需要重建的扇区号。\n\n我们以矿工号 1001, 需要重建的扇区号 123 为例。执行：\n\ndamocles-manager util sealer sectors rebuild 1001 123\n\n\n# 2. 观察 rebuild 的 sealing_thread 封装状态，等待重建完成\n\n执行：\n\ndamocles-worker worker list\n\n\n或者：\n\ndamocles-manager util worker info  <worker instance name or address>\n\n\n观察 state 和 last_err 字段信息。state 字段对应 重建（rebuild）任务的状态流转 中的 state。\n\n或者直接查看扇区状态信息：\n\n// 查询未完成的扇区\ndamocles-manager util sealer sectors state 1001 123\n\n// 查询完成的扇区\ndamocles-manager util sealer sectors state --offline 1001 123\n\n\n\n# 其他相关命令\n\n# 查询所有进行中的重建扇区的信息\n\ndamocles-manager util sealer sectors list --rebuild --sealing=false\n\n\n# 查询所有已完成的重建扇区的信息\n\ndamocles-manager util sealer sectors list --offline --rebuild --sealing=false\n',charsets:{cjk:!0}},{title:"SnapDeal 的支持",frontmatter:{},regularPath:"/zh/operation/snapup.html",relativePath:"zh/operation/snapup.md",key:"v-592d247f",path:"/zh/operation/snapup.html",headers:[{level:2,title:"SnapDeal 简述",slug:"snapdeal-简述",normalizedTitle:"snapdeal 简述",charIndex:19},{level:2,title:"damocles 对 SnapDeal 的支持",slug:"damocles-对-snapdeal-的支持",normalizedTitle:"damocles 对 snapdeal 的支持",charIndex:299},{level:2,title:"示例",slug:"示例",normalizedTitle:"示例",charIndex:616},{level:3,title:"候选扇区的导入",slug:"候选扇区的导入",normalizedTitle:"候选扇区的导入",charIndex:589},{level:3,title:"观察候选扇区的余量",slug:"观察候选扇区的余量",normalizedTitle:"观察候选扇区的余量",charIndex:1108},{level:3,title:"配置 damocles-worker",slug:"配置-damocles-worker",normalizedTitle:"配置 damocles-worker",charIndex:489},{level:3,title:"配置 damocles-manager",slug:"配置-damocles-manager",normalizedTitle:"配置 damocles-manager",charIndex:445},{level:3,title:"注意事项：",slug:"注意事项",normalizedTitle:"注意事项：",charIndex:3023},{level:2,title:"持续优化",slug:"持续优化",normalizedTitle:"持续优化",charIndex:3365}],headersStr:"SnapDeal 简述 damocles 对 SnapDeal 的支持 示例 候选扇区的导入 观察候选扇区的余量 配置 damocles-worker 配置 damocles-manager 注意事项： 持续优化",content:'# SnapDeal 的支持\n\n\n# SnapDeal 简述\n\nSnalDeal 是在 FIP-19 中提出，在 network15 上线的一种扇区升级方案。 相比之前的升级方案需要重新、完整完成一遍封装过程的巨大开销，SnalDeal 显得相当轻量，它的开销大约为：\n\n * 完成一次 add piece\n * 完成一次 tree d\n * 完成一次 snap_encode，其开销约等于一次 pc2\n * 完成一次 snap_prove，其开销约等于一次 c1 + c2 因此，无论是对于新增的真实数据存储需求，还是对存量 CC 扇区 进行转化，SnalDeal 都具备相当的吸引力。\n\n\n# damocles 对 SnapDeal 的支持\n\ndamocles 在设计之初就着眼于提供生产线模式的算力生产方案，为此我们提供了一种不太需要人工介入的 SnapDeal 生产方案，我们称之为 SnapUp。它的步骤大致如下：\n\n 1. 将已有的 CC 扇区 批量导入为本地候选扇区\n 2. 配置 damocles-manager，对指定 SP 启用 SnapUp 支持\n 3. 配置 damocles-worker，将已有的 sealing_thread 转化成 SnapUp 生产计划，或新增用于 SnapUp 的 sealing_thread 整个过程中，使用者仅需关注本地候选扇区的导入和余量，其余过程都会自动完成。\n\n\n# 示例\n\n下面以一套 butterfly 网络上的生产集群为例，逐步演示如何配置 SnapUp 的生产方案。\n\n\n# 候选扇区的导入\n\n使用新增的 util sealer snap fetch 工具，能够按 deadline 将满足限制（剩余生命周期大于 180 天，满足订单的最小生命周期）的 CC 扇区 导入为本地候选扇区。\n\n./dist/bin/damocles-manager util sealer snap fetch 1153 3\n2022-04-15T04:28:03.380Z        DEBUG   policy  policy/const.go:18      NETWORK SETUP   {"name": "butterfly"}\n2022-04-15T04:28:03.401Z        INFO    cmd     internal/util_sealer_snap.go:53 candidate sectors fetched        {"available-in-deadline": 2, "added": 2}\n\n\n\n# 观察候选扇区的余量\n\n./dist/bin/damocles-manager util sealer snap candidates 1153\n2022-04-15T04:28:13.955Z        DEBUG   policy  policy/const.go:18      NETWORK SETUP   {"name": "butterfly"}\ndeadline  count\n3         2\n\n\n可以看到，当前存在 2 个 #3 deadline 中的 CC 扇区 作为候选，可供升级\n\n\n# 配置 damocles-worker\n\ndamocles-worker 中需要配置的内容主要是用于 SnapUp 任务的 sealing_thread，和针对 snap_encode、snap_prove 的计算资源分配。\n\n示例如下：\n\n[[sealing_thread]]\nlocation = "/data/local-snap-1"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-2"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-3"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-4"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-5"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-6"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-6"\nplan = "snapup"\n\n[processors.limitation.concurrent]\n# ...\ntree_d = 1\nsnap_encode = 5\nsnap_prove = 2\n\n[[processors.snap_encode]]\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "0", TMPDIR="/var/tmp/worker0" }\n\n[[processors.snap_prove]]\nenvs = { CUDA_VISIBLE_DEVICES = "0" , TMPDIR="/var/tmp/worker0" }\n\n[[processors.snap_encode]]\nenvs = { FIL_PROOFS_USE_GPU_COLUMN_BUILDER = "1", FIL_PROOFS_USE_GPU_TREE_BUILDER = "1", CUDA_VISIBLE_DEVICES = "1", TMPDIR="/var/tmp/worker1" }\n\n[[processors.snap_prove]]\nenvs = { CUDA_VISIBLE_DEVICES = "1", TMPDIR="/var/tmp/worker1" }\n\n\nsnap_encode 的计算资源分配可以参考 pc2，snap_prove 的计算资源分配可以参考 c2\n\n\n# 配置 damocles-manager\n\ndamocles-manager 中需要的配置内容主要是为指定的 SP 启用 SnapUp，示例如下：\n\n[[Miners]]\nActor = 1153\n[Miners.Sector]\nInitNumber = 0\nMaxNumber = 10000\nEnabled = true\nEnableDeals = false\n\n[Miners.SnapUp]\nEnabled = true\nSender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n\n其中\n\n * [Miners.Sector] 块中的配置内容不会影响 SnapUp 的运转。\n * 在这套配置下，将可以支持：\n   * CC 扇区 持续生产\n   * SnapUp 在本地候选扇区有余量的情况下持续生产\n\n\n# 注意事项：\n\n 1. 考虑到 snap_encode 和 snap_prove 所需的计算资源，如果在同一个 damocles-worker 实例中同时启用常规扇区封装和 SnapUp 的话，可能需要考虑资源竞争的情况，可以参考 07.damocles-worker 外部执行器的配置范例\n 2. 考虑到扇区持久化数据的分布情况，用于 SnapUp 的 damocles-worker 需要同时能够以可读可写的方式访问所有持久化存储空间 (persist store)，且确保他们的命名和 damocles-manager 中一致。\n 3. 基于以上两点，我们推荐使用单独的设备专门进行 SnapUp 的生产，从而避免常规扇区和 SnapUp 混布带来的配置和运维的复杂度。\n\n\n# 持续优化\n\n对于 SnapUp 方案的完善和优化还在不断进行中，目前我们主要关注：\n\n * 将半自动的候选扇区导入转换成自动方式，或提供等效的运维工具\n * 更多候选扇区导入规则，如按存储配置导入\n * 上链消息的聚合，以降低成本\n * 其他能够简化运维、降低成本、提高效率的优化和工具',normalizedContent:'# snapdeal 的支持\n\n\n# snapdeal 简述\n\nsnaldeal 是在 fip-19 中提出，在 network15 上线的一种扇区升级方案。 相比之前的升级方案需要重新、完整完成一遍封装过程的巨大开销，snaldeal 显得相当轻量，它的开销大约为：\n\n * 完成一次 add piece\n * 完成一次 tree d\n * 完成一次 snap_encode，其开销约等于一次 pc2\n * 完成一次 snap_prove，其开销约等于一次 c1 + c2 因此，无论是对于新增的真实数据存储需求，还是对存量 cc 扇区 进行转化，snaldeal 都具备相当的吸引力。\n\n\n# damocles 对 snapdeal 的支持\n\ndamocles 在设计之初就着眼于提供生产线模式的算力生产方案，为此我们提供了一种不太需要人工介入的 snapdeal 生产方案，我们称之为 snapup。它的步骤大致如下：\n\n 1. 将已有的 cc 扇区 批量导入为本地候选扇区\n 2. 配置 damocles-manager，对指定 sp 启用 snapup 支持\n 3. 配置 damocles-worker，将已有的 sealing_thread 转化成 snapup 生产计划，或新增用于 snapup 的 sealing_thread 整个过程中，使用者仅需关注本地候选扇区的导入和余量，其余过程都会自动完成。\n\n\n# 示例\n\n下面以一套 butterfly 网络上的生产集群为例，逐步演示如何配置 snapup 的生产方案。\n\n\n# 候选扇区的导入\n\n使用新增的 util sealer snap fetch 工具，能够按 deadline 将满足限制（剩余生命周期大于 180 天，满足订单的最小生命周期）的 cc 扇区 导入为本地候选扇区。\n\n./dist/bin/damocles-manager util sealer snap fetch 1153 3\n2022-04-15t04:28:03.380z        debug   policy  policy/const.go:18      network setup   {"name": "butterfly"}\n2022-04-15t04:28:03.401z        info    cmd     internal/util_sealer_snap.go:53 candidate sectors fetched        {"available-in-deadline": 2, "added": 2}\n\n\n\n# 观察候选扇区的余量\n\n./dist/bin/damocles-manager util sealer snap candidates 1153\n2022-04-15t04:28:13.955z        debug   policy  policy/const.go:18      network setup   {"name": "butterfly"}\ndeadline  count\n3         2\n\n\n可以看到，当前存在 2 个 #3 deadline 中的 cc 扇区 作为候选，可供升级\n\n\n# 配置 damocles-worker\n\ndamocles-worker 中需要配置的内容主要是用于 snapup 任务的 sealing_thread，和针对 snap_encode、snap_prove 的计算资源分配。\n\n示例如下：\n\n[[sealing_thread]]\nlocation = "/data/local-snap-1"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-2"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-3"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-4"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-5"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-6"\nplan = "snapup"\n\n[[sealing_thread]]\nlocation = "/data/local-snap-6"\nplan = "snapup"\n\n[processors.limitation.concurrent]\n# ...\ntree_d = 1\nsnap_encode = 5\nsnap_prove = 2\n\n[[processors.snap_encode]]\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "0", tmpdir="/var/tmp/worker0" }\n\n[[processors.snap_prove]]\nenvs = { cuda_visible_devices = "0" , tmpdir="/var/tmp/worker0" }\n\n[[processors.snap_encode]]\nenvs = { fil_proofs_use_gpu_column_builder = "1", fil_proofs_use_gpu_tree_builder = "1", cuda_visible_devices = "1", tmpdir="/var/tmp/worker1" }\n\n[[processors.snap_prove]]\nenvs = { cuda_visible_devices = "1", tmpdir="/var/tmp/worker1" }\n\n\nsnap_encode 的计算资源分配可以参考 pc2，snap_prove 的计算资源分配可以参考 c2\n\n\n# 配置 damocles-manager\n\ndamocles-manager 中需要的配置内容主要是为指定的 sp 启用 snapup，示例如下：\n\n[[miners]]\nactor = 1153\n[miners.sector]\ninitnumber = 0\nmaxnumber = 10000\nenabled = true\nenabledeals = false\n\n[miners.snapup]\nenabled = true\nsender = "t1abjxfbp274xpdqcpuaykwkfb43omjotacm2p3za"\n\n\n其中\n\n * [miners.sector] 块中的配置内容不会影响 snapup 的运转。\n * 在这套配置下，将可以支持：\n   * cc 扇区 持续生产\n   * snapup 在本地候选扇区有余量的情况下持续生产\n\n\n# 注意事项：\n\n 1. 考虑到 snap_encode 和 snap_prove 所需的计算资源，如果在同一个 damocles-worker 实例中同时启用常规扇区封装和 snapup 的话，可能需要考虑资源竞争的情况，可以参考 07.damocles-worker 外部执行器的配置范例\n 2. 考虑到扇区持久化数据的分布情况，用于 snapup 的 damocles-worker 需要同时能够以可读可写的方式访问所有持久化存储空间 (persist store)，且确保他们的命名和 damocles-manager 中一致。\n 3. 基于以上两点，我们推荐使用单独的设备专门进行 snapup 的生产，从而避免常规扇区和 snapup 混布带来的配置和运维的复杂度。\n\n\n# 持续优化\n\n对于 snapup 方案的完善和优化还在不断进行中，目前我们主要关注：\n\n * 将半自动的候选扇区导入转换成自动方式，或提供等效的运维工具\n * 更多候选扇区导入规则，如按存储配置导入\n * 上链消息的聚合，以降低成本\n * 其他能够简化运维、降低成本、提高效率的优化和工具',charsets:{cjk:!0}},{title:"任务状态流转",frontmatter:{},regularPath:"/zh/operation/task-flow.html",relativePath:"zh/operation/task-flow.md",key:"v-12b90e11",path:"/zh/operation/task-flow.html",headers:[{level:2,title:"封装（sealer）任务的状态流转",slug:"封装-sealer-任务的状态流转",normalizedTitle:"封装（sealer）任务的状态流转",charIndex:688},{level:2,title:"升级（snapup）任务的状态流转",slug:"升级-snapup-任务的状态流转",normalizedTitle:"升级（snapup）任务的状态流转",charIndex:2360},{level:2,title:"重建（rebuild）任务的状态流转",slug:"重建-rebuild-任务的状态流转",normalizedTitle:"重建（rebuild）任务的状态流转",charIndex:3117},{level:2,title:"Unseal 任务的状态流转",slug:"unseal-任务的状态流转",normalizedTitle:"unseal 任务的状态流转",charIndex:4426},{level:2,title:"与 worker 管理工具结合的使用范例",slug:"与-worker-管理工具结合的使用范例",normalizedTitle:"与 worker 管理工具结合的使用范例",charIndex:4764}],headersStr:"封装（sealer）任务的状态流转 升级（snapup）任务的状态流转 重建（rebuild）任务的状态流转 Unseal 任务的状态流转 与 worker 管理工具结合的使用范例",content:"# 任务状态流转\n\n了解任务的状态流转，有助于使用者了解 damocles-worker 的状态，并对暂停的扇区任务进行有针对性的恢复。\n\n任务的状态流转，和任务的类型有关，即 sealing_thread 中的 plan 选项，因此我们将会分别描述。\n\n在下面的描述中，以 State:: 为 前缀出现的，是状态，以 Event:: 为前缀出现的，是事件或过程。类似\n\nState::A => {\n\tEvent::B => State::C,\n\tEvent::D => State::E,\n}\n\n\n表示：\n\n当任务处于 A 状态时\n\n 1. 如果出现 B 事件，则转向 C 状态\n 2. 如果出现 D 事件，则转向 E 状态\n\n同时还需要说明：\n\n 1. 每种状态，都可能发生向一个或多个其他状态的转换，即 {} 中可能出现一行或多行\n\n 2. 除了罗列出来的具体状态，还存在一些特殊状态，如：\n    \n    * State::Aborted，表示扇区未正常完成\n      \n      1. 任何逻辑判断中，属于 abort 级别的异常，都会使得任务转为 Aborted 状态，任务将被终止，当前 sealing_thread 将会转向下一任务\n      \n      2. 使用者向暂停中的任务发送恢复指令时，如果附带 Aborted 状态，也会实现上述效果\n         \n         因此使用者可以利用这一机制来处理难以恢复，但又尚未被定义为 abort 级别的异常\n    \n    * State::Finished 表示扇区正常完成\n\n\n# 封装（sealer）任务的状态流转\n\n// 空状态，即尚未分配扇区\nState::Empty => {\n\t// 申请新扇区\n\tEvent::Allocate(_) => State::Allocated,\n},\n\n// 新扇区已分配\nState::Allocated => {\n\t// 申请订单\n\tEvent::AcquireDeals(_) => State::DealsAcquired,\n},\n\n// 订单已申请\nState::DealsAcquired => {\n\t// 填充 piece 数据\n\tEvent::AddPiece(_) => State::PieceAdded,\n},\n\n// 数据已填充\nState::PieceAdded => {\n\t// 构造 TreeD\n\tEvent::BuildTreeD => State::TreeDBuilt,\n},\n\n// TreeD 已生成\nState::TreeDBuilt => {\n\t// 申请 pc1 所需的链上随机数\n\tEvent::AssignTicket(_) => State::TicketAssigned,\n},\n\n// pc1 所需的链上随机数已获得\nState::TicketAssigned => {\n\t// 执行 pc1\n\tEvent::PC1(_, _) => State::PC1Done,\n},\n\n// pc1 已完成\nState::PC1Done => {\n\t// 执行 pc2\n\tEvent::PC2(_) => State::PC2Done,\n},\n\n// pc2 已完成\nState::PC2Done => {\n\t// 提交 PreCommit 上链信息\n\tEvent::SubmitPC => State::PCSubmitted,\n},\n\n// PreCommit 上链信息已提交\nState::PCSubmitted => {\n\t// 上链失败，需要重新提交\n\tEvent::ReSubmitPC => State::PC2Done,\n\t// 上链成功\n\tEvent::CheckPC => State::PCLanded,\n},\n\n// PreCommit 信息已上链\nState::PCLanded => {\n\t// 执行扇区文件持久化\n\tEvent::Persist(_) => State::Persisted,\n},\n\n// 扇区文件已持久化\nState::Persisted => {\n\t// 通过持久化文件检查\n\tEvent::SubmitPersistance => State::PersistanceSubmitted,\n},\n\n// 持久化文件已确认\nState::PersistanceSubmitted => {\n\t// 申请 c1 所需的链上随机数\n\tEvent::AssignSeed(_) => State::SeedAssigned,\n},\n\n// c1 所需的链上随机数已获得\nState::SeedAssigned => {\n\t// 执行 c1\n\tEvent::C1(_) => State::C1Done,\n},\n\n// c1 已完成\nState::C1Done => {\n\t// 执行 c2\n\tEvent::C2(_) => State::C2Done,\n},\n\n// c2 已完成\nState::C2Done => {\n\t// 提交 CommitProof 信息\n\tEvent::SubmitProof => State::ProofSubmitted,\n},\n\n// CommitProof 信息已提交\nState::ProofSubmitted => {\n\t// 上链失败，需要重新提交 \n\tEvent::ReSubmitProof => State::C2Done,\n\t// 上链成功或跳过上链检查\n\tEvent::Finish => State::Finished,\n},\n\n\n\n# 升级（snapup）任务的状态流转\n\n// 空状态，即尚未分配扇区\nState::Empty => {\n\t// 分配用于升级的扇区和订单\n\tEvent::AllocatedSnapUpSector(_, _, _) => State::Allocated,\n},\n\n// 升级扇区和订单已分配\nState::Allocated => {\n\t// 填充 piece 数据\n\tEvent::AddPiece(_) => State::PieceAdded,\n},\n\n// 数据已填充\nState::PieceAdded => {\n\t// 构造 TreeD\n\tEvent::BuildTreeD => State::TreeDBuilt,\n},\n\n// TreeD 已生成\nState::TreeDBuilt => {\n\t// 执行 Snap 编码\n\tEvent::SnapEncode(_) => State::SnapEncoded,\n},\n\n// Snap 编码已完成\nState::SnapEncoded => {\n\t// 执行 Snap 证明\n\tEvent::SnapProve(_) => State::SnapProved,\n},\n\n// Snap 证明已生成\nState::SnapProved => {\n\t// 执行扇区文件持久化\n\tEvent::Persist(_) => State::Persisted,\n},\n\n// 扇区文件已持久化\nState::Persisted => {\n\t// 文件未通过检查\n\tEvent::RePersist => State::SnapProved,\n\t// 文件已通过检查\n\tEvent::Finish => State::Finished,\n},\n\n\n\n# 重建（rebuild）任务的状态流转\n\n// 空状态，即尚未分配扇区\nState::Empty => {\n\t// 分配用于重建的扇区信息\n\tEvent::AllocatedRebuildSector(_) => State::Allocated,\n},\n\n// 重建扇区已分配\nState::Allocated => {\n\t// 填充 piece 数据\n\tEvent::AddPiece(_) => State::PieceAdded,\n},\n\n// 数据已填充\nState::PieceAdded => {\n\t// 构造 TreeD\n\tEvent::BuildTreeD => State::TreeDBuilt,\n},\n\n// TreeD 已生成\nState::TreeDBuilt => {\n\t// 执行 pc1\n\tEvent::PC1(_, _) => State::PC1Done,\n},\n\n// pc1 已完成\nState::PC1Done => {\n\t// 执行 pc2\n\tEvent::PC2(_) => State::PC2Done,\n},\n\n// pc2 已完成\nState::PC2Done => {\n\t// 检查 sealed 文件（执行 c1）\n\tEvent::CheckSealed => State::SealedChecked,\n},\n\n// sealed 文件已检查（c1 已完成）\nState::SealedChecked => {\n\t// 非升级扇区，跳过 snapup 相关步骤\n\tEvent::SkipSnap => State::SnapDone,\n\t// 升级扇区，填充 piece 数据\n\tEvent::AddPiece(_) => State::SnapPieceAdded,\n},\n\n// 数据已填充（仅升级扇区）\nState::SnapPieceAdded => {\n\t// 构造 TreeD（仅升级扇区）\n\tEvent::BuildTreeD => State::SnapTreeDBuilt,\n},\n\n// TreeD 已生成（仅升级扇区）\nState::SnapTreeDBuilt => {\n\t// 执行 Snap 编码（仅升级扇区）\n\tEvent::SnapEncode(_) => State::SnapEncoded,\n},\n\n// Snap 编码已完成（仅升级扇区）\nState::SnapEncoded => {\n\t// 执行 Snap 证明（仅升级扇区）\n\tEvent::SnapProve(_) => State::SnapDone,\n},\n\n// snapup 已完成或不需要\nState::SnapDone => {\n\t// 执行扇区文件持久化\n\tEvent::Persist(_) => State::Persisted,\n},\n\n// 扇区文件已持久化\nState::Persisted => {\n\t// 通过持久化文件检查\n\tEvent::SubmitPersistance => State::Finished,\n},\n\n\n\n# Unseal 任务的状态流转\n\n// 空状态，即尚未分配扇区\nState::Empty => {\n\t// 分配用于 Unseal 任务\n\tEvent::AllocatedUnsealSector(_) => State::Allocated,\n},\n\n// Unseal 任务已分配\nState::Allocated => {\n\t// 执行 Unseal 算法\n\tEvent::UnsealDone(_) => State::Unsealed,\n},\n\n// unseal 算法执行完成\nState::Unsealed => {\n\t// 上传 piece 数据到指定位置\n\tEvent::UploadPieceDone => State::Finished,\n},\n\n\n\n# 与 worker 管理工具结合的使用范例\n\n# 1. 对于一个已经报错暂停，且无法恢复的扇区封装任务，如 ticket 已过期，可以通过\n\ndamocles-worker worker resume --state Aborted --index <index>\n\n\n或\n\ndamocles-manager util worker resume <worker instance name or address> <thread index> Aborted\n\n\n终止当前任务。\n\n# 2. 对于一个已经报错暂停，但认为可以重新从之前某个阶段开始重试的扇区封装任务，同样如 ticket 已过期，可以通过\n\ndamocles-worker worker resume --state TreeDBuilt --index <index>\n\n\n或\n\ndamocles-manager util worker resume <worker instance name or address> <thread index> TreeDBuilt\n\n\n令其回退到之前的状态进行尝试。",normalizedContent:"# 任务状态流转\n\n了解任务的状态流转，有助于使用者了解 damocles-worker 的状态，并对暂停的扇区任务进行有针对性的恢复。\n\n任务的状态流转，和任务的类型有关，即 sealing_thread 中的 plan 选项，因此我们将会分别描述。\n\n在下面的描述中，以 state:: 为 前缀出现的，是状态，以 event:: 为前缀出现的，是事件或过程。类似\n\nstate::a => {\n\tevent::b => state::c,\n\tevent::d => state::e,\n}\n\n\n表示：\n\n当任务处于 a 状态时\n\n 1. 如果出现 b 事件，则转向 c 状态\n 2. 如果出现 d 事件，则转向 e 状态\n\n同时还需要说明：\n\n 1. 每种状态，都可能发生向一个或多个其他状态的转换，即 {} 中可能出现一行或多行\n\n 2. 除了罗列出来的具体状态，还存在一些特殊状态，如：\n    \n    * state::aborted，表示扇区未正常完成\n      \n      1. 任何逻辑判断中，属于 abort 级别的异常，都会使得任务转为 aborted 状态，任务将被终止，当前 sealing_thread 将会转向下一任务\n      \n      2. 使用者向暂停中的任务发送恢复指令时，如果附带 aborted 状态，也会实现上述效果\n         \n         因此使用者可以利用这一机制来处理难以恢复，但又尚未被定义为 abort 级别的异常\n    \n    * state::finished 表示扇区正常完成\n\n\n# 封装（sealer）任务的状态流转\n\n// 空状态，即尚未分配扇区\nstate::empty => {\n\t// 申请新扇区\n\tevent::allocate(_) => state::allocated,\n},\n\n// 新扇区已分配\nstate::allocated => {\n\t// 申请订单\n\tevent::acquiredeals(_) => state::dealsacquired,\n},\n\n// 订单已申请\nstate::dealsacquired => {\n\t// 填充 piece 数据\n\tevent::addpiece(_) => state::pieceadded,\n},\n\n// 数据已填充\nstate::pieceadded => {\n\t// 构造 treed\n\tevent::buildtreed => state::treedbuilt,\n},\n\n// treed 已生成\nstate::treedbuilt => {\n\t// 申请 pc1 所需的链上随机数\n\tevent::assignticket(_) => state::ticketassigned,\n},\n\n// pc1 所需的链上随机数已获得\nstate::ticketassigned => {\n\t// 执行 pc1\n\tevent::pc1(_, _) => state::pc1done,\n},\n\n// pc1 已完成\nstate::pc1done => {\n\t// 执行 pc2\n\tevent::pc2(_) => state::pc2done,\n},\n\n// pc2 已完成\nstate::pc2done => {\n\t// 提交 precommit 上链信息\n\tevent::submitpc => state::pcsubmitted,\n},\n\n// precommit 上链信息已提交\nstate::pcsubmitted => {\n\t// 上链失败，需要重新提交\n\tevent::resubmitpc => state::pc2done,\n\t// 上链成功\n\tevent::checkpc => state::pclanded,\n},\n\n// precommit 信息已上链\nstate::pclanded => {\n\t// 执行扇区文件持久化\n\tevent::persist(_) => state::persisted,\n},\n\n// 扇区文件已持久化\nstate::persisted => {\n\t// 通过持久化文件检查\n\tevent::submitpersistance => state::persistancesubmitted,\n},\n\n// 持久化文件已确认\nstate::persistancesubmitted => {\n\t// 申请 c1 所需的链上随机数\n\tevent::assignseed(_) => state::seedassigned,\n},\n\n// c1 所需的链上随机数已获得\nstate::seedassigned => {\n\t// 执行 c1\n\tevent::c1(_) => state::c1done,\n},\n\n// c1 已完成\nstate::c1done => {\n\t// 执行 c2\n\tevent::c2(_) => state::c2done,\n},\n\n// c2 已完成\nstate::c2done => {\n\t// 提交 commitproof 信息\n\tevent::submitproof => state::proofsubmitted,\n},\n\n// commitproof 信息已提交\nstate::proofsubmitted => {\n\t// 上链失败，需要重新提交 \n\tevent::resubmitproof => state::c2done,\n\t// 上链成功或跳过上链检查\n\tevent::finish => state::finished,\n},\n\n\n\n# 升级（snapup）任务的状态流转\n\n// 空状态，即尚未分配扇区\nstate::empty => {\n\t// 分配用于升级的扇区和订单\n\tevent::allocatedsnapupsector(_, _, _) => state::allocated,\n},\n\n// 升级扇区和订单已分配\nstate::allocated => {\n\t// 填充 piece 数据\n\tevent::addpiece(_) => state::pieceadded,\n},\n\n// 数据已填充\nstate::pieceadded => {\n\t// 构造 treed\n\tevent::buildtreed => state::treedbuilt,\n},\n\n// treed 已生成\nstate::treedbuilt => {\n\t// 执行 snap 编码\n\tevent::snapencode(_) => state::snapencoded,\n},\n\n// snap 编码已完成\nstate::snapencoded => {\n\t// 执行 snap 证明\n\tevent::snapprove(_) => state::snapproved,\n},\n\n// snap 证明已生成\nstate::snapproved => {\n\t// 执行扇区文件持久化\n\tevent::persist(_) => state::persisted,\n},\n\n// 扇区文件已持久化\nstate::persisted => {\n\t// 文件未通过检查\n\tevent::repersist => state::snapproved,\n\t// 文件已通过检查\n\tevent::finish => state::finished,\n},\n\n\n\n# 重建（rebuild）任务的状态流转\n\n// 空状态，即尚未分配扇区\nstate::empty => {\n\t// 分配用于重建的扇区信息\n\tevent::allocatedrebuildsector(_) => state::allocated,\n},\n\n// 重建扇区已分配\nstate::allocated => {\n\t// 填充 piece 数据\n\tevent::addpiece(_) => state::pieceadded,\n},\n\n// 数据已填充\nstate::pieceadded => {\n\t// 构造 treed\n\tevent::buildtreed => state::treedbuilt,\n},\n\n// treed 已生成\nstate::treedbuilt => {\n\t// 执行 pc1\n\tevent::pc1(_, _) => state::pc1done,\n},\n\n// pc1 已完成\nstate::pc1done => {\n\t// 执行 pc2\n\tevent::pc2(_) => state::pc2done,\n},\n\n// pc2 已完成\nstate::pc2done => {\n\t// 检查 sealed 文件（执行 c1）\n\tevent::checksealed => state::sealedchecked,\n},\n\n// sealed 文件已检查（c1 已完成）\nstate::sealedchecked => {\n\t// 非升级扇区，跳过 snapup 相关步骤\n\tevent::skipsnap => state::snapdone,\n\t// 升级扇区，填充 piece 数据\n\tevent::addpiece(_) => state::snappieceadded,\n},\n\n// 数据已填充（仅升级扇区）\nstate::snappieceadded => {\n\t// 构造 treed（仅升级扇区）\n\tevent::buildtreed => state::snaptreedbuilt,\n},\n\n// treed 已生成（仅升级扇区）\nstate::snaptreedbuilt => {\n\t// 执行 snap 编码（仅升级扇区）\n\tevent::snapencode(_) => state::snapencoded,\n},\n\n// snap 编码已完成（仅升级扇区）\nstate::snapencoded => {\n\t// 执行 snap 证明（仅升级扇区）\n\tevent::snapprove(_) => state::snapdone,\n},\n\n// snapup 已完成或不需要\nstate::snapdone => {\n\t// 执行扇区文件持久化\n\tevent::persist(_) => state::persisted,\n},\n\n// 扇区文件已持久化\nstate::persisted => {\n\t// 通过持久化文件检查\n\tevent::submitpersistance => state::finished,\n},\n\n\n\n# unseal 任务的状态流转\n\n// 空状态，即尚未分配扇区\nstate::empty => {\n\t// 分配用于 unseal 任务\n\tevent::allocatedunsealsector(_) => state::allocated,\n},\n\n// unseal 任务已分配\nstate::allocated => {\n\t// 执行 unseal 算法\n\tevent::unsealdone(_) => state::unsealed,\n},\n\n// unseal 算法执行完成\nstate::unsealed => {\n\t// 上传 piece 数据到指定位置\n\tevent::uploadpiecedone => state::finished,\n},\n\n\n\n# 与 worker 管理工具结合的使用范例\n\n# 1. 对于一个已经报错暂停，且无法恢复的扇区封装任务，如 ticket 已过期，可以通过\n\ndamocles-worker worker resume --state aborted --index <index>\n\n\n或\n\ndamocles-manager util worker resume <worker instance name or address> <thread index> aborted\n\n\n终止当前任务。\n\n# 2. 对于一个已经报错暂停，但认为可以重新从之前某个阶段开始重试的扇区封装任务，同样如 ticket 已过期，可以通过\n\ndamocles-worker worker resume --state treedbuilt --index <index>\n\n\n或\n\ndamocles-manager util worker resume <worker instance name or address> <thread index> treedbuilt\n\n\n令其回退到之前的状态进行尝试。",charsets:{cjk:!0}},{frontmatter:{},regularPath:"/zh/operation/task-management.html",relativePath:"zh/operation/task-management.md",key:"v-355617e7",path:"/zh/operation/task-management.html",headersStr:null,content:"",normalizedContent:"",charsets:{}},{title:"Unseal 任务的支持",frontmatter:{},regularPath:"/zh/operation/unseal.html",relativePath:"zh/operation/unseal.md",key:"v-75a050c2",path:"/zh/operation/unseal.html",headers:[{level:2,title:"Unseal 任务的支持",slug:"unseal-任务的支持",normalizedTitle:"unseal 任务的支持",charIndex:2},{level:3,title:"概述",slug:"概述",normalizedTitle:"概述",charIndex:19},{level:3,title:"原理",slug:"原理",normalizedTitle:"原理",charIndex:387},{level:3,title:"启用",slug:"启用",normalizedTitle:"启用",charIndex:550},{level:3,title:"手动触发 unseal 任务",slug:"手动触发-unseal-任务",normalizedTitle:"手动触发 unseal 任务",charIndex:818}],headersStr:"Unseal 任务的支持 概述 原理 启用 手动触发 unseal 任务",content:'# Unseal 任务的支持\n\n\n# 概述\n\n订单检索是 Filecoin 生态闭环中的一个重要环节，当前 Droplet 会默认使用其 PieceStore 中的 Piece 数据响应检索订单的数据请求。同时用户也可以通过配置来控制 Droplet 缓存的 Piece 数据，当 Droplet 发现数据库中 没有该目标 Piece 的数据时，就会触发 unseal 任务，并向 Damocles 下发该任务。 Damocles Manager （下称：Manager ）在接收到该任务之后，会将其分配给支持 unseal plan 的 Damocles Worker （下称：Worker）。待 Worker 完成该任务之后，就会将 unseal 之后的文件上传到 unseal 任务中指定的位置（该位置，默认是 Droplet 的 PieceStore） 。\n\n\n# 原理\n\n订单在在封装完成之后会将 sealed file 留存在永久存储目录，用以应对时空证明和 unseal 任务。 Woker 在接收到 unseal 任务之后，会从 Manager 获取目标 Piece 数据 所在扇区的sealed file 和 元数据，然后执行 PC1 的逆向算法，还原 Piece 数据。\n\n\n# 启用\n\nDamocles 支持 unseal 任务 只需要 Worker 启用一个支持 unseal plan 的封装进程即可。 启用 unseal plan的封装进程可以有两种方式：\n\n 1. 直接修改 Woker 的主配置文件，新增一个 封装进程，或者直接修改 现有封装进程的 plan 等待适当的时机重启。\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store1"\nplan = "unseal"\n\n\n 1. 使用配置热更新的方式，添加或修改封装进程。详细参见 配置热更新章节\n\n\n# 手动触发 unseal 任务\n\n在 Piece 数据 意识或者某一些别的特殊情形下，我们可能会希望能够手动触发 unseal 任务，获取 Piece 数据。 这时我们就可以通过 Manager 提供的命令行工具来手动生成并触发 unseal 任务\n\ndamocles-manager util sealer sectors unseal\nNAME:\n   damocles-manager util sealer sectors unseal - unseal specified sector\n\nUSAGE:\n   damocles-manager util sealer sectors unseal command [command options] <piece_cid>\n\nCOMMANDS:\n   help, h  Shows a list of commands or help for one command\n\nOPTIONS:\n   --output value, -o value                        output piece as a car file to the specific path\n   --actor value, --miner value, --actor-id value  specify actor id of miner manully, it must worke with flag "--sector"  (default: 0)\n   --sector value, --sector-id value               specify sector number manully, it must worke with flag "--actor"  (default: 0)\n   --piece-info-from-droplet, --from-droplet       get piece info from droplet, which come from damocles db by default . (default: false)\n   --unseal-file value                             unseal piece from unseal file\n   --offset value                                  specify offset of piece manually (default: 0)\n   --size value                                    specify size of piece manually (default: 0)\n   --dest value                                    specify destination to transfer piece manually, there are five protocols can be used:"file:///path","http://" "https://", "market://store_name/piece_cid", "store://store_name/piece_cid"\n   --help, -h                                      show help (default: false)\n\n\n\n# unseal piece 简单使用\n\n用户可以直接通过 piece 数据的 cid (piececid) 来发布一个 unseal 任务:\n\ndamocles-manager util sealer sectors unseal <piece_cid>\n\n\n此时 Manager 会生成一个 unseal 任务。 此时 Manager 中能看到 unseal 任务相关的日志，形如：\n\nadd new dest to unseal task\n\n\n此时，程序会持续运行，直到 unseal 任务完成，会在当前目录生成一个 名字为 piece cid 的文件。\n\n# flag 解释\n\n# 指定不同方式获取 piece info\n\n还原 piece 数据需要获取 piece 数据在扇区中的位置和大小，unseal 任务 默认会从 Manager 数据库中获取 piece 数据的 offset 和 size 信息，但是有时候，数据库中的数据丢失或者不完整的时候，我们可能希望可以从别的地方获取这些参数或者可以手动指定这两个参数，这时候就可以用到以下三个 flag：\n\n   --from-droplet             get piece info from venus-market, which come from damocles db by default . (default: false)\n   --offset value            specify offset of piece manually (default: 0)\n   --size value              specify size of piece manually (default: 0)\n\n\n * from-droplet 从 Droplet 获取 offset 和 size，前提是 Manager 已经连接了 Droplet。\n * offset：piece 数据在 扇区中的 位置\n * size：Piece 数据的大小\n\n如果你才升级到 v0.7.0 之后的版本没多久，可能会存在数据库中数据不完整的情况（v0.7.0 之前，数据库不会记录 offset ）。\n\n# 指定 piece 数据输出的位置\n\n有时候我们可能希望指定 piece 数据输出的位置，可以使用 -o flag 指定输出位置\n\n   --output value, -o value  output piece as a car file to the specific path\n\n\n# 直接从 unseal 文件还原 piece 数据\n\nDamocles 默认从 sealed 文件 还原 piece 数据，但是如果用户留存了 unseal 文件, 可以直接从 unseal 文件 还原 piece 数据，这会节省大量的时间和资源，这时候可以使用 --unseal-file flag 指定 piece 对应的 unseal 文件 的路径。\n\n   --unseal-file value  unseal piece from unseal file\n\n\n# 通过 dest 协议将文件上传值目标位置\n\n默认情况下，以及加了-oflag 的情况下，Worker unseal 得到的 piece 数据会上传到 Manager ，由 Manager 将其输出到 Manager 所在机器的指定路径上。 但有时候，我们并不希望 Worker上传 piece 数据到 Manager ，而是直接上传到别的目标位置，这个时候就需要用到 --destflag。\n\ndest 协议支持通过以下四种方式指定上传 piece 数据的目标位置：\n\n * 将文件直接输出到 Worker 本地\n   * "file:///path"\n   * 注意，上述 url 中 host 的位置必须为空。\n * 网络位置\n   * "http://" "https://"\n * 上传至 Droplet 的 piece store\n   * "market://store_name/piece_cid"\n   * 其中 store_name 指的是 market 中 piece store 的名字\n * 上传至 Manager 的piece store\n   * "store://store_name/piece_cid"\n   * 其中 store_name 指的是 Manager 中 piece store 的名字\n   * 注意：应确保 Manager 的 piece store已经挂载并配置到 Worker 中\n\n因为从 unseal 文件 还原 piece 数据是不需要经过 Worker 的，所以这个时候指定 --dest flag 是无效的',normalizedContent:'# unseal 任务的支持\n\n\n# 概述\n\n订单检索是 filecoin 生态闭环中的一个重要环节，当前 droplet 会默认使用其 piecestore 中的 piece 数据响应检索订单的数据请求。同时用户也可以通过配置来控制 droplet 缓存的 piece 数据，当 droplet 发现数据库中 没有该目标 piece 的数据时，就会触发 unseal 任务，并向 damocles 下发该任务。 damocles manager （下称：manager ）在接收到该任务之后，会将其分配给支持 unseal plan 的 damocles worker （下称：worker）。待 worker 完成该任务之后，就会将 unseal 之后的文件上传到 unseal 任务中指定的位置（该位置，默认是 droplet 的 piecestore） 。\n\n\n# 原理\n\n订单在在封装完成之后会将 sealed file 留存在永久存储目录，用以应对时空证明和 unseal 任务。 woker 在接收到 unseal 任务之后，会从 manager 获取目标 piece 数据 所在扇区的sealed file 和 元数据，然后执行 pc1 的逆向算法，还原 piece 数据。\n\n\n# 启用\n\ndamocles 支持 unseal 任务 只需要 worker 启用一个支持 unseal plan 的封装进程即可。 启用 unseal plan的封装进程可以有两种方式：\n\n 1. 直接修改 woker 的主配置文件，新增一个 封装进程，或者直接修改 现有封装进程的 plan 等待适当的时机重启。\n\n[[sealing_thread]]\nlocation = "./mock-tmp/store1"\nplan = "unseal"\n\n\n 1. 使用配置热更新的方式，添加或修改封装进程。详细参见 配置热更新章节\n\n\n# 手动触发 unseal 任务\n\n在 piece 数据 意识或者某一些别的特殊情形下，我们可能会希望能够手动触发 unseal 任务，获取 piece 数据。 这时我们就可以通过 manager 提供的命令行工具来手动生成并触发 unseal 任务\n\ndamocles-manager util sealer sectors unseal\nname:\n   damocles-manager util sealer sectors unseal - unseal specified sector\n\nusage:\n   damocles-manager util sealer sectors unseal command [command options] <piece_cid>\n\ncommands:\n   help, h  shows a list of commands or help for one command\n\noptions:\n   --output value, -o value                        output piece as a car file to the specific path\n   --actor value, --miner value, --actor-id value  specify actor id of miner manully, it must worke with flag "--sector"  (default: 0)\n   --sector value, --sector-id value               specify sector number manully, it must worke with flag "--actor"  (default: 0)\n   --piece-info-from-droplet, --from-droplet       get piece info from droplet, which come from damocles db by default . (default: false)\n   --unseal-file value                             unseal piece from unseal file\n   --offset value                                  specify offset of piece manually (default: 0)\n   --size value                                    specify size of piece manually (default: 0)\n   --dest value                                    specify destination to transfer piece manually, there are five protocols can be used:"file:///path","http://" "https://", "market://store_name/piece_cid", "store://store_name/piece_cid"\n   --help, -h                                      show help (default: false)\n\n\n\n# unseal piece 简单使用\n\n用户可以直接通过 piece 数据的 cid (piececid) 来发布一个 unseal 任务:\n\ndamocles-manager util sealer sectors unseal <piece_cid>\n\n\n此时 manager 会生成一个 unseal 任务。 此时 manager 中能看到 unseal 任务相关的日志，形如：\n\nadd new dest to unseal task\n\n\n此时，程序会持续运行，直到 unseal 任务完成，会在当前目录生成一个 名字为 piece cid 的文件。\n\n# flag 解释\n\n# 指定不同方式获取 piece info\n\n还原 piece 数据需要获取 piece 数据在扇区中的位置和大小，unseal 任务 默认会从 manager 数据库中获取 piece 数据的 offset 和 size 信息，但是有时候，数据库中的数据丢失或者不完整的时候，我们可能希望可以从别的地方获取这些参数或者可以手动指定这两个参数，这时候就可以用到以下三个 flag：\n\n   --from-droplet             get piece info from venus-market, which come from damocles db by default . (default: false)\n   --offset value            specify offset of piece manually (default: 0)\n   --size value              specify size of piece manually (default: 0)\n\n\n * from-droplet 从 droplet 获取 offset 和 size，前提是 manager 已经连接了 droplet。\n * offset：piece 数据在 扇区中的 位置\n * size：piece 数据的大小\n\n如果你才升级到 v0.7.0 之后的版本没多久，可能会存在数据库中数据不完整的情况（v0.7.0 之前，数据库不会记录 offset ）。\n\n# 指定 piece 数据输出的位置\n\n有时候我们可能希望指定 piece 数据输出的位置，可以使用 -o flag 指定输出位置\n\n   --output value, -o value  output piece as a car file to the specific path\n\n\n# 直接从 unseal 文件还原 piece 数据\n\ndamocles 默认从 sealed 文件 还原 piece 数据，但是如果用户留存了 unseal 文件, 可以直接从 unseal 文件 还原 piece 数据，这会节省大量的时间和资源，这时候可以使用 --unseal-file flag 指定 piece 对应的 unseal 文件 的路径。\n\n   --unseal-file value  unseal piece from unseal file\n\n\n# 通过 dest 协议将文件上传值目标位置\n\n默认情况下，以及加了-oflag 的情况下，worker unseal 得到的 piece 数据会上传到 manager ，由 manager 将其输出到 manager 所在机器的指定路径上。 但有时候，我们并不希望 worker上传 piece 数据到 manager ，而是直接上传到别的目标位置，这个时候就需要用到 --destflag。\n\ndest 协议支持通过以下四种方式指定上传 piece 数据的目标位置：\n\n * 将文件直接输出到 worker 本地\n   * "file:///path"\n   * 注意，上述 url 中 host 的位置必须为空。\n * 网络位置\n   * "http://" "https://"\n * 上传至 droplet 的 piece store\n   * "market://store_name/piece_cid"\n   * 其中 store_name 指的是 market 中 piece store 的名字\n * 上传至 manager 的piece store\n   * "store://store_name/piece_cid"\n   * 其中 store_name 指的是 manager 中 piece store 的名字\n   * 注意：应确保 manager 的 piece store已经挂载并配置到 worker 中\n\n因为从 unseal 文件 还原 piece 数据是不需要经过 worker 的，所以这个时候指定 --dest flag 是无效的',charsets:{cjk:!0}},{title:"damocles-worker 配置指北",frontmatter:{},regularPath:"/zh/operation/worker-config-guide.html",relativePath:"zh/operation/worker-config-guide.md",key:"v-5a820f7d",path:"/zh/operation/worker-config-guide.html",headers:[{level:2,title:"sealing 任务各阶段资源消耗情况 (32 GiB 扇区)",slug:"sealing-任务各阶段资源消耗情况-32-gib-扇区",normalizedTitle:"sealing 任务各阶段资源消耗情况 (32 gib 扇区)",charIndex:27},{level:2,title:"damocles-worker-util 工具使用",slug:"damocles-worker-util-工具使用",normalizedTitle:"damocles-worker-util 工具使用",charIndex:2020},{level:3,title:"hwinfo",slug:"hwinfo",normalizedTitle:"hwinfo",charIndex:2107},{level:3,title:"sealcalc",slug:"sealcalc",normalizedTitle:"sealcalc",charIndex:2140}],headersStr:"sealing 任务各阶段资源消耗情况 (32 GiB 扇区) damocles-worker-util 工具使用 hwinfo sealcalc",content:'# damocles-worker 配置指北\n\n\n# sealing 任务各阶段资源消耗情况 (32 GiB 扇区)\n\n测试机型：\n\n * CPU: AMD EPYC 7642 (max MHz: 2300)\n * GPU: RTX 3080\n * Memory: DIMM DDR4 Synchronous Registered (Buffered) 2933 MHz (0.3 ns)\n\nSTAGE            并发数   耗时           内存        CPU                      GPU   DISKIO 读      DISKIO 写      备注\nWindowPoSt       1     ~4-10mins    ~120GiB   RAYON_NUM_THREADS*100%   ✓     TODO          -             \nWinningPoSt      1     ~1-10s       -         -                        -     -             -             \nAddPieces        1     ~3mins       ~210MiB   RAYON_NUM_THREADS*100%   x     <=32GiB       32GiB         \nTreeD            1     ~1min        ~47GiB    RAYON_NUM_THREADS*100%   x     32GiB         64GiB         64 GiB 扇区 ~60GiB RAM\nPC1              1     ~177mins     <=64GiB   150%                     x     -             352GiB        \nPC2              1     ~10-13mins   ~64GiB    RAYON_NUM_THREADS*100%   ✓     384GiB        ~37GiB        \nSupraPC2         1     ~2-5mins     ~40GiB    ~400%                    ✓     384GiB        ~37GiB        \nWaitSeed         -     75mins       -         -                        x     -             -             \nC1               1     ~1s          -         -                        x     -             -             \nC2               1     ~13-16mins   TODO      RAYON_NUM_THREADS*100%   ✓     -             -             \nSupraC2          1     ~3-5mins     128GiB    RAYON_NUM_THREADS*100%   ✓     -             -             \nSnapEncode       1     ~3-5mins     TODO      RAYON_NUM_THREADS*100%   ✓     ~32GiB(NFS)   ~32GiB(NFS)   建议每个 GPU 并发多个 SnapEncode 任务，让 IO 并发, 减少 GPU 空闲时间\nSnapProve        1     ~3-5mins     TODO      RAYON_NUM_THREADS*100%   ✓     -             -             \nSupraSnapProve   1     ~3-5mins     TODO      RAYON_NUM_THREADS*100%   ✓     ~32GiB(NFS)   ~32GiB(NFS)   \nUnseal           1     TODO         TODO      TODO                     x     TODO          TODO          \n\n备注: RAYON_NUM_THREADS 环境变量用于配置任务使用的线程数量，默认为 CPU 核心数。\n\n\n# damocles-worker-util 工具使用\n\ndamocles-worker-util 包含一组 damocles-worker 相关的实用工具。其中包括：\n\n * hwinfo (Hardware information)\n * sealcalc (Sealing calculator)\n\n\n# hwinfo\n\nhwinfo 显示硬件信息，我们可以根据输出的硬件信息合理的配置 damocles-worker, 以便于我们更有效地利用它们。\n\nhwinfo 当前可获取的信息如下：\n\n * CPU 拓扑 (包括 CPU 核心数，NUMA Memory Node, CPU Cache 等)\n * 磁盘信息\n * GPU 信息\n * 内存信息\n\n参数说明：\n\ndamocles-worker-util-hwinfo\n显示硬件信息\n\nUSAGE:\n    damocles-worker-util hwinfo [OPTIONS]\n\nOPTIONS:\n        --full    显示完整的 CPU 拓扑信息\n    -h, --help    打印帮助信息\n\n\n# hwinfo 依赖安装\n\n * hwloc 2.x 用于获取 CPU 拓扑信息\n * OpenCL 用于获取 GPU 信息\n\n# hwloc 2.x 安装\n\n# 在 Ubuntu 20.04 或之后的版本可以直接使用 apt 安装\n\n apt install hwloc=2.\\*\n\n\n# 源码安装：\n\n# 安装必要的工具. \napt install -y wget make gcc\n# 下载 hwloc-2.7.1.tar.gz\nwget https://download.open-mpi.org/release/hwloc/v2.7/hwloc-2.7.1.tar.gz\n\ntar -zxpf hwloc-2.7.1.tar.gz\ncd hwloc-2.7.1\n./configure --prefix=/usr/local\nmake -j$(nproc)\nsudo make install\nldconfig /usr/local/lib\n\n\n# OpenCL 安装\n\napt install ocl-icd-opencl-dev\n\n\n# hwinfo 实例\n\n在一台有 2 个 32 核 CPU 的机器上运行：\n\ndamocles-worker-util hwinfo\n\n\n输出：\n\nCPU topology:\nMachine (503.55 GiB)\n├── Package (251.57 GiB) (*** *** *** 32-Core Processor)\n│   ├── NUMANode (#0 251.57 GiB)\n│   ├── L3 (#0 16 MiB)\n│   │   └── PU #0 + PU #1 + PU #2 + PU #3\n│   ├── L3 (#1 16 MiB)\n│   │   └── PU #4 + PU #5 + PU #6 + PU #7\n│   ├── L3 (#2 16 MiB)\n│   │   └── PU #8 + PU #9 + PU #10 + PU #11\n│   ├── L3 (#3 16 MiB)                       \n│   │   └── PU #12 + PU #13 + PU #14 + PU #15\n│   ├── L3 (#4 16 MiB)                       \n│   │   └── PU #16 + PU #17 + PU #18 + PU #19\n│   ├── L3 (#5 16 MiB)\n│   │   └── PU #20 + PU #21 + PU #22 + PU #23\n│   ├── L3 (#6 16 MiB)\n│   │   └── PU #24 + PU #25 + PU #26 + PU #27\n│   └── L3 (#7 16 MiB)\n│       └── PU #28 + PU #29 + PU #30 + PU #31\n└── Package (251.98 GiB) (*** *** *** 32-Core Processor)\n    ├── NUMANode (#1 251.98 GiB)\n    ├── L3 (#8 16 MiB)\n    │   └── PU #32 + PU #33 + PU #34 + PU #35\n    ├── L3 (#9 16 MiB)\n    │   └── PU #36 + PU #37 + PU #38 + PU #39\n    ├── L3 (#10 16 MiB)\n    │   └── PU #40 + PU #41 + PU #42 + PU #43\n    ├── L3 (#11 16 MiB)\n    │   └── PU #44 + PU #45 + PU #46 + PU #47\n    ├── L3 (#12 16 MiB)\n    │   └── PU #48 + PU #49 + PU #50 + PU #51\n    ├── L3 (#13 16 MiB)\n    │   └── PU #52 + PU #53 + PU #54 + PU #55\n    ├── L3 (#14 16 MiB)\n    │   └── PU #56 + PU #57 + PU #58 + PU #59\n    └── L3 (#15 16 MiB)\n        └── PU #60 + PU #61 + PU #62 + PU #63\n\nDisks:\n╭───────────┬─────────────┬─────────────┬────────────┬───────────────────────────────────────╮\n│ Disk type │ Device name │ Mount point │ Filesystem │                 Space                 │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ SSD       │ /dev/sda3   │ /           │ ext4       │ 346.87 GiB / 434.68 GiB (79.80% used) │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ SSD       │ /dev/sda2   │ /boot       │ ext4       │ 675.00 MiB / 3.87 GiB (17.01% used)   │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ SSD       │ /dev/md127  │ /mnt/mount  │ ext4       │ 4.83 TiB / 13.86 TiB (34.86% used)    │\n╰───────────┴─────────────┴─────────────┴────────────┴───────────────────────────────────────╯\n\nGPU:\n╭─────────────────────────┬────────┬───────────╮\n│           Name          │ Vendor │   Memory  │\n├─────────────────────────┼────────┼───────────┤\n│ NVIDIA GeForce RTX 3080 │ NVIDIA │ 9.78 GiB  │\n├─────────────────────────┼────────┼───────────┤\n│ NVIDIA GeForce RTX 3080 │ NVIDIA │ 9.78 GiB  │\n├─────────────────────────┼────────┼───────────┤\n│ NVIDIA GeForce RTX 3080 │ NVIDIA │ 9.78 GiB  │\n╰─────────────────────────┴────────┴───────────╯\n\n\nMemory:\n╭──────────────┬───────────────────┬────────────┬─────────────╮\n│ Total memory │    Used memory    │ Total swap │  Used swap  │\n├──────────────┼───────────────────┼────────────┼─────────────┤\n│ 515.63 GiB   │ 33.51 GiB (6.50%) │ 0 B        │ 0 B (0.00%) │\n╰──────────────┴───────────────────┴────────────┴─────────────╯\n\n\n从输出 CPU topology 信息来看，这台机器有两个 NUMANode:\n\n 1. NUMANode #0 的 CPU 集合：0-31\n 2. NUMANode #1 的 CPU 集合：32-63\n\n我们可以在 damocles-worker 的配置文件中修改外部执行器配置组 ([[[processors.{stage_name}]]](./03.damocles-worker 的配置解析.md#processorsstage_name)) 通过 cgroup.cpuset + numa_preferred 配置项限制该外部执行仅使用指定的 NUMANode 中的 CPU, 内存也尽量优先从该 NUMANode 中分配，进而提高 CPU 工作效率 (damocles v0.5.0 之后，支持加载 NUMA 亲和的 hugepage 内存文件，如果启用该功能可以跨 NUMA 节点分配 cpuset 不会产生影响)。\n\n例：\n\n# damocles-worker.toml\n\n[[processors.{stage_name}]]\nnuma_preferred = 0\ncgroup.cpuset = "0-3"\n# ...\n\n[[processors.{stage_name}]]\nnuma_preferred = 1\ncgroup.cpuset = "32-35"\n# ...\n\n\n----------------------------------------\n\n\n# sealcalc\n\nsealcalc 通过给定的参数计算出各个时间段每个阶段任务的运行状态，可以通过调整各任务的最大并发数量以及 sealing_threads 来达到封装效率的最大化。\n\n参数说明：\n\nUSAGE:\n    damocles-worker-util sealcalc [OPTIONS] --tree_d_mins <tree_d_mins> --tree_d_concurrent <tree_d_concurrent> --pc1_mins <pc1_mins> --pc1_concurrent <pc1_concurrent> --pc2_mins <pc2_mins> --pc2_concurrent <pc2_concurrent> --c2_mins <c2_mins> --c2_concurrent <c2_concurrent> --sealing_threads <sealing_threads>\n\nOPTIONS:\n        --c2_concurrent <c2_concurrent>              指定 c2 阶段的最大并发数量\n        --c2_mins <c2_mins>                          指定单次执行 c2 阶段的任务的时间, 单位: 分钟\n        --calculate_days <calculate_days>            计算总时长, 单位: 天 [默认: 30]\n        --calculate_step_mins <calculate_step_mins>  输出的步长, 单位: 分钟 [默认: 60], 如果此值为 60 则每行结果间隔 1 小时\n        --csv                                        以 csv 格式输出结果\n    -h, --help                                       打印帮助信息\n        --pc1_concurrent <pc1_concurrent>            指定 pc1 阶段的最大并发数量\n        --pc1_mins <pc1_mins>                        指定单次执行 pc1 阶段的任务所需的时间, 单位: 分钟\n        --pc2_concurrent <pc2_concurrent>            指定 pc2 阶段的最大并发数量\n        --pc2_mins <pc2_mins>                        指定单次执行 pc2 阶段的任务所需的时间, 单位: 分钟\n        --sealing_threads <sealing_threads>          指定 sealing_threads 工作线程的数量\n        --seed_mins <seed_mins>                      指定等待 seed 的时长, 单位: 分钟 [默认: 80]\n        --tree_d_concurrent <tree_d_concurrent>      指定 tree_d 阶段的最大并发数量\n        --tree_d_mins <tree_d_mins>                  指定单次执行 tree_d 阶段的任务的时间, 单位: 分钟\n\n\n# sealcalc 实例：\n\n# 固定的参数：\n\n * tree_d 任务执行所需时间：10 分钟\n * pc1 任务执行所需时间：320 分钟\n * pc2 任务执行所需时间：25 分钟\n * c2 任务执行所需时间：18 分钟\n\n# 可调整的参数：\n\n * sealing_threads 工作线程数量：18\n * tree_d 最大并发数：2\n * pc1 最大并发数：10\n * pc2 最大并发数：5\n * c2 最大并发数：2\n\ndamocles-worker-util sealcalc --tree_d_mins=10 --pc1_mins=320 --pc2_mins=1 --c2_mins=2 --tree_d_concurrent=2 --pc1_concurrent=10 --pc2_concurrent=5 --c2_concurrent=2 --sealing_threads=18\n\n\n输出如下：\n\n┌sealing calculator─────────────────────────────────────────────────────┐\n│time    sealing    tree_d      pc1      pc2     wait    c2     finished│\n│(mins)  threads    (...)      (...)     (...)   seed   (...)   sectors │\n│                                                                       │\n│0       2/18        2/2       0/10       0/5      0     0/2      0     │\n│60      14/18       2/2       10/10      0/5      0     0/2      0     │\n│120     18/18       0/2       10/10      0/5      0     0/2      0     │\n│180     18/18       0/2       10/10      0/5      0     0/2      0     │\n│240     18/18       0/2       10/10      0/5      0     0/2      0     │\n│300     18/18       0/2       10/10      0/5      0     0/2      0     │\n│360     18/18       0/2       10/10      2/5      6     0/2      0     │\n│420     18/18       2/2       8/10       0/5      8     0/2      2     │\n│480     18/18       0/2       10/10      0/5      0     0/2      10    │\n│540     18/18       0/2       10/10      0/5      0     0/2      10    │\n│600     18/18       0/2       10/10      0/5      0     0/2      10    │\n│660     18/18       0/2       10/10      2/5      2     0/2      10    │\n│720     18/18       0/2       10/10      0/5      8     0/2      10    │\n│780     18/18       0/2       10/10      0/5      2     0/2      18    │\n│840     18/18       0/2       10/10      0/5      0     0/2      20    │\n│900     18/18       0/2       10/10      0/5      0     0/2      20    │\n│960     18/18       0/2       10/10      0/5      0     0/2      20    │\n│1020    18/18       0/2       10/10      0/5      8     0/2      20    │\n│1080    18/18       2/2       10/10      0/5      4     0/2      26    │\n│1140    18/18       0/2       10/10      0/5      2     0/2      28    │\n│1200    18/18       0/2       10/10      0/5      0     0/2      30    │\n│1260    18/18       0/2       10/10      0/5      0     0/2      30    │\n│1320    18/18       0/2       10/10      2/5      6     0/2      30    │\n│1380    18/18       2/2       10/10      0/5      6     0/2      32    │\n│1440    18/18       0/2       10/10      0/5      2     0/2      38    │\n│1500    18/18       0/2       10/10      0/5      0     0/2      40    │\n│1560    18/18       0/2       10/10      0/5      0     0/2      40    │\n│1620    18/18       0/2       10/10      2/5      2     0/2      40    │\n│1680    18/18       0/2       10/10      0/5      8     0/2      40    │\n│1740    18/18       0/2       10/10      0/5      2     0/2      48    │\n└───────────────────────────────────────────────────────────────────────┘\n\n\n方向键可以翻页\n\n输出结果各列说明：\n\n * time (mins): 时间，单位分钟。输出的每一项数据都是在此时间的运行结果\n * sealing thread (running/total): 封装线程状态 (正在运行的线程/总线程)\n * tree_d (running/total): tree_d 阶段的任务状态 (正在运行的任务数量/总任务数量)\n * pc1 (running/total): pc1 阶段的任务状态 (正在运行的任务数量/总任务数量)\n * pc2 (running/total): pc2 阶段的任务状态 (正在运行的任务数量/总任务数量)\n * wait seed: 等待 seed 的任务数量\n * c2 (running/total): c2 阶段的任务状态 (正在运行的任务数量/总任务数量)\n * finish sector: 当前时间已完成的扇区\n\n我们可以通过不断的调整更合理的上述的可调整的参数, 来达到封装效率的最大化。这些参数可以给 damocles-worker 的配置作为参考。',normalizedContent:'# damocles-worker 配置指北\n\n\n# sealing 任务各阶段资源消耗情况 (32 gib 扇区)\n\n测试机型：\n\n * cpu: amd epyc 7642 (max mhz: 2300)\n * gpu: rtx 3080\n * memory: dimm ddr4 synchronous registered (buffered) 2933 mhz (0.3 ns)\n\nstage            并发数   耗时           内存        cpu                      gpu   diskio 读      diskio 写      备注\nwindowpost       1     ~4-10mins    ~120gib   rayon_num_threads*100%   ✓     todo          -             \nwinningpost      1     ~1-10s       -         -                        -     -             -             \naddpieces        1     ~3mins       ~210mib   rayon_num_threads*100%   x     <=32gib       32gib         \ntreed            1     ~1min        ~47gib    rayon_num_threads*100%   x     32gib         64gib         64 gib 扇区 ~60gib ram\npc1              1     ~177mins     <=64gib   150%                     x     -             352gib        \npc2              1     ~10-13mins   ~64gib    rayon_num_threads*100%   ✓     384gib        ~37gib        \nsuprapc2         1     ~2-5mins     ~40gib    ~400%                    ✓     384gib        ~37gib        \nwaitseed         -     75mins       -         -                        x     -             -             \nc1               1     ~1s          -         -                        x     -             -             \nc2               1     ~13-16mins   todo      rayon_num_threads*100%   ✓     -             -             \nsuprac2          1     ~3-5mins     128gib    rayon_num_threads*100%   ✓     -             -             \nsnapencode       1     ~3-5mins     todo      rayon_num_threads*100%   ✓     ~32gib(nfs)   ~32gib(nfs)   建议每个 gpu 并发多个 snapencode 任务，让 io 并发, 减少 gpu 空闲时间\nsnapprove        1     ~3-5mins     todo      rayon_num_threads*100%   ✓     -             -             \nsuprasnapprove   1     ~3-5mins     todo      rayon_num_threads*100%   ✓     ~32gib(nfs)   ~32gib(nfs)   \nunseal           1     todo         todo      todo                     x     todo          todo          \n\n备注: rayon_num_threads 环境变量用于配置任务使用的线程数量，默认为 cpu 核心数。\n\n\n# damocles-worker-util 工具使用\n\ndamocles-worker-util 包含一组 damocles-worker 相关的实用工具。其中包括：\n\n * hwinfo (hardware information)\n * sealcalc (sealing calculator)\n\n\n# hwinfo\n\nhwinfo 显示硬件信息，我们可以根据输出的硬件信息合理的配置 damocles-worker, 以便于我们更有效地利用它们。\n\nhwinfo 当前可获取的信息如下：\n\n * cpu 拓扑 (包括 cpu 核心数，numa memory node, cpu cache 等)\n * 磁盘信息\n * gpu 信息\n * 内存信息\n\n参数说明：\n\ndamocles-worker-util-hwinfo\n显示硬件信息\n\nusage:\n    damocles-worker-util hwinfo [options]\n\noptions:\n        --full    显示完整的 cpu 拓扑信息\n    -h, --help    打印帮助信息\n\n\n# hwinfo 依赖安装\n\n * hwloc 2.x 用于获取 cpu 拓扑信息\n * opencl 用于获取 gpu 信息\n\n# hwloc 2.x 安装\n\n# 在 ubuntu 20.04 或之后的版本可以直接使用 apt 安装\n\n apt install hwloc=2.\\*\n\n\n# 源码安装：\n\n# 安装必要的工具. \napt install -y wget make gcc\n# 下载 hwloc-2.7.1.tar.gz\nwget https://download.open-mpi.org/release/hwloc/v2.7/hwloc-2.7.1.tar.gz\n\ntar -zxpf hwloc-2.7.1.tar.gz\ncd hwloc-2.7.1\n./configure --prefix=/usr/local\nmake -j$(nproc)\nsudo make install\nldconfig /usr/local/lib\n\n\n# opencl 安装\n\napt install ocl-icd-opencl-dev\n\n\n# hwinfo 实例\n\n在一台有 2 个 32 核 cpu 的机器上运行：\n\ndamocles-worker-util hwinfo\n\n\n输出：\n\ncpu topology:\nmachine (503.55 gib)\n├── package (251.57 gib) (*** *** *** 32-core processor)\n│   ├── numanode (#0 251.57 gib)\n│   ├── l3 (#0 16 mib)\n│   │   └── pu #0 + pu #1 + pu #2 + pu #3\n│   ├── l3 (#1 16 mib)\n│   │   └── pu #4 + pu #5 + pu #6 + pu #7\n│   ├── l3 (#2 16 mib)\n│   │   └── pu #8 + pu #9 + pu #10 + pu #11\n│   ├── l3 (#3 16 mib)                       \n│   │   └── pu #12 + pu #13 + pu #14 + pu #15\n│   ├── l3 (#4 16 mib)                       \n│   │   └── pu #16 + pu #17 + pu #18 + pu #19\n│   ├── l3 (#5 16 mib)\n│   │   └── pu #20 + pu #21 + pu #22 + pu #23\n│   ├── l3 (#6 16 mib)\n│   │   └── pu #24 + pu #25 + pu #26 + pu #27\n│   └── l3 (#7 16 mib)\n│       └── pu #28 + pu #29 + pu #30 + pu #31\n└── package (251.98 gib) (*** *** *** 32-core processor)\n    ├── numanode (#1 251.98 gib)\n    ├── l3 (#8 16 mib)\n    │   └── pu #32 + pu #33 + pu #34 + pu #35\n    ├── l3 (#9 16 mib)\n    │   └── pu #36 + pu #37 + pu #38 + pu #39\n    ├── l3 (#10 16 mib)\n    │   └── pu #40 + pu #41 + pu #42 + pu #43\n    ├── l3 (#11 16 mib)\n    │   └── pu #44 + pu #45 + pu #46 + pu #47\n    ├── l3 (#12 16 mib)\n    │   └── pu #48 + pu #49 + pu #50 + pu #51\n    ├── l3 (#13 16 mib)\n    │   └── pu #52 + pu #53 + pu #54 + pu #55\n    ├── l3 (#14 16 mib)\n    │   └── pu #56 + pu #57 + pu #58 + pu #59\n    └── l3 (#15 16 mib)\n        └── pu #60 + pu #61 + pu #62 + pu #63\n\ndisks:\n╭───────────┬─────────────┬─────────────┬────────────┬───────────────────────────────────────╮\n│ disk type │ device name │ mount point │ filesystem │                 space                 │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ ssd       │ /dev/sda3   │ /           │ ext4       │ 346.87 gib / 434.68 gib (79.80% used) │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ ssd       │ /dev/sda2   │ /boot       │ ext4       │ 675.00 mib / 3.87 gib (17.01% used)   │\n├───────────┼─────────────┼─────────────┼────────────┼───────────────────────────────────────┤\n│ ssd       │ /dev/md127  │ /mnt/mount  │ ext4       │ 4.83 tib / 13.86 tib (34.86% used)    │\n╰───────────┴─────────────┴─────────────┴────────────┴───────────────────────────────────────╯\n\ngpu:\n╭─────────────────────────┬────────┬───────────╮\n│           name          │ vendor │   memory  │\n├─────────────────────────┼────────┼───────────┤\n│ nvidia geforce rtx 3080 │ nvidia │ 9.78 gib  │\n├─────────────────────────┼────────┼───────────┤\n│ nvidia geforce rtx 3080 │ nvidia │ 9.78 gib  │\n├─────────────────────────┼────────┼───────────┤\n│ nvidia geforce rtx 3080 │ nvidia │ 9.78 gib  │\n╰─────────────────────────┴────────┴───────────╯\n\n\nmemory:\n╭──────────────┬───────────────────┬────────────┬─────────────╮\n│ total memory │    used memory    │ total swap │  used swap  │\n├──────────────┼───────────────────┼────────────┼─────────────┤\n│ 515.63 gib   │ 33.51 gib (6.50%) │ 0 b        │ 0 b (0.00%) │\n╰──────────────┴───────────────────┴────────────┴─────────────╯\n\n\n从输出 cpu topology 信息来看，这台机器有两个 numanode:\n\n 1. numanode #0 的 cpu 集合：0-31\n 2. numanode #1 的 cpu 集合：32-63\n\n我们可以在 damocles-worker 的配置文件中修改外部执行器配置组 ([[[processors.{stage_name}]]](./03.damocles-worker 的配置解析.md#processorsstage_name)) 通过 cgroup.cpuset + numa_preferred 配置项限制该外部执行仅使用指定的 numanode 中的 cpu, 内存也尽量优先从该 numanode 中分配，进而提高 cpu 工作效率 (damocles v0.5.0 之后，支持加载 numa 亲和的 hugepage 内存文件，如果启用该功能可以跨 numa 节点分配 cpuset 不会产生影响)。\n\n例：\n\n# damocles-worker.toml\n\n[[processors.{stage_name}]]\nnuma_preferred = 0\ncgroup.cpuset = "0-3"\n# ...\n\n[[processors.{stage_name}]]\nnuma_preferred = 1\ncgroup.cpuset = "32-35"\n# ...\n\n\n----------------------------------------\n\n\n# sealcalc\n\nsealcalc 通过给定的参数计算出各个时间段每个阶段任务的运行状态，可以通过调整各任务的最大并发数量以及 sealing_threads 来达到封装效率的最大化。\n\n参数说明：\n\nusage:\n    damocles-worker-util sealcalc [options] --tree_d_mins <tree_d_mins> --tree_d_concurrent <tree_d_concurrent> --pc1_mins <pc1_mins> --pc1_concurrent <pc1_concurrent> --pc2_mins <pc2_mins> --pc2_concurrent <pc2_concurrent> --c2_mins <c2_mins> --c2_concurrent <c2_concurrent> --sealing_threads <sealing_threads>\n\noptions:\n        --c2_concurrent <c2_concurrent>              指定 c2 阶段的最大并发数量\n        --c2_mins <c2_mins>                          指定单次执行 c2 阶段的任务的时间, 单位: 分钟\n        --calculate_days <calculate_days>            计算总时长, 单位: 天 [默认: 30]\n        --calculate_step_mins <calculate_step_mins>  输出的步长, 单位: 分钟 [默认: 60], 如果此值为 60 则每行结果间隔 1 小时\n        --csv                                        以 csv 格式输出结果\n    -h, --help                                       打印帮助信息\n        --pc1_concurrent <pc1_concurrent>            指定 pc1 阶段的最大并发数量\n        --pc1_mins <pc1_mins>                        指定单次执行 pc1 阶段的任务所需的时间, 单位: 分钟\n        --pc2_concurrent <pc2_concurrent>            指定 pc2 阶段的最大并发数量\n        --pc2_mins <pc2_mins>                        指定单次执行 pc2 阶段的任务所需的时间, 单位: 分钟\n        --sealing_threads <sealing_threads>          指定 sealing_threads 工作线程的数量\n        --seed_mins <seed_mins>                      指定等待 seed 的时长, 单位: 分钟 [默认: 80]\n        --tree_d_concurrent <tree_d_concurrent>      指定 tree_d 阶段的最大并发数量\n        --tree_d_mins <tree_d_mins>                  指定单次执行 tree_d 阶段的任务的时间, 单位: 分钟\n\n\n# sealcalc 实例：\n\n# 固定的参数：\n\n * tree_d 任务执行所需时间：10 分钟\n * pc1 任务执行所需时间：320 分钟\n * pc2 任务执行所需时间：25 分钟\n * c2 任务执行所需时间：18 分钟\n\n# 可调整的参数：\n\n * sealing_threads 工作线程数量：18\n * tree_d 最大并发数：2\n * pc1 最大并发数：10\n * pc2 最大并发数：5\n * c2 最大并发数：2\n\ndamocles-worker-util sealcalc --tree_d_mins=10 --pc1_mins=320 --pc2_mins=1 --c2_mins=2 --tree_d_concurrent=2 --pc1_concurrent=10 --pc2_concurrent=5 --c2_concurrent=2 --sealing_threads=18\n\n\n输出如下：\n\n┌sealing calculator─────────────────────────────────────────────────────┐\n│time    sealing    tree_d      pc1      pc2     wait    c2     finished│\n│(mins)  threads    (...)      (...)     (...)   seed   (...)   sectors │\n│                                                                       │\n│0       2/18        2/2       0/10       0/5      0     0/2      0     │\n│60      14/18       2/2       10/10      0/5      0     0/2      0     │\n│120     18/18       0/2       10/10      0/5      0     0/2      0     │\n│180     18/18       0/2       10/10      0/5      0     0/2      0     │\n│240     18/18       0/2       10/10      0/5      0     0/2      0     │\n│300     18/18       0/2       10/10      0/5      0     0/2      0     │\n│360     18/18       0/2       10/10      2/5      6     0/2      0     │\n│420     18/18       2/2       8/10       0/5      8     0/2      2     │\n│480     18/18       0/2       10/10      0/5      0     0/2      10    │\n│540     18/18       0/2       10/10      0/5      0     0/2      10    │\n│600     18/18       0/2       10/10      0/5      0     0/2      10    │\n│660     18/18       0/2       10/10      2/5      2     0/2      10    │\n│720     18/18       0/2       10/10      0/5      8     0/2      10    │\n│780     18/18       0/2       10/10      0/5      2     0/2      18    │\n│840     18/18       0/2       10/10      0/5      0     0/2      20    │\n│900     18/18       0/2       10/10      0/5      0     0/2      20    │\n│960     18/18       0/2       10/10      0/5      0     0/2      20    │\n│1020    18/18       0/2       10/10      0/5      8     0/2      20    │\n│1080    18/18       2/2       10/10      0/5      4     0/2      26    │\n│1140    18/18       0/2       10/10      0/5      2     0/2      28    │\n│1200    18/18       0/2       10/10      0/5      0     0/2      30    │\n│1260    18/18       0/2       10/10      0/5      0     0/2      30    │\n│1320    18/18       0/2       10/10      2/5      6     0/2      30    │\n│1380    18/18       2/2       10/10      0/5      6     0/2      32    │\n│1440    18/18       0/2       10/10      0/5      2     0/2      38    │\n│1500    18/18       0/2       10/10      0/5      0     0/2      40    │\n│1560    18/18       0/2       10/10      0/5      0     0/2      40    │\n│1620    18/18       0/2       10/10      2/5      2     0/2      40    │\n│1680    18/18       0/2       10/10      0/5      8     0/2      40    │\n│1740    18/18       0/2       10/10      0/5      2     0/2      48    │\n└───────────────────────────────────────────────────────────────────────┘\n\n\n方向键可以翻页\n\n输出结果各列说明：\n\n * time (mins): 时间，单位分钟。输出的每一项数据都是在此时间的运行结果\n * sealing thread (running/total): 封装线程状态 (正在运行的线程/总线程)\n * tree_d (running/total): tree_d 阶段的任务状态 (正在运行的任务数量/总任务数量)\n * pc1 (running/total): pc1 阶段的任务状态 (正在运行的任务数量/总任务数量)\n * pc2 (running/total): pc2 阶段的任务状态 (正在运行的任务数量/总任务数量)\n * wait seed: 等待 seed 的任务数量\n * c2 (running/total): c2 阶段的任务状态 (正在运行的任务数量/总任务数量)\n * finish sector: 当前时间已完成的扇区\n\n我们可以通过不断的调整更合理的上述的可调整的参数, 来达到封装效率的最大化。这些参数可以给 damocles-worker 的配置作为参考。',charsets:{cjk:!0}}],themeConfig:{logo:"/assets/damocles-icon.png",lastUpdated:"Last Updated",docsRepo:"ipfs-force-community/damocles-docs",docsDir:"docs",docsBranch:"main",editLinks:!0,displayAllHeaders:!1,locales:{"/":{selectText:"Languages",label:"English",lang:"en-US",title:"Damocles",description:"Damocles (formerly known as Venus Power Service) is THE Filecoin storage power solution.",nav:[{text:"Introduction",link:"/intro/"},{text:"Deployment & Operation",link:"/operation/"},{text:"About",link:"/about/"}],sidebar:{"/intro/":[{title:"Introduction",collapsable:!1,children:[["","Overview"],["architecture.md","Architecture"]]}],"/operation/":[{title:"Deployment",collapsable:!1,children:[["","Getting started"]]},{title:"Configurations",collapsable:!1,children:[["damocles-manager-config.md","damocles-manager config","https://github.com/ipfs-force-community/damocles/blob/main/docs/en/04.damocles-manager-config.md"],["damocles-worker-config.md","damocles-worker config","https://github.com/ipfs-force-community/damocles/blob/main/docs/en/03.damocles-worker-config.md"]]},{title:"Operations",collapsable:!1,children:[["task-management.md","Task management","https://github.com/ipfs-force-community/damocles/blob/main/docs/en/10.damocles-worker-task-management.md"],["task-flow.md","Task flow","https://github.com/ipfs-force-community/damocles/blob/main/docs/en/11.task-status-flow.md"],["poster.md","Dedicated PoSt node","https://github.com/ipfs-force-community/damocles/blob/main/docs/en/09.poster-node.md"],["snapup.md","Snapdeal support","https://github.com/ipfs-force-community/damocles/blob/main/docs/en/08.snapdeal-support.md"],["worker-config-guide.md","Worker config guide","https://github.com/ipfs-force-community/damocles/blob/main/docs/en/12.damocles-worker-config-guide.md"]]},{title:"Migration",collapsable:!1,children:[["migrate-sectors.md","Import existing sectors","https://github.com/ipfs-force-community/damocles/blob/main/docs/en/06.migrate-sectors-to-damocles.md"]]}],"/about/":[{title:"",collapsable:!1,children:[["","About Us"]]}]}},"/zh/":{selectText:"选择语言",label:"简体中文",title:"执剑人",description:"执剑人（原Venus算力服务）是一个Filecoin存储算力解决方案",nav:[{text:"功能介绍",link:"/zh/intro/"},{text:"部署/运维",link:"/zh/operation/"},{text:"研发/设计",link:"/zh/developer/"},{text:"关于",link:"/zh/about/"}],sidebar:{"/zh/intro/":[{title:"功能特性",collapsable:!1,children:[["","概要"],["architecture.md","架构"]]}],"/zh/operation/":[{title:"部署",collapsable:!1,children:[["quick-start.md","快速上手","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/05.%E5%BF%AB%E9%80%9F%E5%90%AF%E7%94%A8.md"]]},{title:"配置",collapsable:!1,children:[["damocles-manager-config.md","damocles-manager 配置","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/04.damocles-manager%E7%9A%84%E9%85%8D%E7%BD%AE%E8%A7%A3%E6%9E%90.md"],["damocles-worker-config.md","damocles-worker 配置","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/03.damocles-worker%E7%9A%84%E9%85%8D%E7%BD%AE%E8%A7%A3%E6%9E%90.md"],["processors-config-example.md","外部执行器配置范例","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/07.damocles-worker%E5%A4%96%E9%83%A8%E6%89%A7%E8%A1%8C%E5%99%A8%E7%9A%84%E9%85%8D%E7%BD%AE%E8%8C%83%E4%BE%8B.md"]]},{title:"运维",collapsable:!1,children:[["task-management.md","worker 任务管理","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/10.venus-worker%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86.md"],["task-flow.md","任务状态流转","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/11.%E4%BB%BB%E5%8A%A1%E7%8A%B6%E6%80%81%E6%B5%81%E8%BD%AC.md"],["poster.md","独立 Poster","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/09.%E7%8B%AC%E7%AB%8B%E8%BF%90%E8%A1%8C%E7%9A%84poster%E8%8A%82%E7%82%B9.md"],["snapup.md","snapdeal 支持","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/08.snapdeal%E7%9A%84%E6%94%AF%E6%8C%81.md"],["worker-config-guide.md","damocles-worker 配置指北","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/12.damocles-worker-%E9%85%8D%E7%BD%AE%E6%8C%87%E5%8C%97.md"],["custom-algo.md","自定义算法/存储方案","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/13.%E8%87%AA%E5%AE%9A%E4%B9%89%E7%AE%97%E6%B3%95%E5%92%8C%E5%AD%98%E5%82%A8%E6%96%B9%E6%A1%88.md"],["metrics.md","damocles-manager metrics 使用","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/14.damocles-manager%E7%9A%84mertics%E4%BD%BF%E7%94%A8.md"],["hugeTLB.md","PC1 hugeTLB 支持","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/15.damocles-worker_PC1_HugeTLB_Pages_%E6%94%AF%E6%8C%81.md"],["sector-rebuild.md","扇区重建支持","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/16.%E6%89%87%E5%8C%BA%E9%87%8D%E5%BB%BA%E7%9A%84%E6%94%AF%E6%8C%81.md"],["unseal.md","unseal 反封装","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/18.Unseal%E4%BB%BB%E5%8A%A1%E7%9A%84%E6%94%AF%E6%8C%81.md"]]},{title:"迁移",collapsable:!1,children:[["migrate-sectors.md","导入已有扇区","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/06.%E5%AF%BC%E5%85%A5%E5%B7%B2%E5%AD%98%E5%9C%A8%E7%9A%84%E6%89%87%E5%8C%BA%E6%95%B0%E6%8D%AE.md"],["migrate-miner.md","lotus-miner 与 damocles 切换流程","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/17.%20damocles%E4%B8%8Elotus-miner%E5%88%87%E6%8D%A2%E6%B5%81%E7%A8%8B.md"]]},{title:"FAQ",collapsable:!1,children:[["faq.md","常见问题","https://github.com/ipfs-force-community/damocles/blob/main/docs/zh/QA.md"]]}],"/zh/developer/":[{title:"设计理念",collapsable:!1,children:[["","简述"],["concept.md","概念和基础设施","https://github.com/ipfs-force-community/venus-cluster/blob/main/docs/zh/02.%E6%A6%82%E5%BF%B5%E5%92%8C%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD.md"]]}],"/zh/about/":[{title:"",collapsable:!1,children:[["","关于我们"]]}]}}}},locales:{"/":{lang:"en-US",title:"Damocles",description:"Damocles, formerly known as Venus Power Service, is THE Filecoin storage power solution.",path:"/"},"/zh/":{lang:"zh-CN",title:"执剑人",description:"执剑人，原Venus算力服务，是一个Filecoin存储算力解决方案",path:"/zh/"}}};t(317);Ar.component("RedirectTutorial",(function(){return t.e(10).then(t.bind(null,390))})),Ar.component("Badge",(function(){return Promise.all([t.e(0),t.e(3)]).then(t.bind(null,436))})),Ar.component("CodeBlock",(function(){return Promise.all([t.e(0),t.e(4)]).then(t.bind(null,391))})),Ar.component("CodeGroup",(function(){return Promise.all([t.e(0),t.e(5)]).then(t.bind(null,392))}));t(318);var Ci={name:"BackToTop",props:{threshold:{type:Number,default:300}},data:function(){return{scrollTop:null}},computed:{show:function(){return this.scrollTop>this.threshold}},mounted:function(){var e=this;this.scrollTop=this.getScrollTop(),window.addEventListener("scroll",ti()((function(){e.scrollTop=e.getScrollTop()}),100))},methods:{getScrollTop:function(){return window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0},scrollToTop:function(){window.scrollTo({top:0,behavior:"smooth"}),this.scrollTop=0}}},Ei=(t(319),Object(_i.a)(Ci,(function(){var e=this.$createElement,n=this._self._c||e;return n("transition",{attrs:{name:"fade"}},[this.show?n("svg",{staticClass:"go-to-top",attrs:{xmlns:"http://www.w3.org/2000/svg",viewBox:"0 0 49.484 28.284"},on:{click:this.scrollToTop}},[n("g",{attrs:{transform:"translate(-229 -126.358)"}},[n("rect",{attrs:{fill:"currentColor",width:"35",height:"5",rx:"2",transform:"translate(229 151.107) rotate(-45)"}}),this._v(" "),n("rect",{attrs:{fill:"currentColor",width:"35",height:"5",rx:"2",transform:"translate(274.949 154.642) rotate(-135)"}})])]):this._e()])}),[],!1,null,"5fd4ef0c",null).exports),Ii=(t(320),[{},function(e){e.Vue.mixin({computed:{$dataBlock:function(){return this.$options.__data__block__}}})},{},function(e){e.Vue.component("BackToTop",Ei)},{},function(e){e.Vue.component("CodeCopy",wi)}]),Ti=["BackToTop"];t(181);function Ui(e,n){if(!(e instanceof n))throw new TypeError("Cannot call a class as a function")}t(101);function zi(e,n){for(var t=0;t<n.length;t++){var o=n[t];o.enumerable=o.enumerable||!1,o.configurable=!0,"value"in o&&(o.writable=!0),Object.defineProperty(e,o.key,o)}}function Oi(e,n,t){return n&&zi(e.prototype,n),t&&zi(e,t),e}t(173);function Ai(e,n){return(Ai=Object.setPrototypeOf||function(e,n){return e.__proto__=n,e})(e,n)}t(174);function Di(e){return(Di=Object.setPrototypeOf?Object.getPrototypeOf:function(e){return e.__proto__||Object.getPrototypeOf(e)})(e)}t(116),t(107);function Mi(e,n){return!n||"object"!==Ss(n)&&"function"!=typeof n?function(e){if(void 0===e)throw new ReferenceError("this hasn't been initialised - super() hasn't been called");return e}(e):n}function ji(e){var n=function(){if("undefined"==typeof Reflect||!Reflect.construct)return!1;if(Reflect.construct.sham)return!1;if("function"==typeof Proxy)return!0;try{return Date.prototype.toString.call(Reflect.construct(Date,[],(function(){}))),!0}catch(e){return!1}}();return function(){var t,o=Di(e);if(n){var r=Di(this).constructor;t=Reflect.construct(o,arguments,r)}else t=o.apply(this,arguments);return Mi(this,t)}}var Li=function(e){!function(e,n){if("function"!=typeof n&&null!==n)throw new TypeError("Super expression must either be null or a function");e.prototype=Object.create(n&&n.prototype,{constructor:{value:e,writable:!0,configurable:!0}}),n&&Ai(e,n)}(t,e);var n=ji(t);function t(){return Ui(this,t),n.apply(this,arguments)}return t}(function(){function e(){Ui(this,e),this.store=new Ar({data:{state:{}}})}return Oi(e,[{key:"$get",value:function(e){return this.store.state[e]}},{key:"$set",value:function(e,n){Ar.set(this.store.state,e,n)}},{key:"$emit",value:function(){var e;(e=this.store).$emit.apply(e,arguments)}},{key:"$on",value:function(){var e;(e=this.store).$on.apply(e,arguments)}}]),e}());Object.assign(Li.prototype,{getPageAsyncComponent:Fs,getLayoutAsyncComponent:Gs,getAsyncComponent:$s,getVueComponent:qs});var Bi={install:function(e){var n=new Li;e.$vuepress=n,e.prototype.$vuepress=n}};function Ri(e){e.beforeEach((function(n,t,o){if(Ni(e,n.path))o();else if(/(\/|\.html)$/.test(n.path))if(/\/$/.test(n.path)){var r=n.path.replace(/\/$/,"")+".html";Ni(e,r)?o(r):o()}else o();else{var a=n.path+"/",s=n.path+".html";Ni(e,s)?o(s):Ni(e,a)?o(a):o()}}))}function Ni(e,n){return e.options.routes.filter((function(e){return e.path.toLowerCase()===n.toLowerCase()})).length>0}var Fi={props:{pageKey:String,slotKey:{type:String,default:"default"}},render:function(e){var n=this.pageKey||this.$parent.$page.key;return Ws("pageKey",n),Ar.component(n)||Ar.component(n,Fs(n)),Ar.component(n)?e(n):e("")}},Gi={functional:!0,props:{slotKey:String,required:!0},render:function(e,n){var t=n.props,o=n.slots;return e("div",{class:["content__".concat(t.slotKey)]},o()[t.slotKey])}},$i={computed:{openInNewWindowTitle:function(){return this.$themeLocaleConfig.openNewWindowText||"(opens new window)"}}},qi=(t(322),t(323),Object(_i.a)($i,(function(){var e=this.$createElement,n=this._self._c||e;return n("span",[n("svg",{staticClass:"icon outbound",attrs:{xmlns:"http://www.w3.org/2000/svg","aria-hidden":"true",focusable:"false",x:"0px",y:"0px",viewBox:"0 0 100 100",width:"15",height:"15"}},[n("path",{attrs:{fill:"currentColor",d:"M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"}}),this._v(" "),n("polygon",{attrs:{fill:"currentColor",points:"45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"}})]),this._v(" "),n("span",{staticClass:"sr-only"},[this._v(this._s(this.openInNewWindowTitle))])])}),[],!1,null,null,null).exports);function Vi(){return(Vi=Object(o.a)(regeneratorRuntime.mark((function e(n){var t,o,r,a;return regeneratorRuntime.wrap((function(e){for(;;)switch(e.prev=e.next){case 0:return t="undefined"!=typeof window&&window.__VUEPRESS_ROUTER_BASE__?window.__VUEPRESS_ROUTER_BASE__:Si.routerBase||Si.base,Ri(o=new xs({base:t,mode:"history",fallback:!1,routes:Pi,scrollBehavior:function(e,n,t){return t||(e.hash?!Ar.$vuepress.$get("disableScrollBehavior")&&{selector:decodeURIComponent(e.hash)}:{x:0,y:0})}})),r={},e.prev=4,e.next=7,Promise.all(Ii.filter((function(e){return"function"==typeof e})).map((function(e){return e({Vue:Ar,options:r,router:o,siteData:Si,isServer:n})})));case 7:e.next=12;break;case 9:e.prev=9,e.t0=e.catch(4),console.error(e.t0);case 12:return a=new Ar(Object.assign(r,{router:o,render:function(e){return e("div",{attrs:{id:"app"}},[e("RouterView",{ref:"layout"}),e("div",{class:"global-ui"},Ti.map((function(n){return e(n)})))])}})),e.abrupt("return",{app:a,router:o});case 14:case"end":return e.stop()}}),e,null,[[4,9]])})))).apply(this,arguments)}Ar.config.productionTip=!1,Ar.use(xs),Ar.use(Bi),Ar.mixin(function(e,n){var t=arguments.length>2&&void 0!==arguments[2]?arguments[2]:Ar;Ps(n),t.$vuepress.$set("siteData",n);var o=e(t.$vuepress.$get("siteData")),r=new o,a=Object.getOwnPropertyDescriptors(Object.getPrototypeOf(r)),s={};return Object.keys(a).reduce((function(e,n){return n.startsWith("$")&&(e[n]=a[n].get),e}),s),{computed:s}}((function(e){return function(){function n(){Ui(this,n)}return Oi(n,[{key:"setPage",value:function(e){this.__page=e}},{key:"$site",get:function(){return e}},{key:"$themeConfig",get:function(){return this.$site.themeConfig}},{key:"$frontmatter",get:function(){return this.$page.frontmatter}},{key:"$localeConfig",get:function(){var e,n,t=this.$site.locales,o=void 0===t?{}:t;for(var r in o)"/"===r?n=o[r]:0===this.$page.path.indexOf(r)&&(e=o[r]);return e||n||{}}},{key:"$siteTitle",get:function(){return this.$localeConfig.title||this.$site.title||""}},{key:"$canonicalUrl",get:function(){var e=this.$page.frontmatter.canonicalUrl;return"string"==typeof e&&e}},{key:"$title",get:function(){var e=this.$page,n=this.$page.frontmatter.metaTitle;if("string"==typeof n)return n;var t=this.$siteTitle,o=e.frontmatter.home?null:e.frontmatter.title||e.title;return t?o?o+" | "+t:t:o||"VuePress"}},{key:"$description",get:function(){var e=function(e){if(e){var n=e.filter((function(e){return"description"===e.name}))[0];if(n)return n.content}}(this.$page.frontmatter.meta);return e||(this.$page.frontmatter.description||this.$localeConfig.description||this.$site.description||"")}},{key:"$lang",get:function(){return this.$page.frontmatter.lang||this.$localeConfig.lang||"en-US"}},{key:"$localePath",get:function(){return this.$localeConfig.path||"/"}},{key:"$themeLocaleConfig",get:function(){return(this.$site.themeConfig.locales||{})[this.$localePath]||{}}},{key:"$page",get:function(){return this.__page?this.__page:function(e,n){for(var t=0;t<e.length;t++){var o=e[t];if(o.path.toLowerCase()===n.toLowerCase())return o}return{path:"",frontmatter:{}}}(this.$site.pages,this.$route.path)}}]),n}()}),Si)),Ar.component("Content",Fi),Ar.component("ContentSlotsDistributor",Gi),Ar.component("OutboundLink",qi),Ar.component("ClientOnly",{functional:!0,render:function(e,n){var t=n.parent,o=n.children;if(t._isMounted)return o;t.$once("hook:mounted",(function(){t.$forceUpdate()}))}}),Ar.component("Layout",Gs("Layout")),Ar.component("NotFound",Gs("NotFound")),Ar.prototype.$withBase=function(e){var n=this.$site.base;return"/"===e.charAt(0)?n+e.slice(1):e},window.__VUEPRESS__={version:"1.8.0",hash:"4bc255d"},function(e){return Vi.apply(this,arguments)}(!1).then((function(e){var n=e.app;e.router.onReady((function(){n.$mount("#app")}))}))}]);